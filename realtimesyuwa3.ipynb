{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95bf11a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [STEP 1] ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŒ²ç”» ---\n",
      "ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã«ãƒ•ã‚©ãƒ¼ã‚«ã‚¹ã‚’åˆã‚ã›ã¦ãã ã•ã„ã€‚\n",
      "  's'ã‚­ãƒ¼: éŒ²ç”»é–‹å§‹\n",
      "  'e'ã‚­ãƒ¼: éŒ²ç”»åœæ­¢\n",
      "  'q'ã‚­ãƒ¼: ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµ‚äº†\n",
      "\n",
      "[éŒ²ç”»é–‹å§‹] -> ä¿å­˜å…ˆ: c:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\Inference_Pipeline\\recorded_videos\\rec_20251011_112943.mp4\n",
      "[éŒ²ç”»çµ‚äº†]\n",
      "\n",
      "--- [STEP 2] MediaPipeã«ã‚ˆã‚‹éª¨æ ¼åº§æ¨™ã®æŠ½å‡º ---\n",
      "å‡¦ç†ä¸­ã®å‹•ç”»: c:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\Inference_Pipeline\\recorded_videos\\rec_20251011_112943.mp4\n",
      "JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸ: c:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\Inference_Pipeline\\json_output\\rec_20251011_112943\n",
      "\n",
      "--- [STEP 3] JSONã‹ã‚‰å˜ä¸€TXTãƒ•ã‚¡ã‚¤ãƒ«ã¸å¤‰æ› ---\n",
      "TXTãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®å¤‰æ›ãŒå®Œäº†ã—ã¾ã—ãŸ: c:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\Inference_Pipeline\\txt_converted\\rec_20251011_112943.txt\n",
      "\n",
      "--- [STEP 4] æ¨è«–ç”¨ãƒ‡ãƒ¼ã‚¿(X_val.txt)ã®ä½œæˆ ---\n",
      "1å€‹ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ç”Ÿæˆã—ã€æ¨è«–ç”¨ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: c:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\Inference_Pipeline\\final_input\\X_val.txt\n",
      "\n",
      "--- [STEP 5] LSTMãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹æ¨è«– ---\n",
      "ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã¾ãŸã¯æ¨è«–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: Unable to revive model from config. When overriding the `get_config()` method, make sure that the returned config contains all items used as arguments in the  constructor to <class '__main__.LSTM_RNN'>, which is the default behavior. You can override this default behavior by defining a `from_config(cls, config)` class method to specify how to create an instance of LSTM_RNN from its config.\n",
      "\n",
      "Received config={'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}}\n",
      "\n",
      "Error encountered during deserialization: LSTM_RNN.__init__() missing 3 required positional arguments: 'n_input', 'n_hidden', and 'n_classes'\n",
      "æ¨è«–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: cannot access local variable 'model' where it is not associated with a value\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ==========================================\n",
    "# Real-time Sign Language Recognition Pipeline\n",
    "# ä¿®æ­£ç‰ˆï¼ˆLSTM_RNNãƒ¢ãƒ‡ãƒ«ã‚’æ§‹é€ å†å®šç¾©ï¼‹é‡ã¿èª­ã¿è¾¼ã¿ï¼‰\n",
    "# ==========================================\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# --- ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹å®šç¾© ---\n",
    "# ============================================================\n",
    "# ============================================================\n",
    "# --- ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹å®šç¾©ï¼ˆbuildå¯¾å¿œç‰ˆï¼‰ ---\n",
    "# ============================================================\n",
    "class LSTM_RNN(tf.keras.Model):\n",
    "    def __init__(self, n_input, n_hidden, n_classes, **kwargs):\n",
    "        super(LSTM_RNN, self).__init__(**kwargs)\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.input_dense = tf.keras.layers.Dense(n_hidden, activation='relu')\n",
    "        self.lstm1 = tf.keras.layers.LSTM(n_hidden, return_sequences=True)\n",
    "        self.lstm2 = tf.keras.layers.LSTM(n_hidden)\n",
    "        self.out = tf.keras.layers.Dense(n_classes, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.input_dense(x)\n",
    "        x = self.lstm1(x)\n",
    "        x = self.lstm2(x)\n",
    "        return self.out(x)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape = (batch, timesteps, features)\n",
    "        dummy = tf.zeros(input_shape)\n",
    "        _ = self.call(dummy)\n",
    "        super().build(input_shape)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# --- è¨­å®š ---\n",
    "# ============================================================\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # GPUç„¡åŠ¹åŒ–ï¼ˆå¿…è¦ã«å¿œã˜ã¦å¤‰æ›´ï¼‰\n",
    "\n",
    "MODEL_PATH = r\"C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\weights\\model.h5\"\n",
    "\n",
    "LABELS = [\n",
    "    \"ageru\", \"understand\", \"annsinnsuru\", \"heavy\"\n",
    "]\n",
    "\n",
    "BASE_OUTPUT_DIR = os.path.join(os.getcwd(), \"Inference_Pipeline\")\n",
    "SEQUENCE_LENGTH = 30\n",
    "N_HIDDEN = 34\n",
    "\n",
    "# ============================================================\n",
    "# --- STEP 1: éŒ²ç”» ---\n",
    "# ============================================================\n",
    "def record_video(save_dir, sequence_length):\n",
    "    print(\"--- [STEP 1] ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŒ²ç”» ---\")\n",
    "    print(\"ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã«ãƒ•ã‚©ãƒ¼ã‚«ã‚¹ã‚’åˆã‚ã›ã¦ãã ã•ã„ã€‚\")\n",
    "    print(\"  's'ã‚­ãƒ¼: éŒ²ç”»é–‹å§‹\")\n",
    "    print(\"  'e'ã‚­ãƒ¼: éŒ²ç”»åœæ­¢\")\n",
    "    print(\"  'q'ã‚­ãƒ¼: ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµ‚äº†\")\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"ã‚¨ãƒ©ãƒ¼: ã‚«ãƒ¡ãƒ©ã‚’é–‹ã‘ã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "        return None\n",
    "\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = 30.0\n",
    "    video_writer = None\n",
    "    is_recording = False\n",
    "    saved_filepath = None\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"ã‚¨ãƒ©ãƒ¼: ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’èª­ã¿å–ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "            break\n",
    "\n",
    "        display_frame = frame.copy()\n",
    "\n",
    "        if is_recording:\n",
    "            cv2.circle(display_frame, (40, 40), 15, (0, 0, 255), -1)\n",
    "            cv2.putText(display_frame, 'REC', (70, 50),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            if video_writer:\n",
    "                video_writer.write(frame)\n",
    "\n",
    "        cv2.imshow('Real-time Video Recorder', display_frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if key == ord('s') and not is_recording:\n",
    "            is_recording = True\n",
    "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"rec_{timestamp}.mp4\"\n",
    "            saved_filepath = os.path.join(save_dir, filename)\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            video_writer = cv2.VideoWriter(\n",
    "                saved_filepath, fourcc, fps, (frame_width, frame_height))\n",
    "            print(f\"\\n[éŒ²ç”»é–‹å§‹] -> ä¿å­˜å…ˆ: {saved_filepath}\")\n",
    "\n",
    "        elif key == ord('e') and is_recording:\n",
    "            print(\"[éŒ²ç”»çµ‚äº†]\")\n",
    "            is_recording = False\n",
    "            if video_writer:\n",
    "                video_writer.release()\n",
    "                video_writer = None\n",
    "            break\n",
    "\n",
    "        elif key == ord('q'):\n",
    "            print(\"ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚\")\n",
    "            if video_writer:\n",
    "                video_writer.release()\n",
    "            saved_filepath = None\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return saved_filepath\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# --- STEP 2: MediaPipeéª¨æ ¼åº§æ¨™æŠ½å‡º ---\n",
    "# ============================================================\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    keypoints = {}\n",
    "    keypoints[\"left_hand\"] = [{\"id\": i, \"x\": res.x, \"y\": res.y, \"z\": res.z}\n",
    "                              for i, res in enumerate(results.left_hand_landmarks.landmark)] if results.left_hand_landmarks else []\n",
    "    keypoints[\"right_hand\"] = [{\"id\": i, \"x\": res.x, \"y\": res.y, \"z\": res.z}\n",
    "                               for i, res in enumerate(results.right_hand_landmarks.landmark)] if results.right_hand_landmarks else []\n",
    "    return keypoints\n",
    "\n",
    "\n",
    "def extract_landmarks_from_video(video_path, json_output_dir_base, sequence_length):\n",
    "    print(\"\\n--- [STEP 2] MediaPipeã«ã‚ˆã‚‹éª¨æ ¼åº§æ¨™ã®æŠ½å‡º ---\")\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"ã‚¨ãƒ©ãƒ¼: å‹•ç”»ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {video_path}\")\n",
    "        return None\n",
    "\n",
    "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    video_json_dir = os.path.join(json_output_dir_base, video_name)\n",
    "    os.makedirs(video_json_dir, exist_ok=True)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    print(f\"å‡¦ç†ä¸­ã®å‹•ç”»: {video_path}\")\n",
    "\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        for frame_num in range(sequence_length):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"è­¦å‘Š: ãƒ•ãƒ¬ãƒ¼ãƒ ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚\")\n",
    "                break\n",
    "\n",
    "            _, results = mediapipe_detection(frame, holistic)\n",
    "            keypoints = extract_keypoints(results)\n",
    "\n",
    "            with open(os.path.join(video_json_dir, f\"{frame_num}.json\"), 'w') as f:\n",
    "                json.dump(keypoints, f, indent=4)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸ: {video_json_dir}\")\n",
    "    return video_json_dir\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# --- STEP 3: JSONâ†’TXTå¤‰æ› ---\n",
    "# ============================================================\n",
    "def convert_json_to_txt(json_dir, txt_output_dir):\n",
    "    print(\"\\n--- [STEP 3] JSONã‹ã‚‰å˜ä¸€TXTãƒ•ã‚¡ã‚¤ãƒ«ã¸å¤‰æ› ---\")\n",
    "    os.makedirs(txt_output_dir, exist_ok=True)\n",
    "    video_name = os.path.basename(json_dir)\n",
    "    output_path = os.path.join(txt_output_dir, f\"{video_name}.txt\")\n",
    "\n",
    "    NUM_KEYPOINTS_PER_HAND = 21\n",
    "    json_files = sorted(glob.glob(os.path.join(json_dir, \"*.json\")),\n",
    "                        key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))\n",
    "\n",
    "    frames_data = []\n",
    "    for jf in json_files:\n",
    "        with open(jf) as f:\n",
    "            data = json.load(f)\n",
    "        frame_data = []\n",
    "\n",
    "        for hand in [\"left_hand\", \"right_hand\"]:\n",
    "            if data.get(hand):\n",
    "                for kp in data[hand]:\n",
    "                    frame_data.extend([kp[\"x\"], kp[\"y\"]])\n",
    "            else:\n",
    "                frame_data.extend([0.0] * (NUM_KEYPOINTS_PER_HAND * 2))\n",
    "\n",
    "        frames_data.append(frame_data)\n",
    "\n",
    "    with open(output_path, \"w\") as out:\n",
    "        for frame in frames_data:\n",
    "            out.write(\",\".join(map(str, frame)) + \"\\n\")\n",
    "\n",
    "    print(f\"TXTãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®å¤‰æ›ãŒå®Œäº†ã—ã¾ã—ãŸ: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# --- STEP 4: X_val.txtç”Ÿæˆ ---\n",
    "# ============================================================\n",
    "def prepare_data_for_lstm(source_txt_path, final_output_path, num_steps):\n",
    "    print(\"\\n--- [STEP 4] æ¨è«–ç”¨ãƒ‡ãƒ¼ã‚¿(X_val.txt)ã®ä½œæˆ ---\")\n",
    "    df = pd.read_csv(source_txt_path, header=None)\n",
    "    df.interpolate(inplace=True)\n",
    "    df.fillna(0.0, inplace=True)\n",
    "\n",
    "    num_frames = len(df)\n",
    "    if num_frames < num_steps:\n",
    "        print(\"ã‚¨ãƒ©ãƒ¼: ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚\")\n",
    "        return False\n",
    "\n",
    "    sequences = [df.iloc[:num_steps].values]\n",
    "    os.makedirs(os.path.dirname(final_output_path), exist_ok=True)\n",
    "    with open(final_output_path, 'w') as f:\n",
    "        for seq in sequences:\n",
    "            np.savetxt(f, seq, delimiter=',')\n",
    "\n",
    "    print(f\"1å€‹ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ç”Ÿæˆã—ã€æ¨è«–ç”¨ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: {final_output_path}\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# --- STEP 5: æ¨è«–ï¼ˆä¿®æ­£ç‰ˆï¼‰ ---\n",
    "# ============================================================\n",
    "# ============================================================\n",
    "# --- STEP 5: æ¨è«–ï¼ˆä¿®æ­£ç‰ˆï¼‰ ---\n",
    "# ============================================================\n",
    "def run_inference(x_val_path, model_path, labels, n_steps, n_hidden):\n",
    "    print(\"\\n--- [STEP 5] LSTMãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹æ¨è«– ---\")\n",
    "\n",
    "    try:\n",
    "        # --- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ---\n",
    "        X_ = np.loadtxt(x_val_path, delimiter=',', dtype=np.float32)\n",
    "        X_val = np.expand_dims(X_, axis=0)  # shape: (1, frameæ•°, featureæ•°)\n",
    "        n_input = X_val.shape[2]\n",
    "        n_classes = len(labels)\n",
    "\n",
    "        # --- ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆæ§‹é€ ã”ã¨ï¼‰ ---\n",
    "        model = tf.keras.models.load_model(\n",
    "            model_path,\n",
    "            custom_objects={'LSTM_RNN': LSTM_RNN}\n",
    "        )\n",
    "\n",
    "        print(f\"ãƒ¢ãƒ‡ãƒ«ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {model_path}\")\n",
    "        model.summary()\n",
    "\n",
    "        # --- æ¨è«– ---\n",
    "        preds = model.predict(X_val)\n",
    "        pred_index = np.argmax(preds, axis=1)[0]\n",
    "        pred_label = labels[pred_index]\n",
    "\n",
    "        print(f\"\\n--- [æœ€çµ‚æ¨è«–çµæœ] ---\")\n",
    "        print(f\"ğŸ‘‰ éŒ²ç”»ã•ã‚ŒãŸæ‰‹è©±ã®å‹•ä½œã¯ã€ {pred_label} ã€ã§ã™ã€‚\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã¾ãŸã¯æ¨è«–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "    # æ¨è«–\n",
    "    try:\n",
    "        preds = model.predict(X_val)\n",
    "        pred_index = np.argmax(preds, axis=1)[0]\n",
    "        pred_label = labels[pred_index]\n",
    "        print(f\"\\n--- [æœ€çµ‚æ¨è«–çµæœ] ---\")\n",
    "        print(f\"ğŸ‘‰ éŒ²ç”»ã•ã‚ŒãŸæ‰‹è©±ã®å‹•ä½œã¯ã€ {pred_label} ã€ã§ã™ã€‚\")\n",
    "    except Exception as e:\n",
    "        print(f\"æ¨è«–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# --- å®Ÿè¡Œéƒ¨ ---\n",
    "# ============================================================\n",
    "if __name__ == '__main__':\n",
    "    video_dir = os.path.join(BASE_OUTPUT_DIR, \"recorded_videos\")\n",
    "    json_dir = os.path.join(BASE_OUTPUT_DIR, \"json_output\")\n",
    "    txt_dir = os.path.join(BASE_OUTPUT_DIR, \"txt_converted\")\n",
    "    final_dir = os.path.join(BASE_OUTPUT_DIR, \"final_input\")\n",
    "    x_val_path = os.path.join(final_dir, \"X_val.txt\")\n",
    "\n",
    "    for d in [video_dir, json_dir, txt_dir, final_dir]:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    recorded = record_video(video_dir, SEQUENCE_LENGTH)\n",
    "    if recorded:\n",
    "        json_path = extract_landmarks_from_video(recorded, json_dir, SEQUENCE_LENGTH)\n",
    "        txt_path = convert_json_to_txt(json_path, txt_dir)\n",
    "        if prepare_data_for_lstm(txt_path, x_val_path, SEQUENCE_LENGTH):\n",
    "            run_inference(x_val_path, MODEL_PATH, LABELS, SEQUENCE_LENGTH, N_HIDDEN)\n",
    "    else:\n",
    "        print(\"éŒ²ç”»ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸã€‚\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
