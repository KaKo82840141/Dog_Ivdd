{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2840733c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF: 2.19.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy.random' has no attribute 'set_seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTF:\u001b[39m\u001b[33m\"\u001b[39m, tf.__version__)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m tf.random.set_seed(\u001b[32m42\u001b[39m); np.random.set_seed(\u001b[32m42\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# ====== パス設定（必要に応じて変更） ======\u001b[39;00m\n\u001b[32m     39\u001b[39m TRAIN_ROOT = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mkanno\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mvscode\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mRNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mRNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m: module 'numpy.random' has no attribute 'set_seed'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Train (3 keypoints: left back paw, right back paw, tail set)\n",
    "- NORMALIZE_MODE = \"zscore\" (train1相当) or \"tail_minmax\" (train2相当)\n",
    "- outputs:\n",
    "  - data/train/fig/curve_<TS>.png\n",
    "  - data/train/val_misclassified/val_misclassified_<TS>.csv\n",
    "  - data/train/train1_model or train2_model / ivdd_lstm_3kp_<TS>_best.keras / _final.keras\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"-1\")\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "\n",
    "import re, glob\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ====== パス設定（あなたのリポジトリ最上位に合わせて修正可）======\n",
    "REPO_ROOT = r\"C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\"\n",
    "TRAIN_ROOT = os.path.join(REPO_ROOT, \"data\", \"train\")\n",
    "TEST_ROOT  = os.path.join(REPO_ROOT, \"data\", \"test\")\n",
    "\n",
    "TRAIN_CSV_DIR = os.path.join(TRAIN_ROOT, \"train_csv\")\n",
    "TRAIN_GLOB    = os.path.join(TRAIN_CSV_DIR, \"*.csv\")\n",
    "FIG_DIR       = os.path.join(TRAIN_ROOT, \"fig\")\n",
    "VALERR_DIR    = os.path.join(TRAIN_ROOT, \"val_misclassified\")\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "os.makedirs(VALERR_DIR, exist_ok=True)\n",
    "\n",
    "# ====== 実行設定 ======\n",
    "RUN_ID = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# 正規化選択: \"zscore\" (train1相当) / \"tail_minmax\" (train2相当)\n",
    "NORMALIZE_MODE = \"tail_minmax\"   # ←必要に応じて \"zscore\" に変更\n",
    "\n",
    "# モデル保存先はモードに応じて\n",
    "MODEL_DIR = os.path.join(TRAIN_ROOT, \"train1_model\" if NORMALIZE_MODE==\"zscore\" else \"train2_model\")\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# ====== データ設定 ======\n",
    "KEYPOINTS = [\"left back paw\", \"right back paw\", \"tail set\"]\n",
    "USE_LIKELIHOOD      = False\n",
    "MIN_KEEP_LIKELIHOOD = 0.6\n",
    "SEQ_LEN  = 60\n",
    "STRIDE   = 30\n",
    "DIMS     = 6  # 3keypoints × (x,y)\n",
    "\n",
    "# ====== 学習ハイパラ ======\n",
    "N_HIDDEN   = 30\n",
    "BATCH_SIZE = 30\n",
    "EPOCHS     = 60\n",
    "LR         = 1e-4\n",
    "L2_LAMBDA  = 1e-4\n",
    "\n",
    "VAL_SPLIT_BY_FILE = True\n",
    "\n",
    "# ラベル: 0=normal, 1=ivdd\n",
    "CLASS_NAMES  = [\"normal\", \"ivdd\"]\n",
    "NAME2IDX     = {\"normal\":0, \"ivdd\":1}\n",
    "\n",
    "# ====== ユーティリティ ======\n",
    "def _norm_name(s: str) -> str:\n",
    "    return \"\".join(ch for ch in s.lower() if ch not in \" _-\")\n",
    "\n",
    "def _resolve_keypoints(all_bodyparts, requested):\n",
    "    norm2orig = {}\n",
    "    for bp in all_bodyparts:\n",
    "        k = _norm_name(bp)\n",
    "        if k not in norm2orig:\n",
    "            norm2orig[k] = bp\n",
    "    resolved, missing = [], []\n",
    "    for req in requested:\n",
    "        k = _norm_name(req)\n",
    "        if k in norm2orig:\n",
    "            resolved.append(norm2orig[k])\n",
    "        else:\n",
    "            missing.append(req)\n",
    "    if missing:\n",
    "        raise ValueError(f\"指定KPが見つかりません: {missing}\\n利用可能: {all_bodyparts}\")\n",
    "    return resolved\n",
    "\n",
    "def infer_label_from_filename(path: str) -> int:\n",
    "    \"\"\"\n",
    "    'ivdd', 'ivdd1','ivdd2',... を ivdd と判定。'normal' は normal。\n",
    "    ファイル名→未決なら先頭トークン→親ディレクトリの順で決める。\n",
    "    \"\"\"\n",
    "    name = os.path.basename(path).lower()\n",
    "    stem = os.path.splitext(name)[0]\n",
    "    tokens = [t for t in re.split(r'[^a-z0-9]+', stem) if t]\n",
    "    token_set = set(tokens)\n",
    "\n",
    "    has_ivdd = any(t == \"ivdd\" or t.startswith(\"ivdd\") for t in tokens)\n",
    "    has_normal = \"normal\" in token_set\n",
    "\n",
    "    if has_ivdd and not has_normal:\n",
    "        return NAME2IDX[\"ivdd\"]\n",
    "    if has_normal and not has_ivdd:\n",
    "        return NAME2IDX[\"normal\"]\n",
    "\n",
    "    if tokens and tokens[0] in NAME2IDX:\n",
    "        return NAME2IDX[tokens[0]]\n",
    "\n",
    "    parent_tokens = [t for t in re.split(r'[^a-z0-9]+', os.path.dirname(path).lower()) if t]\n",
    "    p_has_ivdd   = any(t == \"ivdd\" or t.startswith(\"ivdd\") for t in parent_tokens)\n",
    "    p_has_normal = \"normal\" in set(parent_tokens)\n",
    "    if p_has_ivdd and not p_has_normal:\n",
    "        return NAME2IDX[\"ivdd\"]\n",
    "    if p_has_normal and not p_has_ivdd:\n",
    "        return NAME2IDX[\"normal\"]\n",
    "\n",
    "    raise ValueError(f\"ラベル不明: {name}（'ivdd' か 'normal' を含めてください）\")\n",
    "\n",
    "def read_dlc_3kp_xy(csv_path: str, keypoints, use_likelihood=True, min_keep_likelihood=0.6):\n",
    "    df = pd.read_csv(csv_path, header=[0,1,2], index_col=0)\n",
    "    bodyparts = list({bp for (_, bp, _) in df.columns})\n",
    "    use_kps = _resolve_keypoints(bodyparts, keypoints)\n",
    "\n",
    "    cols = {}\n",
    "    for bp in use_kps:\n",
    "        cols[f\"{bp}_x\"] = df.xs((bp, \"x\"), level=[1,2], axis=1)\n",
    "        cols[f\"{bp}_y\"] = df.xs((bp, \"y\"), level=[1,2], axis=1)\n",
    "    X_df = pd.concat(cols.values(), axis=1)\n",
    "    X_df.columns = list(cols.keys())\n",
    "\n",
    "    if use_likelihood:\n",
    "        for bp in use_kps:\n",
    "            try:\n",
    "                lcol = df.xs((bp, \"likelihood\"), level=[1,2], axis=1).values.flatten()\n",
    "                low = lcol < min_keep_likelihood\n",
    "                for c in [f\"{bp}_x\", f\"{bp}_y\"]:\n",
    "                    v = X_df[c].values\n",
    "                    v[low] = np.nan\n",
    "                    X_df[c] = v\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "    X_df = X_df.interpolate(method=\"linear\", limit_direction=\"both\", axis=0)\n",
    "    X_df = X_df.bfill().ffill().fillna(0.0)\n",
    "    return X_df.values.astype(np.float32), use_kps  # (T,6)\n",
    "\n",
    "def zscore_per_file(X: np.ndarray, eps=1e-6) -> np.ndarray:\n",
    "    mu = X.mean(axis=0, keepdims=True)\n",
    "    sd = X.std(axis=0, keepdims=True)\n",
    "    return (X - mu) / (sd + eps)\n",
    "\n",
    "def normalize_tailset_minmax(X: np.ndarray, used_kps: list[str], ref_name=\"tail set\", eps=1e-6) -> np.ndarray:\n",
    "    low = [s.lower() for s in used_kps]\n",
    "    if ref_name.lower() not in low:\n",
    "        raise ValueError(f\"'{ref_name}' が used_kps にありません: {used_kps}\")\n",
    "    r = low.index(ref_name.lower())\n",
    "\n",
    "    Xc = X.copy()\n",
    "    cx, cy = X[:, 2*r], X[:, 2*r+1]\n",
    "    for i in range(len(used_kps)):\n",
    "        Xc[:, 2*i]   -= cx\n",
    "        Xc[:, 2*i+1] -= cy\n",
    "\n",
    "    mn = Xc.min(axis=0, keepdims=True)\n",
    "    mx = Xc.max(axis=0, keepdims=True)\n",
    "    return (Xc - mn) / (mx - mn + eps)\n",
    "\n",
    "def make_windows(X: np.ndarray, seq_len: int, stride: int):\n",
    "    n = X.shape[0]\n",
    "    if n < seq_len:\n",
    "        return np.empty((0, seq_len, X.shape[1]), dtype=X.dtype), []\n",
    "    starts = list(range(0, n - seq_len + 1, stride))\n",
    "    Xw = np.stack([X[s:s+seq_len] for s in starts], axis=0)\n",
    "    return Xw, starts\n",
    "\n",
    "def build_dataset(csv_paths, seq_len=SEQ_LEN, stride=STRIDE):\n",
    "    Xs, ys, fids, starts_all = [], [], [], []\n",
    "    used_kps_any = None\n",
    "    for p in csv_paths:\n",
    "        y = infer_label_from_filename(p)\n",
    "        X_raw, used_kps = read_dlc_3kp_xy(p, KEYPOINTS, USE_LIKELIHOOD, MIN_KEEP_LIKELIHOOD)\n",
    "        if used_kps_any is None:\n",
    "            used_kps_any = used_kps\n",
    "        if X_raw.shape[1] != DIMS:\n",
    "            raise ValueError(f\"{os.path.basename(p)}: 次元{X_raw.shape[1]} != 期待{DIMS}\")\n",
    "\n",
    "        if NORMALIZE_MODE == \"zscore\":\n",
    "            Xn = zscore_per_file(X_raw)\n",
    "        elif NORMALIZE_MODE == \"tail_minmax\":\n",
    "            Xn = normalize_tailset_minmax(X_raw, used_kps)\n",
    "        else:\n",
    "            raise ValueError(\"NORMALIZE_MODE は 'zscore' or 'tail_minmax'\")\n",
    "\n",
    "        Xw, sidx = make_windows(Xn, seq_len, stride)\n",
    "        if Xw.shape[0] == 0:\n",
    "            print(f\"[WARN] {os.path.basename(p)}: フレーム不足でスキップ\")\n",
    "            continue\n",
    "\n",
    "        Xs.append(Xw)\n",
    "        ys.append(np.full((Xw.shape[0],), y, dtype=np.int64))\n",
    "        fids.extend([os.path.basename(p)]*Xw.shape[0])\n",
    "        starts_all.extend(sidx)\n",
    "\n",
    "    if not Xs:\n",
    "        raise RuntimeError(\"データが作れませんでした。CSVと命名（ivdd*/normal*）を確認してください。\")\n",
    "    X = np.concatenate(Xs, axis=0)\n",
    "    y = np.concatenate(ys, axis=0)\n",
    "    fids = np.array(fids)\n",
    "    starts_all = np.array(starts_all)\n",
    "    print(f\"[INFO] 使用キーポイント: {used_kps_any}\")\n",
    "    return X, y, fids, starts_all\n",
    "\n",
    "def build_model(seq_len: int, dims: int, n_hidden: int, l2_lambda: float=1e-4) -> keras.Model:\n",
    "    reg = keras.regularizers.l2(l2_lambda)\n",
    "    inp = keras.Input(shape=(seq_len, dims))\n",
    "    x   = keras.layers.TimeDistributed(keras.layers.Dense(n_hidden, activation=\"relu\", kernel_regularizer=reg))(inp)\n",
    "    x   = keras.layers.LSTM(n_hidden, return_sequences=True, kernel_regularizer=reg)(x)\n",
    "    x   = keras.layers.LSTM(n_hidden, kernel_regularizer=reg)(x)\n",
    "    out = keras.layers.Dense(1, kernel_regularizer=reg)(x)  # logits\n",
    "    return keras.Model(inp, out, name=\"ivdd_lstm_3kp\")\n",
    "\n",
    "# ====== データ読み込み ======\n",
    "csv_files = sorted(glob.glob(TRAIN_GLOB))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"学習CSVが見つかりません: {TRAIN_GLOB}\")\n",
    "\n",
    "X, y, file_ids, starts = build_dataset(csv_files, SEQ_LEN, STRIDE)\n",
    "print(\"X:\", X.shape, \"y:\", y.shape, \"files:\", len(np.unique(file_ids)))\n",
    "\n",
    "# ====== 分割 ======\n",
    "if VAL_SPLIT_BY_FILE:\n",
    "    uniq = np.unique(file_ids)\n",
    "    tr_files, va_files = train_test_split(uniq, test_size=0.2, random_state=42, shuffle=True)\n",
    "    tr_mask = np.isin(file_ids, tr_files)\n",
    "    va_mask = np.isin(file_ids, va_files)\n",
    "    X_train, y_train, starts_tr = X[tr_mask], y[tr_mask], starts[tr_mask]\n",
    "    X_val,   y_val,   starts_va = X[va_mask], y[va_mask], starts[va_mask]\n",
    "    file_ids_tr, file_ids_va = file_ids[tr_mask], file_ids[va_mask]\n",
    "else:\n",
    "    X_train, X_val, y_train, y_val, starts_tr, starts_va, file_ids_tr, file_ids_va = train_test_split(\n",
    "        X, y, starts, file_ids, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "print(\"train:\", X_train.shape, \"val:\", X_val.shape)\n",
    "\n",
    "# ====== class_weight ======\n",
    "if set(np.unique(y_train)) == {0,1}:\n",
    "    cw = compute_class_weight(\"balanced\", classes=np.array([0,1]), y=y_train)\n",
    "    class_weight = {0: float(cw[0]), 1: float(cw[1])}\n",
    "else:\n",
    "    class_weight = None\n",
    "print(\"class_weight:\", class_weight)\n",
    "\n",
    "# ====== モデル ======\n",
    "model = build_model(SEQ_LEN, DIMS, N_HIDDEN, L2_LAMBDA)\n",
    "opt = keras.optimizers.Adam(learning_rate=LR)\n",
    "model.compile(optimizer=opt, loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\")])\n",
    "model.summary()\n",
    "\n",
    "best_path  = os.path.join(MODEL_DIR, f\"ivdd_lstm_3kp_{RUN_ID}_best.keras\")\n",
    "final_path = os.path.join(MODEL_DIR, f\"ivdd_lstm_3kp_{RUN_ID}_final.keras\")\n",
    "\n",
    "cbs = [\n",
    "    keras.callbacks.ModelCheckpoint(best_path, monitor=\"val_accuracy\", mode=\"max\",\n",
    "                                    save_best_only=True, verbose=1),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", mode=\"min\",\n",
    "                                      factor=0.5, patience=5, min_lr=1e-5, verbose=1),\n",
    "]\n",
    "\n",
    "hist = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "    class_weight=class_weight, callbacks=cbs, verbose=1\n",
    ")\n",
    "\n",
    "# ====== 学習曲線（1枚） ======\n",
    "curve_png = os.path.join(FIG_DIR, f\"curve_{RUN_ID}.png\")\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1); plt.plot(hist.history[\"loss\"]); plt.plot(hist.history[\"val_loss\"]); plt.title(\"Loss\"); plt.legend([\"train\",\"val\"])\n",
    "plt.subplot(1,2,2); plt.plot(hist.history[\"accuracy\"]); plt.plot(hist.history[\"val_accuracy\"]); plt.title(\"Accuracy\"); plt.legend([\"train\",\"val\"])\n",
    "plt.tight_layout(); plt.savefig(curve_png, dpi=150); plt.close()\n",
    "print(\"[INFO] saved curve:\", curve_png)\n",
    "\n",
    "# ====== 検証(ウィンドウ)＆誤分類CSV ======\n",
    "logits_val = model.predict(X_val, batch_size=64)\n",
    "p_ivdd = tf.math.sigmoid(logits_val).numpy().ravel()\n",
    "y_pred = (p_ivdd >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n[Window-level] classification_report:\")\n",
    "print(classification_report(y_val, y_pred, target_names=CLASS_NAMES, digits=4))\n",
    "print(\"[Window-level] confusion matrix:\\n\", confusion_matrix(y_val, y_pred))\n",
    "\n",
    "# 誤分類ウィンドウの保存\n",
    "miss_mask = (y_val != y_pred)\n",
    "df_miss = pd.DataFrame({\n",
    "    \"file\": file_ids_va[miss_mask],\n",
    "    \"start\": starts_va[miss_mask],\n",
    "    \"true\": [CLASS_NAMES[t] for t in y_val[miss_mask]],\n",
    "    \"pred\": [CLASS_NAMES[p] for p in y_pred[miss_mask]],\n",
    "    \"p_ivdd\": p_ivdd[miss_mask],\n",
    "    \"p_normal\": 1.0 - p_ivdd[miss_mask],\n",
    "})\n",
    "miss_csv = os.path.join(VALERR_DIR, f\"val_misclassified_{RUN_ID}.csv\")\n",
    "df_miss.to_csv(miss_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[INFO] saved misclassified csv:\", miss_csv)\n",
    "\n",
    "# ====== 最終保存 ======\n",
    "model.save(final_path)\n",
    "print(\"[INFO] saved model:\", final_path)\n",
    "print(\"[DONE] 3KP training complete:\", NORMALIZE_MODE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLya",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
