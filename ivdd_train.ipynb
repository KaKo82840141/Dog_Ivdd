{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdc0d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF: 2.19.0\n",
      "[WARN] ivdd1_case1DLC_resnet50_IvddOct30shuffle1_100000.csv: フレーム不足（60未満）でスキップ\n",
      "[INFO] 使用キーポイント実名: ['left back paw', 'right back paw', 'left front paw', 'right front paw', 'tail set']\n",
      "X: (1236, 60, 10) y: (1236,) files: 170\n",
      "train: (979, 60, 10) val: (257, 60, 10)\n",
      "class_weight: {0: 1.1049661399548534, 1: 0.9132462686567164}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"ivdd_lstm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"ivdd_lstm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ td_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ logits (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input (\u001b[38;5;33mInputLayer\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m10\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ td_dense (\u001b[38;5;33mTimeDistributed\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m30\u001b[0m)         │           \u001b[38;5;34m330\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm1 (\u001b[38;5;33mLSTM\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m30\u001b[0m)         │         \u001b[38;5;34m7,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm2 (\u001b[38;5;33mLSTM\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)             │         \u001b[38;5;34m7,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ logits (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m31\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,001</span> (58.60 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m15,001\u001b[0m (58.60 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,001</span> (58.60 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m15,001\u001b[0m (58.60 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4271 - loss: 0.7014\n",
      "Epoch 1: val_accuracy improved from -inf to 0.38132, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-092956_best.keras\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.4279 - loss: 0.7014 - val_accuracy: 0.3813 - val_loss: 0.7242 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4271 - loss: 0.7010\n",
      "Epoch 2: val_accuracy did not improve from 0.38132\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4279 - loss: 0.7010 - val_accuracy: 0.3813 - val_loss: 0.7038 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4306 - loss: 0.6890\n",
      "Epoch 3: val_accuracy improved from 0.38132 to 0.40078, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-092956_best.keras\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4316 - loss: 0.6890 - val_accuracy: 0.4008 - val_loss: 0.7005 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5340 - loss: 0.6729\n",
      "Epoch 4: val_accuracy did not improve from 0.40078\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5335 - loss: 0.6730 - val_accuracy: 0.3891 - val_loss: 0.6999 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5304 - loss: 0.6582\n",
      "Epoch 5: val_accuracy improved from 0.40078 to 0.47860, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-092956_best.keras\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5312 - loss: 0.6582 - val_accuracy: 0.4786 - val_loss: 0.6775 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5683 - loss: 0.6415\n",
      "Epoch 6: val_accuracy did not improve from 0.47860\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5681 - loss: 0.6417 - val_accuracy: 0.4436 - val_loss: 0.6896 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5821 - loss: 0.6282\n",
      "Epoch 7: val_accuracy improved from 0.47860 to 0.48638, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-092956_best.keras\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5827 - loss: 0.6282 - val_accuracy: 0.4864 - val_loss: 0.7073 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m28/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5862 - loss: 0.6289\n",
      "Epoch 8: val_accuracy did not improve from 0.48638\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5865 - loss: 0.6289 - val_accuracy: 0.4864 - val_loss: 0.6982 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6005 - loss: 0.6129\n",
      "Epoch 9: val_accuracy improved from 0.48638 to 0.56031, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-092956_best.keras\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6009 - loss: 0.6127 - val_accuracy: 0.5603 - val_loss: 0.6800 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m27/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6596 - loss: 0.6127\n",
      "Epoch 10: val_accuracy did not improve from 0.56031\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6540 - loss: 0.6148 - val_accuracy: 0.4008 - val_loss: 0.7152 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m27/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5986 - loss: 0.6101\n",
      "Epoch 11: val_accuracy did not improve from 0.56031\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6021 - loss: 0.6088 - val_accuracy: 0.5292 - val_loss: 0.6875 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m30/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6276 - loss: 0.5873\n",
      "Epoch 12: val_accuracy did not improve from 0.56031\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6281 - loss: 0.5868 - val_accuracy: 0.5486 - val_loss: 0.7070 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m27/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6378 - loss: 0.5690\n",
      "Epoch 13: val_accuracy did not improve from 0.56031\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6406 - loss: 0.5683 - val_accuracy: 0.5409 - val_loss: 0.7301 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m27/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6641 - loss: 0.5637\n",
      "Epoch 14: val_accuracy did not improve from 0.56031\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6658 - loss: 0.5627 - val_accuracy: 0.5447 - val_loss: 0.7310 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m27/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6734 - loss: 0.5540\n",
      "Epoch 15: val_accuracy did not improve from 0.56031\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6739 - loss: 0.5536 - val_accuracy: 0.5447 - val_loss: 0.7292 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m27/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6700 - loss: 0.5458\n",
      "Epoch 16: val_accuracy improved from 0.56031 to 0.59533, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-092956_best.keras\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6744 - loss: 0.5447 - val_accuracy: 0.5953 - val_loss: 0.7189 - learning_rate: 2.5000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m26/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6907 - loss: 0.5293\n",
      "Epoch 17: val_accuracy did not improve from 0.59533\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6924 - loss: 0.5294 - val_accuracy: 0.5914 - val_loss: 0.7191 - learning_rate: 2.5000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m30/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6943 - loss: 0.5268\n",
      "Epoch 18: val_accuracy improved from 0.59533 to 0.60311, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-092956_best.keras\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6949 - loss: 0.5265 - val_accuracy: 0.6031 - val_loss: 0.7317 - learning_rate: 2.5000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m28/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6992 - loss: 0.5286\n",
      "Epoch 19: val_accuracy did not improve from 0.60311\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7011 - loss: 0.5278 - val_accuracy: 0.5875 - val_loss: 0.7365 - learning_rate: 2.5000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7025 - loss: 0.5214\n",
      "Epoch 20: val_accuracy did not improve from 0.60311\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7028 - loss: 0.5212 - val_accuracy: 0.5759 - val_loss: 0.7392 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m27/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6997 - loss: 0.5235\n",
      "Epoch 21: val_accuracy did not improve from 0.60311\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7020 - loss: 0.5227 - val_accuracy: 0.5837 - val_loss: 0.7398 - learning_rate: 1.2500e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m27/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7107 - loss: 0.5152\n",
      "Epoch 22: val_accuracy did not improve from 0.60311\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7109 - loss: 0.5151 - val_accuracy: 0.5681 - val_loss: 0.7559 - learning_rate: 1.2500e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7040 - loss: 0.5221\n",
      "Epoch 23: val_accuracy did not improve from 0.60311\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7044 - loss: 0.5219 - val_accuracy: 0.5759 - val_loss: 0.7539 - learning_rate: 1.2500e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7063 - loss: 0.5190\n",
      "Epoch 24: val_accuracy did not improve from 0.60311\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7066 - loss: 0.5188 - val_accuracy: 0.5798 - val_loss: 0.7553 - learning_rate: 1.2500e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m27/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7049 - loss: 0.5179\n",
      "Epoch 25: val_accuracy did not improve from 0.60311\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7074 - loss: 0.5172 - val_accuracy: 0.5603 - val_loss: 0.7622 - learning_rate: 1.2500e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6965 - loss: 0.5222\n",
      "Epoch 26: val_accuracy did not improve from 0.60311\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6974 - loss: 0.5220 - val_accuracy: 0.5681 - val_loss: 0.7657 - learning_rate: 6.2500e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m28/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6756 - loss: 0.5240\n",
      "Epoch 27: val_accuracy did not improve from 0.60311\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6806 - loss: 0.5226 - val_accuracy: 0.5759 - val_loss: 0.7593 - learning_rate: 6.2500e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m29/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6900 - loss: 0.5162\n",
      "Epoch 28: val_accuracy did not improve from 0.60311\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6930 - loss: 0.5155 - val_accuracy: 0.5720 - val_loss: 0.7685 - learning_rate: 6.2500e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6901 - loss: 0.5174\n",
      "Epoch 29: val_accuracy did not improve from 0.60311\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6911 - loss: 0.5170 - val_accuracy: 0.5720 - val_loss: 0.7611 - learning_rate: 6.2500e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6939 - loss: 0.5111\n",
      "Epoch 30: val_accuracy did not improve from 0.60311\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6948 - loss: 0.5109 - val_accuracy: 0.5720 - val_loss: 0.7621 - learning_rate: 6.2500e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6932 - loss: 0.5092\n",
      "Epoch 31: val_accuracy did not improve from 0.60311\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6942 - loss: 0.5090 - val_accuracy: 0.5914 - val_loss: 0.7524 - learning_rate: 3.1250e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7184 - loss: 0.5039\n",
      "Epoch 32: val_accuracy did not improve from 0.60311\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7188 - loss: 0.5037 - val_accuracy: 0.5914 - val_loss: 0.7557 - learning_rate: 3.1250e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m28/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7170 - loss: 0.5044\n",
      "Epoch 33: val_accuracy did not improve from 0.60311\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7195 - loss: 0.5037 - val_accuracy: 0.5875 - val_loss: 0.7563 - learning_rate: 3.1250e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m27/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7159 - loss: 0.5039\n",
      "Epoch 34: val_accuracy did not improve from 0.60311\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7189 - loss: 0.5032 - val_accuracy: 0.5914 - val_loss: 0.7574 - learning_rate: 3.1250e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7177 - loss: 0.5031\n",
      "Epoch 35: val_accuracy did not improve from 0.60311\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7183 - loss: 0.5029 - val_accuracy: 0.5992 - val_loss: 0.7582 - learning_rate: 3.1250e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m27/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7126 - loss: 0.5030\n",
      "Epoch 36: val_accuracy did not improve from 0.60311\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7156 - loss: 0.5023 - val_accuracy: 0.5953 - val_loss: 0.7550 - learning_rate: 1.5625e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m26/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7279 - loss: 0.5012\n",
      "Epoch 37: val_accuracy did not improve from 0.60311\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7300 - loss: 0.5006 - val_accuracy: 0.5992 - val_loss: 0.7559 - learning_rate: 1.5625e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7271 - loss: 0.5008\n",
      "Epoch 38: val_accuracy did not improve from 0.60311\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7275 - loss: 0.5006 - val_accuracy: 0.5992 - val_loss: 0.7562 - learning_rate: 1.5625e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m27/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7246 - loss: 0.5009\n",
      "Epoch 39: val_accuracy did not improve from 0.60311\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7267 - loss: 0.5003 - val_accuracy: 0.5953 - val_loss: 0.7566 - learning_rate: 1.5625e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7263 - loss: 0.5003\n",
      "Epoch 40: val_accuracy did not improve from 0.60311\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7267 - loss: 0.5001 - val_accuracy: 0.5914 - val_loss: 0.7569 - learning_rate: 1.5625e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m27/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7231 - loss: 0.5004\n",
      "Epoch 41: val_accuracy did not improve from 0.60311\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7251 - loss: 0.4998 - val_accuracy: 0.5953 - val_loss: 0.7562 - learning_rate: 1.0000e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m30/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7270 - loss: 0.4996\n",
      "Epoch 42: val_accuracy did not improve from 0.60311\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7276 - loss: 0.4993 - val_accuracy: 0.5953 - val_loss: 0.7564 - learning_rate: 1.0000e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m26/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7263 - loss: 0.4996\n",
      "Epoch 43: val_accuracy did not improve from 0.60311\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7287 - loss: 0.4991 - val_accuracy: 0.5953 - val_loss: 0.7566 - learning_rate: 1.0000e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7258 - loss: 0.4991\n",
      "Epoch 44: val_accuracy did not improve from 0.60311\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7262 - loss: 0.4990 - val_accuracy: 0.5953 - val_loss: 0.7568 - learning_rate: 1.0000e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7264 - loss: 0.4990\n",
      "Epoch 45: val_accuracy did not improve from 0.60311\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7268 - loss: 0.4988 - val_accuracy: 0.5953 - val_loss: 0.7570 - learning_rate: 1.0000e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m28/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7232 - loss: 0.4992\n",
      "Epoch 46: val_accuracy did not improve from 0.60311\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7251 - loss: 0.4987 - val_accuracy: 0.5953 - val_loss: 0.7572 - learning_rate: 1.0000e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m27/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7226 - loss: 0.4991\n",
      "Epoch 47: val_accuracy did not improve from 0.60311\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7251 - loss: 0.4985 - val_accuracy: 0.5953 - val_loss: 0.7574 - learning_rate: 1.0000e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7229 - loss: 0.4986\n",
      "Epoch 48: val_accuracy did not improve from 0.60311\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7234 - loss: 0.4984 - val_accuracy: 0.5953 - val_loss: 0.7576 - learning_rate: 1.0000e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7217 - loss: 0.4984\n",
      "Epoch 49: val_accuracy did not improve from 0.60311\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7221 - loss: 0.4982 - val_accuracy: 0.5914 - val_loss: 0.7577 - learning_rate: 1.0000e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m26/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7191 - loss: 0.4986\n",
      "Epoch 50: val_accuracy did not improve from 0.60311\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7221 - loss: 0.4981 - val_accuracy: 0.5914 - val_loss: 0.7579 - learning_rate: 1.0000e-05\n",
      "[INFO] saved figs: C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\fig\\loss_20251124-092956.png C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\fig\\accuracy_20251124-092956.png\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step\n",
      "\n",
      "[Window-level] classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal     0.4946    0.4694    0.4817        98\n",
      "        ivdd     0.6829    0.7044    0.6935       159\n",
      "\n",
      "    accuracy                         0.6148       257\n",
      "   macro avg     0.5888    0.5869    0.5876       257\n",
      "weighted avg     0.6111    0.6148    0.6127       257\n",
      "\n",
      "[Window-level] confusion matrix:\n",
      " [[ 46  52]\n",
      " [ 47 112]]\n",
      "[INFO] saved final model to: C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-092956_final.keras\n",
      "[INFO] saved params to: C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\train_params_20251124-092956.json\n",
      "[DONE] training complete.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Binary IVDD training (ivdd vs normal) from DeepLabCut CSV (3-level header)\n",
    "要件:\n",
    "- model/ フォルダに学習モデルを毎回ユニーク名で保存（best と final）\n",
    "- fig/ フォルダに学習曲線（loss/accuracy）を毎回ユニーク名で保存\n",
    "- 前処理: tail_set を原点に平行移動 → 各次元を min-max 正規化（ファイル単位）\n",
    "- EarlyStopping は使わない\n",
    "- ネットワーク構造: TimeDistributed(Dense->ReLU) → LSTM → LSTM → Dense(1 logits)（従来通り）\n",
    "\"\"\"\n",
    "\n",
    "# ===== 安定運用: GPU無効/ログ控えめ/スレッド抑制（必要なら） =====\n",
    "import os\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"-1\")\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "\n",
    "import glob, re, json, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TF:\", tf.__version__)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ========= パラメータ =========\n",
    "# 学習用CSVを置いたフォルダ（例: train ディレクトリ）\n",
    "DATA_DIR   = r\"C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\"\n",
    "CSV_GLOB   = os.path.join(DATA_DIR, \"*.csv\")\n",
    "\n",
    "# 保存先（要件）\n",
    "MODEL_DIR = os.path.join(DATA_DIR, \"model\")\n",
    "FIG_DIR   = os.path.join(DATA_DIR, \"fig\")\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "\n",
    "# ランごとユニークID\n",
    "RUN_ID = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# 指定の5点（順序がそのまま (x,y) の並び順になります）\n",
    "KEYPOINTS = [\n",
    "    \"left back paw\",\n",
    "    \"right back paw\",\n",
    "    \"left front paw\",\n",
    "    \"right front paw\",\n",
    "    \"tail set\",\n",
    "]\n",
    "\n",
    "# DLCのlikelihood閾値で低信頼を欠損扱いに\n",
    "USE_LIKELIHOOD      = False\n",
    "MIN_KEEP_LIKELIHOOD = 0.6\n",
    "\n",
    "# ウィンドウ設定（必要に応じて 60/30 などに変更）\n",
    "SEQ_LEN  = 60\n",
    "STRIDE   = 30\n",
    "\n",
    "# 次元・ネットワーク\n",
    "DIMS      = 10           # 5点×(x,y)\n",
    "N_HIDDEN  = 30\n",
    "N_CLASSES = 1            # バイナリ → ロジット1本\n",
    "\n",
    "# 学習ハイパパラメータ\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS     = 50\n",
    "LR         = 1e-3        # floatにしておくと ReduceLROnPlateau が適用可能\n",
    "L2_LAMBDA  = 1e-4\n",
    "\n",
    "# 分割設定\n",
    "VAL_SPLIT_BY_FILE = True   # ファイル単位で分割（時系列リーク防止）\n",
    "\n",
    "# ラベル表記（ivdd を positive=1 として扱います）\n",
    "CLASS_NAMES  = [\"normal\", \"ivdd\"]  # index 0=normal, 1=ivdd\n",
    "CLASS_TO_IDX = {\"normal\": 0, \"ivdd\": 1}\n",
    "\n",
    "# ========= ラベル推定（ファイル名から） =========\n",
    "def infer_label_from_filename(path: str) -> int:\n",
    "    \"\"\"\n",
    "    ファイル名からラベルを推定。\n",
    "    1) stem を非英数字で分割したトークンに 'ivdd' / 'normal' が厳密一致すれば採用\n",
    "    2) 未決なら先頭トークンが 'ivdd' / 'normal' なら採用\n",
    "    3) まだ未決なら親ディレクトリ名トークンを参照（片方のみ含む場合）\n",
    "    \"\"\"\n",
    "    name = os.path.basename(path).lower()\n",
    "    stem = os.path.splitext(name)[0]\n",
    "    tokens = [t for t in re.split(r'[^a-z0-9]+', stem) if t]\n",
    "    token_set = set(tokens)\n",
    "\n",
    "    has_ivdd   = ('ivdd' in token_set)\n",
    "    has_normal = ('normal' in token_set)\n",
    "\n",
    "    if has_ivdd and not has_normal:\n",
    "        return CLASS_TO_IDX[\"ivdd\"]\n",
    "    if has_normal and not has_ivdd:\n",
    "        return CLASS_TO_IDX[\"normal\"]\n",
    "\n",
    "    if tokens:\n",
    "        if tokens[0] in CLASS_TO_IDX:\n",
    "            return CLASS_TO_IDX[tokens[0]]\n",
    "\n",
    "    parent_tokens = [t for t in re.split(r'[^a-z0-9]+', os.path.dirname(path).lower()) if t]\n",
    "    pset = set(parent_tokens)\n",
    "    if ('ivdd' in pset) and ('normal' not in pset):\n",
    "        return CLASS_TO_IDX[\"ivdd\"]\n",
    "    if ('normal' in pset) and ('ivdd' not in pset):\n",
    "        return CLASS_TO_IDX[\"normal\"]\n",
    "\n",
    "    raise ValueError(\n",
    "        f\"ラベルを特定できません: {name} \"\n",
    "        f\"(推奨: ファイル名の先頭を 'ivdd_' または 'normal_' にしてください)\"\n",
    "    )\n",
    "\n",
    "# ========= 前処理（DLC 3 レベルヘッダ） =========\n",
    "def _norm_name(s: str) -> str:\n",
    "    return \"\".join(ch for ch in s.lower() if ch not in \" _-\")\n",
    "\n",
    "def _resolve_keypoints(all_bodyparts, requested):\n",
    "    norm2orig = {}\n",
    "    for bp in all_bodyparts:\n",
    "        k = _norm_name(bp)\n",
    "        if k not in norm2orig:\n",
    "            norm2orig[k] = bp\n",
    "    resolved, missing = [], []\n",
    "    for req in requested:\n",
    "        k = _norm_name(req)\n",
    "        if k in norm2orig:\n",
    "            resolved.append(norm2orig[k])\n",
    "        else:\n",
    "            missing.append(req)\n",
    "    if missing:\n",
    "        raise ValueError(f\"指定キーポイントがCSVで見つかりません: {missing}\\n利用可能: {all_bodyparts}\")\n",
    "    return resolved\n",
    "\n",
    "def read_dlc_5kp_xy(csv_path: str,\n",
    "                    keypoints,\n",
    "                    use_likelihood=True,\n",
    "                    min_keep_likelihood=0.6):\n",
    "    \"\"\"\n",
    "    (1) DLC CSV(3段ヘッダ)読込 → (2) 指定5点の (x,y) 抜き出し → (3) 低likelihoodをNaN\n",
    "       → (4) 線形補間 + bfill/ffill → 0埋め → (T,10) を返す\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, header=[0,1,2], index_col=0)\n",
    "    bodyparts = list({bp for (_, bp, _) in df.columns})\n",
    "    use_kps = _resolve_keypoints(bodyparts, keypoints)\n",
    "\n",
    "    cols = {}\n",
    "    for bp in use_kps:\n",
    "        cols[f\"{bp}_x\"] = df.xs((bp, \"x\"), level=[1,2], axis=1)\n",
    "        cols[f\"{bp}_y\"] = df.xs((bp, \"y\"), level=[1,2], axis=1)\n",
    "    X_df = pd.concat(cols.values(), axis=1)\n",
    "    X_df.columns = list(cols.keys())\n",
    "\n",
    "    if use_likelihood:\n",
    "        for bp in use_kps:\n",
    "            try:\n",
    "                lcol = df.xs((bp, \"likelihood\"), level=[1,2], axis=1).values.flatten()\n",
    "                low = lcol < min_keep_likelihood\n",
    "                for c in [f\"{bp}_x\", f\"{bp}_y\"]:\n",
    "                    v = X_df[c].values\n",
    "                    v[low] = np.nan\n",
    "                    X_df[c] = v\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "    X_df = X_df.interpolate(method=\"linear\", limit_direction=\"both\", axis=0)\n",
    "    # FutureWarning 回避: bfill()/ffill() を使う\n",
    "    X_df = X_df.bfill().ffill().fillna(0.0)\n",
    "\n",
    "    X = X_df.values.astype(np.float32)  # (T, 10)\n",
    "    return X, use_kps\n",
    "\n",
    "# === tail_set中心 + min-max 正規化（ファイル単位・各次元独立）===\n",
    "def normalize_tailset_minmax(\n",
    "    X: np.ndarray,         # (T,10)\n",
    "    used_kps: list[str],   # read_dlc_5kp_xy で実際に使われた実名\n",
    "    ref_name: str = \"tail set\",\n",
    "    eps: float = 1e-6\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    1) tail_setを原点に平行移動（各フレーム）\n",
    "    2) 平行移動後の各次元(全フレーム)で min-max 正規化（0〜1）\n",
    "       x' = (x - min) / (max - min + eps)\n",
    "    \"\"\"\n",
    "    low = [s.lower() for s in used_kps]\n",
    "    if ref_name.lower() not in low:\n",
    "        raise ValueError(f\"'{ref_name}' が used_kps に見つかりません: {used_kps}\")\n",
    "    ref_idx = low.index(ref_name.lower())\n",
    "\n",
    "    # 1) 原点平行移動\n",
    "    cx = X[:, 2*ref_idx]\n",
    "    cy = X[:, 2*ref_idx + 1]\n",
    "    Xc = X.copy()\n",
    "    for i in range(len(used_kps)):\n",
    "        Xc[:, 2*i]   -= cx\n",
    "        Xc[:, 2*i+1] -= cy\n",
    "\n",
    "    # 2) min-max 正規化（ファイル全体、各次元独立）\n",
    "    x_min = Xc.min(axis=0, keepdims=True)\n",
    "    x_max = Xc.max(axis=0, keepdims=True)\n",
    "    Xn = (Xc - x_min) / (x_max - x_min + eps)\n",
    "    return Xn\n",
    "\n",
    "def make_windows(X: np.ndarray, seq_len: int, stride: int) -> np.ndarray:\n",
    "    n = X.shape[0]\n",
    "    if n < seq_len:\n",
    "        return np.empty((0, seq_len, X.shape[1]), dtype=X.dtype)\n",
    "    starts = range(0, n - seq_len + 1, stride)\n",
    "    return np.stack([X[s:s+seq_len] for s in starts], axis=0)\n",
    "\n",
    "def build_dataset(csv_paths, seq_len=SEQ_LEN, stride=STRIDE):\n",
    "    \"\"\"\n",
    "    返り値:\n",
    "      X: (N, T, D)\n",
    "      y: (N,) 0/1\n",
    "      file_ids: (N,) 元ファイル名\n",
    "    \"\"\"\n",
    "    X_list, y_list, file_ids = [], [], []\n",
    "    used_kps_any = None\n",
    "\n",
    "    for p in csv_paths:\n",
    "        y_lab = infer_label_from_filename(p)  # 0=normal,1=ivdd\n",
    "\n",
    "        X_raw, used_kps = read_dlc_5kp_xy(\n",
    "            p, keypoints=KEYPOINTS,\n",
    "            use_likelihood=USE_LIKELIHOOD,\n",
    "            min_keep_likelihood=MIN_KEEP_LIKELIHOOD\n",
    "        )\n",
    "        if used_kps_any is None:\n",
    "            used_kps_any = used_kps\n",
    "\n",
    "        if X_raw.shape[1] != DIMS:\n",
    "            raise ValueError(f\"{os.path.basename(p)}: 次元 {X_raw.shape[1]} != 期待 {DIMS}\")\n",
    "\n",
    "        # ★ tail_set中心 + min-max 正規化\n",
    "        X_norm = normalize_tailset_minmax(X_raw, used_kps, ref_name=\"tail set\")\n",
    "\n",
    "        X_win = make_windows(X_norm, seq_len, stride)  # (M, T, D)\n",
    "        if X_win.shape[0] == 0:\n",
    "            print(f\"[WARN] {os.path.basename(p)}: フレーム不足（{seq_len}未満）でスキップ\")\n",
    "            continue\n",
    "\n",
    "        X_list.append(X_win)\n",
    "        y_list.append(np.full((X_win.shape[0],), y_lab, dtype=np.int64))\n",
    "        file_ids += [os.path.basename(p)] * X_win.shape[0]\n",
    "\n",
    "    if not X_list:\n",
    "        raise RuntimeError(\"データが作れませんでした。CSV名に 'ivdd' / 'normal' が含まれているか確認してください。\")\n",
    "\n",
    "    X = np.concatenate(X_list, axis=0)\n",
    "    y = np.concatenate(y_list, axis=0)\n",
    "    file_ids = np.array(file_ids)\n",
    "    print(f\"[INFO] 使用キーポイント実名: {used_kps_any}\")\n",
    "    return X, y, file_ids\n",
    "\n",
    "# ========= モデル定義（Functional; 構造は従来通り） =========\n",
    "def build_lstm_model(seq_len: int, dims: int, n_hidden: int, l2_lambda: float = 1e-4) -> keras.Model:\n",
    "    reg = keras.regularizers.l2(l2_lambda)\n",
    "    inp = keras.Input(shape=(seq_len, dims), name=\"input\")\n",
    "    td  = keras.layers.TimeDistributed(keras.layers.Dense(n_hidden, activation=\"relu\", kernel_regularizer=reg),\n",
    "                                       name=\"td_dense\")(inp)\n",
    "    x   = keras.layers.LSTM(n_hidden, return_sequences=True, kernel_regularizer=reg, name=\"lstm1\")(td)\n",
    "    x   = keras.layers.LSTM(n_hidden, kernel_regularizer=reg, name=\"lstm2\")(x)\n",
    "    out = keras.layers.Dense(1, kernel_regularizer=reg, name=\"logits\")(x)  # from_logits=True\n",
    "    model = keras.Model(inp, out, name=\"ivdd_lstm\")\n",
    "    return model\n",
    "\n",
    "# ========= データ読み込み =========\n",
    "csv_files = sorted(glob.glob(CSV_GLOB))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"CSV が見つかりません: {CSV_GLOB}\")\n",
    "\n",
    "X, y, file_ids = build_dataset(csv_files, SEQ_LEN, STRIDE)\n",
    "print(\"X:\", X.shape, \"y:\", y.shape, \"files:\", len(np.unique(file_ids)))\n",
    "\n",
    "# ========= 分割（ファイル単位推奨） =========\n",
    "if VAL_SPLIT_BY_FILE:\n",
    "    uniq = np.unique(file_ids)\n",
    "    tr_files, va_files = train_test_split(uniq, test_size=0.2, random_state=42, shuffle=True)\n",
    "    tr_mask = np.isin(file_ids, tr_files)\n",
    "    va_mask = np.isin(file_ids, va_files)\n",
    "    X_train, y_train = X[tr_mask], y[tr_mask]\n",
    "    X_val,   y_val   = X[va_mask], y[va_mask]\n",
    "else:\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"train:\", X_train.shape, \"val:\", X_val.shape)\n",
    "\n",
    "# ========= クラス不均衡対策 (0/1 の重み) =========\n",
    "# 片方しか存在しない場合は class_weight を None に\n",
    "unique_classes = np.unique(y_train)\n",
    "if set(unique_classes) == {0, 1}:\n",
    "    cls_w = compute_class_weight(class_weight=\"balanced\", classes=np.array([0,1]), y=y_train)\n",
    "    class_weight = {0: float(cls_w[0]), 1: float(cls_w[1])}\n",
    "else:\n",
    "    class_weight = None\n",
    "print(\"class_weight:\", class_weight)\n",
    "\n",
    "# ========= モデル準備・学習（EarlyStopping なし） =========\n",
    "model = build_lstm_model(SEQ_LEN, DIMS, N_HIDDEN, L2_LAMBDA)\n",
    "model.summary()\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=LR)\n",
    "loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = [keras.metrics.BinaryAccuracy(name=\"accuracy\")]\n",
    "model.compile(optimizer=opt, loss=loss_fn, metrics=metrics)\n",
    "\n",
    "# チェックポイント（best のみ）※要件: model/ にユニーク名で保存\n",
    "best_model_path  = os.path.join(MODEL_DIR, f\"ivdd_lstm_{RUN_ID}_best.keras\")\n",
    "final_model_path = os.path.join(MODEL_DIR, f\"ivdd_lstm_{RUN_ID}_final.keras\")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        best_model_path, monitor=\"val_accuracy\",\n",
    "        save_best_only=True, save_weights_only=False, mode=\"max\", verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", mode=\"min\",\n",
    "        factor=0.5, patience=5, min_lr=1e-5, verbose=1\n",
    "    ),\n",
    "    # ★ EarlyStopping は使わない（要件）\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_weight=class_weight,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# ========= 学習曲線を fig/ にユニーク名で保存 =========\n",
    "loss_png = os.path.join(FIG_DIR, f\"loss_{RUN_ID}.png\")\n",
    "acc_png  = os.path.join(FIG_DIR, f\"accuracy_{RUN_ID}.png\")\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history[\"loss\"], label=\"train\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val\")\n",
    "plt.title(\"Loss\")\n",
    "plt.legend(); plt.tight_layout()\n",
    "plt.savefig(loss_png, dpi=150); plt.close()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history[\"accuracy\"], label=\"train\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend(); plt.tight_layout()\n",
    "plt.savefig(acc_png, dpi=150); plt.close()\n",
    "\n",
    "print(\"[INFO] saved figs:\", loss_png, acc_png)\n",
    "\n",
    "# ========= 検証セットでの簡易評価（ウィンドウ単位） =========\n",
    "logits_val = model.predict(X_val, batch_size=64)\n",
    "y_pred = (tf.math.sigmoid(logits_val).numpy().ravel() >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n[Window-level] classification_report:\")\n",
    "print(classification_report(y_val, y_pred, target_names=CLASS_NAMES, digits=4))\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "print(\"[Window-level] confusion matrix:\\n\", cm)\n",
    "\n",
    "# ========= 最終モデルを保存（model/ にユニーク名） =========\n",
    "model.save(final_model_path)\n",
    "print(\"[INFO] saved final model to:\", final_model_path)\n",
    "\n",
    "# # （任意）学習に使った主要パラメータも一緒に保存しておくと後で便利\n",
    "# params_json = os.path.join(MODEL_DIR, f\"train_params_{RUN_ID}.json\")\n",
    "# with open(params_json, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump({\n",
    "#         \"RUN_ID\": RUN_ID,\n",
    "#         \"KEYPOINTS\": KEYPOINTS,\n",
    "#         \"USE_LIKELIHOOD\": USE_LIKELIHOOD,\n",
    "#         \"MIN_KEEP_LIKELIHOOD\": MIN_KEEP_LIKELIHOOD,\n",
    "#         \"SEQ_LEN\": SEQ_LEN,\n",
    "#         \"STRIDE\": STRIDE,\n",
    "#         \"DIMS\": DIMS,\n",
    "#         \"N_HIDDEN\": N_HIDDEN,\n",
    "#         \"L2_LAMBDA\": L2_LAMBDA,\n",
    "#         \"CLASS_NAMES\": CLASS_NAMES,\n",
    "#         \"VAL_SPLIT_BY_FILE\": VAL_SPLIT_BY_FILE,\n",
    "#         \"BATCH_SIZE\": BATCH_SIZE,\n",
    "#         \"EPOCHS\": EPOCHS,\n",
    "#         \"LR\": LR,\n",
    "#         \"DATA_DIR\": DATA_DIR,\n",
    "#         \"CSV_GLOB\": CSV_GLOB\n",
    "#     }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# print(\"[INFO] saved params to:\", params_json)\n",
    "print(\"[DONE] training complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLya",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
