{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bdc0d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF: 2.19.0\n",
      "[WARN] ivdd1_case1DLC_resnet50_IvddOct30shuffle1_100000.csv: フレーム不足（60未満）でスキップ\n",
      "[INFO] 使用キーポイント実名: ['left back paw', 'right back paw', 'left front paw', 'right front paw', 'tail set']\n",
      "X: (1236, 60, 10) y: (1236,) files: 170\n",
      "train: (979, 60, 10) val: (257, 60, 10)\n",
      "class_weight: {0: 1.1049661399548534, 1: 0.9132462686567164}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"ivdd_lstm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"ivdd_lstm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ td_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ logits (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input (\u001b[38;5;33mInputLayer\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m10\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ td_dense (\u001b[38;5;33mTimeDistributed\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m30\u001b[0m)         │           \u001b[38;5;34m330\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm1 (\u001b[38;5;33mLSTM\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m30\u001b[0m)         │         \u001b[38;5;34m7,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm2 (\u001b[38;5;33mLSTM\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)             │         \u001b[38;5;34m7,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ logits (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m31\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,001</span> (58.60 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m15,001\u001b[0m (58.60 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,001</span> (58.60 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m15,001\u001b[0m (58.60 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4251 - loss: 0.6985\n",
      "Epoch 1: val_accuracy improved from -inf to 0.38132, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-124122_best.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.4276 - loss: 0.6989 - val_accuracy: 0.3813 - val_loss: 0.6994 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4251 - loss: 0.6977\n",
      "Epoch 2: val_accuracy did not improve from 0.38132\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4276 - loss: 0.6981 - val_accuracy: 0.3813 - val_loss: 0.7022 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m28/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4218 - loss: 0.6971\n",
      "Epoch 3: val_accuracy did not improve from 0.38132\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4276 - loss: 0.6979 - val_accuracy: 0.3813 - val_loss: 0.7030 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4251 - loss: 0.6968\n",
      "Epoch 4: val_accuracy did not improve from 0.38132\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4276 - loss: 0.6971 - val_accuracy: 0.3813 - val_loss: 0.7030 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4251 - loss: 0.6956\n",
      "Epoch 5: val_accuracy did not improve from 0.38132\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4276 - loss: 0.6959 - val_accuracy: 0.3813 - val_loss: 0.7028 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m32/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4261 - loss: 0.6941\n",
      "Epoch 6: val_accuracy did not improve from 0.38132\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4276 - loss: 0.6943 - val_accuracy: 0.3813 - val_loss: 0.7027 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m32/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4261 - loss: 0.6922\n",
      "Epoch 7: val_accuracy did not improve from 0.38132\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4276 - loss: 0.6924 - val_accuracy: 0.3813 - val_loss: 0.7012 - learning_rate: 5.0000e-05\n",
      "Epoch 8/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4251 - loss: 0.6902\n",
      "Epoch 8: val_accuracy did not improve from 0.38132\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4276 - loss: 0.6905 - val_accuracy: 0.3813 - val_loss: 0.7001 - learning_rate: 5.0000e-05\n",
      "Epoch 9/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4251 - loss: 0.6881\n",
      "Epoch 9: val_accuracy did not improve from 0.38132\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4276 - loss: 0.6884 - val_accuracy: 0.3813 - val_loss: 0.7000 - learning_rate: 5.0000e-05\n",
      "Epoch 10/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4251 - loss: 0.6858\n",
      "Epoch 10: val_accuracy did not improve from 0.38132\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4276 - loss: 0.6861 - val_accuracy: 0.3813 - val_loss: 0.7004 - learning_rate: 5.0000e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m29/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4230 - loss: 0.6826\n",
      "Epoch 11: val_accuracy did not improve from 0.38132\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4276 - loss: 0.6831 - val_accuracy: 0.3813 - val_loss: 0.7017 - learning_rate: 5.0000e-05\n",
      "Epoch 12/50\n",
      "\u001b[1m29/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4230 - loss: 0.6791\n",
      "Epoch 12: val_accuracy did not improve from 0.38132\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4276 - loss: 0.6796 - val_accuracy: 0.3813 - val_loss: 0.6986 - learning_rate: 2.5000e-05\n",
      "Epoch 13/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4279 - loss: 0.6758\n",
      "Epoch 13: val_accuracy did not improve from 0.38132\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4287 - loss: 0.6759 - val_accuracy: 0.3813 - val_loss: 0.6983 - learning_rate: 2.5000e-05\n",
      "Epoch 14/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4275 - loss: 0.6723\n",
      "Epoch 14: val_accuracy did not improve from 0.38132\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4302 - loss: 0.6725 - val_accuracy: 0.3813 - val_loss: 0.6989 - learning_rate: 2.5000e-05\n",
      "Epoch 15/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4295 - loss: 0.6687\n",
      "Epoch 15: val_accuracy improved from 0.38132 to 0.38911, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-124122_best.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4326 - loss: 0.6689 - val_accuracy: 0.3891 - val_loss: 0.6995 - learning_rate: 2.5000e-05\n",
      "Epoch 16/50\n",
      "\u001b[1m32/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4497 - loss: 0.6648\n",
      "Epoch 16: val_accuracy did not improve from 0.38911\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4514 - loss: 0.6649 - val_accuracy: 0.3891 - val_loss: 0.7002 - learning_rate: 2.5000e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4716 - loss: 0.6605\n",
      "Epoch 17: val_accuracy did not improve from 0.38911\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4753 - loss: 0.6606 - val_accuracy: 0.3891 - val_loss: 0.7008 - learning_rate: 2.5000e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4844 - loss: 0.6562\n",
      "Epoch 18: val_accuracy improved from 0.38911 to 0.39300, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-124122_best.keras\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4877 - loss: 0.6563 - val_accuracy: 0.3930 - val_loss: 0.7011 - learning_rate: 2.5000e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5050 - loss: 0.6524\n",
      "Epoch 19: val_accuracy did not improve from 0.39300\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5077 - loss: 0.6524 - val_accuracy: 0.3930 - val_loss: 0.6920 - learning_rate: 1.2500e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5111 - loss: 0.6473\n",
      "Epoch 20: val_accuracy improved from 0.39300 to 0.40467, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-124122_best.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5137 - loss: 0.6475 - val_accuracy: 0.4047 - val_loss: 0.6917 - learning_rate: 1.2500e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5203 - loss: 0.6452\n",
      "Epoch 21: val_accuracy improved from 0.40467 to 0.40856, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-124122_best.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5236 - loss: 0.6455 - val_accuracy: 0.4086 - val_loss: 0.6909 - learning_rate: 1.2500e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5318 - loss: 0.6432\n",
      "Epoch 22: val_accuracy improved from 0.40856 to 0.41634, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-124122_best.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5351 - loss: 0.6434 - val_accuracy: 0.4163 - val_loss: 0.6900 - learning_rate: 1.2500e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5533 - loss: 0.6413\n",
      "Epoch 23: val_accuracy did not improve from 0.41634\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5555 - loss: 0.6415 - val_accuracy: 0.4163 - val_loss: 0.6892 - learning_rate: 1.2500e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5567 - loss: 0.6396\n",
      "Epoch 24: val_accuracy improved from 0.41634 to 0.42802, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-124122_best.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5588 - loss: 0.6398 - val_accuracy: 0.4280 - val_loss: 0.6885 - learning_rate: 1.2500e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5625 - loss: 0.6380\n",
      "Epoch 25: val_accuracy did not improve from 0.42802\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5642 - loss: 0.6382 - val_accuracy: 0.4202 - val_loss: 0.6879 - learning_rate: 1.2500e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5720 - loss: 0.6365\n",
      "Epoch 26: val_accuracy did not improve from 0.42802\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5731 - loss: 0.6367 - val_accuracy: 0.4202 - val_loss: 0.6873 - learning_rate: 1.2500e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5770 - loss: 0.6351\n",
      "Epoch 27: val_accuracy improved from 0.42802 to 0.43969, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-124122_best.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5778 - loss: 0.6354 - val_accuracy: 0.4397 - val_loss: 0.6868 - learning_rate: 1.2500e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5798 - loss: 0.6339\n",
      "Epoch 28: val_accuracy improved from 0.43969 to 0.44747, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-124122_best.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5806 - loss: 0.6341 - val_accuracy: 0.4475 - val_loss: 0.6863 - learning_rate: 1.2500e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5829 - loss: 0.6327\n",
      "Epoch 29: val_accuracy improved from 0.44747 to 0.46693, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-124122_best.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5837 - loss: 0.6329 - val_accuracy: 0.4669 - val_loss: 0.6859 - learning_rate: 1.2500e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5862 - loss: 0.6316\n",
      "Epoch 30: val_accuracy improved from 0.46693 to 0.48249, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-124122_best.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5869 - loss: 0.6318 - val_accuracy: 0.4825 - val_loss: 0.6855 - learning_rate: 1.2500e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m32/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5894 - loss: 0.6306\n",
      "Epoch 31: val_accuracy did not improve from 0.48249\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5900 - loss: 0.6307 - val_accuracy: 0.4825 - val_loss: 0.6852 - learning_rate: 1.2500e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m28/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5828 - loss: 0.6290\n",
      "Epoch 32: val_accuracy improved from 0.48249 to 0.49805, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-124122_best.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5868 - loss: 0.6298 - val_accuracy: 0.4981 - val_loss: 0.6848 - learning_rate: 1.2500e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5879 - loss: 0.6286\n",
      "Epoch 33: val_accuracy improved from 0.49805 to 0.50195, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-124122_best.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5898 - loss: 0.6288 - val_accuracy: 0.5019 - val_loss: 0.6845 - learning_rate: 1.2500e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5876 - loss: 0.6277\n",
      "Epoch 34: val_accuracy improved from 0.50195 to 0.50973, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-124122_best.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5895 - loss: 0.6279 - val_accuracy: 0.5097 - val_loss: 0.6841 - learning_rate: 1.2500e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5942 - loss: 0.6268\n",
      "Epoch 35: val_accuracy improved from 0.50973 to 0.52140, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-124122_best.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5958 - loss: 0.6271 - val_accuracy: 0.5214 - val_loss: 0.6838 - learning_rate: 1.2500e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6005 - loss: 0.6260\n",
      "Epoch 36: val_accuracy improved from 0.52140 to 0.53307, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-124122_best.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6017 - loss: 0.6263 - val_accuracy: 0.5331 - val_loss: 0.6835 - learning_rate: 1.2500e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6069 - loss: 0.6253\n",
      "Epoch 37: val_accuracy did not improve from 0.53307\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6083 - loss: 0.6255 - val_accuracy: 0.5292 - val_loss: 0.6831 - learning_rate: 1.2500e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m32/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6093 - loss: 0.6246\n",
      "Epoch 38: val_accuracy improved from 0.53307 to 0.53696, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-124122_best.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6103 - loss: 0.6248 - val_accuracy: 0.5370 - val_loss: 0.6828 - learning_rate: 1.2500e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6043 - loss: 0.6238\n",
      "Epoch 39: val_accuracy did not improve from 0.53696\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6062 - loss: 0.6241 - val_accuracy: 0.5370 - val_loss: 0.6824 - learning_rate: 1.2500e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6031 - loss: 0.6231\n",
      "Epoch 40: val_accuracy did not improve from 0.53696\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6052 - loss: 0.6234 - val_accuracy: 0.5370 - val_loss: 0.6820 - learning_rate: 1.2500e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6084 - loss: 0.6225\n",
      "Epoch 41: val_accuracy improved from 0.53696 to 0.54475, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-124122_best.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6103 - loss: 0.6227 - val_accuracy: 0.5447 - val_loss: 0.6817 - learning_rate: 1.2500e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6158 - loss: 0.6218\n",
      "Epoch 42: val_accuracy improved from 0.54475 to 0.55642, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-124122_best.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6172 - loss: 0.6221 - val_accuracy: 0.5564 - val_loss: 0.6813 - learning_rate: 1.2500e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6175 - loss: 0.6212\n",
      "Epoch 43: val_accuracy did not improve from 0.55642\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6191 - loss: 0.6215 - val_accuracy: 0.5564 - val_loss: 0.6809 - learning_rate: 1.2500e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6172 - loss: 0.6206\n",
      "Epoch 44: val_accuracy did not improve from 0.55642\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6186 - loss: 0.6209 - val_accuracy: 0.5564 - val_loss: 0.6805 - learning_rate: 1.2500e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6178 - loss: 0.6201\n",
      "Epoch 45: val_accuracy did not improve from 0.55642\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6193 - loss: 0.6203 - val_accuracy: 0.5564 - val_loss: 0.6801 - learning_rate: 1.2500e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6193 - loss: 0.6195\n",
      "Epoch 46: val_accuracy did not improve from 0.55642\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6206 - loss: 0.6198 - val_accuracy: 0.5564 - val_loss: 0.6797 - learning_rate: 1.2500e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6245 - loss: 0.6189\n",
      "Epoch 47: val_accuracy improved from 0.55642 to 0.56420, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-124122_best.keras\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6256 - loss: 0.6192 - val_accuracy: 0.5642 - val_loss: 0.6793 - learning_rate: 1.2500e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6272 - loss: 0.6184\n",
      "Epoch 48: val_accuracy did not improve from 0.56420\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6282 - loss: 0.6187 - val_accuracy: 0.5642 - val_loss: 0.6789 - learning_rate: 1.2500e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6272 - loss: 0.6179\n",
      "Epoch 49: val_accuracy did not improve from 0.56420\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6282 - loss: 0.6182 - val_accuracy: 0.5642 - val_loss: 0.6785 - learning_rate: 1.2500e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m31/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6277 - loss: 0.6174\n",
      "Epoch 50: val_accuracy did not improve from 0.56420\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6288 - loss: 0.6177 - val_accuracy: 0.5642 - val_loss: 0.6781 - learning_rate: 1.2500e-05\n",
      "[INFO] saved figs: C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\fig\\loss_20251124-124122.png C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\fig\\accuracy_20251124-124122.png\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step\n",
      "\n",
      "[Window-level] classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal     0.4931    0.7245    0.5868        98\n",
      "        ivdd     0.7611    0.5409    0.6324       159\n",
      "\n",
      "    accuracy                         0.6109       257\n",
      "   macro avg     0.6271    0.6327    0.6096       257\n",
      "weighted avg     0.6589    0.6109    0.6150       257\n",
      "\n",
      "[Window-level] confusion matrix:\n",
      " [[71 27]\n",
      " [73 86]]\n",
      "[INFO] saved final model to: C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\model\\ivdd_lstm_20251124-124122_final.keras\n",
      "[DONE] training complete.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Binary IVDD training (ivdd vs normal) from DeepLabCut CSV (3-level header)\n",
    "要件:\n",
    "- model/ フォルダに学習モデルを毎回ユニーク名で保存（best と final）\n",
    "- fig/ フォルダに学習曲線（loss/accuracy）を毎回ユニーク名で保存\n",
    "- 前処理: tail_set を原点に平行移動 → 各次元を min-max 正規化（ファイル単位）\n",
    "- EarlyStopping は使わない\n",
    "- ネットワーク構造: TimeDistributed(Dense->ReLU) → LSTM → LSTM → Dense(1 logits)（従来通り）\n",
    "\"\"\"\n",
    "\n",
    "# ===== 安定運用: GPU無効/ログ控えめ/スレッド抑制（必要なら） =====\n",
    "import os\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"-1\")\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "\n",
    "import glob, re, json, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TF:\", tf.__version__)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ========= パラメータ =========\n",
    "# 学習用CSVを置いたフォルダ（例: train ディレクトリ）\n",
    "DATA_DIR   = r\"C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\"\n",
    "CSV_GLOB   = os.path.join(DATA_DIR, \"*.csv\")\n",
    "\n",
    "# 保存先（要件）\n",
    "MODEL_DIR = os.path.join(DATA_DIR, \"model\")\n",
    "FIG_DIR   = os.path.join(DATA_DIR, \"fig\")\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "\n",
    "# ランごとユニークID\n",
    "RUN_ID = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# 指定の5点（順序がそのまま (x,y) の並び順になります）\n",
    "KEYPOINTS = [\n",
    "    \"left back paw\",\n",
    "    \"right back paw\",\n",
    "    \"left front paw\",\n",
    "    \"right front paw\",\n",
    "    \"tail set\",\n",
    "]\n",
    "\n",
    "# DLCのlikelihood閾値で低信頼を欠損扱いに\n",
    "USE_LIKELIHOOD      = False\n",
    "MIN_KEEP_LIKELIHOOD = 0.6\n",
    "\n",
    "# ウィンドウ設定（必要に応じて 60/30 などに変更）\n",
    "SEQ_LEN  = 60\n",
    "STRIDE   = 30\n",
    "\n",
    "# 次元・ネットワーク\n",
    "DIMS      = 10           # 5点×(x,y)\n",
    "N_HIDDEN  = 30\n",
    "N_CLASSES = 1            # バイナリ → ロジット1本\n",
    "\n",
    "# 学習ハイパパラメータ\n",
    "BATCH_SIZE = 30\n",
    "EPOCHS     = 50\n",
    "LR         = 1e-4        # floatにしておくと ReduceLROnPlateau が適用可能\n",
    "L2_LAMBDA  = 1e-4\n",
    "\n",
    "# 分割設定\n",
    "VAL_SPLIT_BY_FILE = True   # ファイル単位で分割（時系列リーク防止）\n",
    "\n",
    "# ラベル表記（ivdd を positive=1 として扱います）\n",
    "CLASS_NAMES  = [\"normal\", \"ivdd\"]  # index 0=normal, 1=ivdd\n",
    "CLASS_TO_IDX = {\"normal\": 0, \"ivdd\": 1}\n",
    "\n",
    "# ========= ラベル推定（ファイル名から） =========\n",
    "def infer_label_from_filename(path: str) -> int:\n",
    "    \"\"\"\n",
    "    ファイル名からラベルを推定。\n",
    "    1) stem を非英数字で分割したトークンに 'ivdd' / 'normal' が厳密一致すれば採用\n",
    "    2) 未決なら先頭トークンが 'ivdd' / 'normal' なら採用\n",
    "    3) まだ未決なら親ディレクトリ名トークンを参照（片方のみ含む場合）\n",
    "    \"\"\"\n",
    "    name = os.path.basename(path).lower()\n",
    "    stem = os.path.splitext(name)[0]\n",
    "    tokens = [t for t in re.split(r'[^a-z0-9]+', stem) if t]\n",
    "    token_set = set(tokens)\n",
    "\n",
    "    has_ivdd   = ('ivdd' in token_set)\n",
    "    has_normal = ('normal' in token_set)\n",
    "\n",
    "    if has_ivdd and not has_normal:\n",
    "        return CLASS_TO_IDX[\"ivdd\"]\n",
    "    if has_normal and not has_ivdd:\n",
    "        return CLASS_TO_IDX[\"normal\"]\n",
    "\n",
    "    if tokens:\n",
    "        if tokens[0] in CLASS_TO_IDX:\n",
    "            return CLASS_TO_IDX[tokens[0]]\n",
    "\n",
    "    parent_tokens = [t for t in re.split(r'[^a-z0-9]+', os.path.dirname(path).lower()) if t]\n",
    "    pset = set(parent_tokens)\n",
    "    if ('ivdd' in pset) and ('normal' not in pset):\n",
    "        return CLASS_TO_IDX[\"ivdd\"]\n",
    "    if ('normal' in pset) and ('ivdd' not in pset):\n",
    "        return CLASS_TO_IDX[\"normal\"]\n",
    "\n",
    "    raise ValueError(\n",
    "        f\"ラベルを特定できません: {name} \"\n",
    "        f\"(推奨: ファイル名の先頭を 'ivdd_' または 'normal_' にしてください)\"\n",
    "    )\n",
    "\n",
    "# ========= 前処理（DLC 3 レベルヘッダ） =========\n",
    "def _norm_name(s: str) -> str:\n",
    "    return \"\".join(ch for ch in s.lower() if ch not in \" _-\")\n",
    "\n",
    "def _resolve_keypoints(all_bodyparts, requested):\n",
    "    norm2orig = {}\n",
    "    for bp in all_bodyparts:\n",
    "        k = _norm_name(bp)\n",
    "        if k not in norm2orig:\n",
    "            norm2orig[k] = bp\n",
    "    resolved, missing = [], []\n",
    "    for req in requested:\n",
    "        k = _norm_name(req)\n",
    "        if k in norm2orig:\n",
    "            resolved.append(norm2orig[k])\n",
    "        else:\n",
    "            missing.append(req)\n",
    "    if missing:\n",
    "        raise ValueError(f\"指定キーポイントがCSVで見つかりません: {missing}\\n利用可能: {all_bodyparts}\")\n",
    "    return resolved\n",
    "\n",
    "def read_dlc_5kp_xy(csv_path: str,\n",
    "                    keypoints,\n",
    "                    use_likelihood=True,\n",
    "                    min_keep_likelihood=0.6):\n",
    "    \"\"\"\n",
    "    (1) DLC CSV(3段ヘッダ)読込 → (2) 指定5点の (x,y) 抜き出し → (3) 低likelihoodをNaN\n",
    "       → (4) 線形補間 + bfill/ffill → 0埋め → (T,10) を返す\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, header=[0,1,2], index_col=0)\n",
    "    bodyparts = list({bp for (_, bp, _) in df.columns})\n",
    "    use_kps = _resolve_keypoints(bodyparts, keypoints)\n",
    "\n",
    "    cols = {}\n",
    "    for bp in use_kps:\n",
    "        cols[f\"{bp}_x\"] = df.xs((bp, \"x\"), level=[1,2], axis=1)\n",
    "        cols[f\"{bp}_y\"] = df.xs((bp, \"y\"), level=[1,2], axis=1)\n",
    "    X_df = pd.concat(cols.values(), axis=1)\n",
    "    X_df.columns = list(cols.keys())\n",
    "\n",
    "    if use_likelihood:\n",
    "        for bp in use_kps:\n",
    "            try:\n",
    "                lcol = df.xs((bp, \"likelihood\"), level=[1,2], axis=1).values.flatten()\n",
    "                low = lcol < min_keep_likelihood\n",
    "                for c in [f\"{bp}_x\", f\"{bp}_y\"]:\n",
    "                    v = X_df[c].values\n",
    "                    v[low] = np.nan\n",
    "                    X_df[c] = v\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "    X_df = X_df.interpolate(method=\"linear\", limit_direction=\"both\", axis=0)\n",
    "    # FutureWarning 回避: bfill()/ffill() を使う\n",
    "    X_df = X_df.bfill().ffill().fillna(0.0)\n",
    "\n",
    "    X = X_df.values.astype(np.float32)  # (T, 10)\n",
    "    return X, use_kps\n",
    "\n",
    "# === tail_set中心 + min-max 正規化（ファイル単位・各次元独立）===\n",
    "def normalize_tailset_minmax(\n",
    "    X: np.ndarray,         # (T,10)\n",
    "    used_kps: list[str],   # read_dlc_5kp_xy で実際に使われた実名\n",
    "    ref_name: str = \"tail set\",\n",
    "    eps: float = 1e-6\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    1) tail_setを原点に平行移動（各フレーム）\n",
    "    2) 平行移動後の各次元(全フレーム)で min-max 正規化（0〜1）\n",
    "       x' = (x - min) / (max - min + eps)\n",
    "    \"\"\"\n",
    "    low = [s.lower() for s in used_kps]\n",
    "    if ref_name.lower() not in low:\n",
    "        raise ValueError(f\"'{ref_name}' が used_kps に見つかりません: {used_kps}\")\n",
    "    ref_idx = low.index(ref_name.lower())\n",
    "\n",
    "    # 1) 原点平行移動\n",
    "    cx = X[:, 2*ref_idx]\n",
    "    cy = X[:, 2*ref_idx + 1]\n",
    "    Xc = X.copy()\n",
    "    for i in range(len(used_kps)):\n",
    "        Xc[:, 2*i]   -= cx\n",
    "        Xc[:, 2*i+1] -= cy\n",
    "\n",
    "    # 2) min-max 正規化（ファイル全体、各次元独立）\n",
    "    x_min = Xc.min(axis=0, keepdims=True)\n",
    "    x_max = Xc.max(axis=0, keepdims=True)\n",
    "    Xn = (Xc - x_min) / (x_max - x_min + eps)\n",
    "    return Xn\n",
    "\n",
    "def make_windows(X: np.ndarray, seq_len: int, stride: int) -> np.ndarray:\n",
    "    n = X.shape[0]\n",
    "    if n < seq_len:\n",
    "        return np.empty((0, seq_len, X.shape[1]), dtype=X.dtype)\n",
    "    starts = range(0, n - seq_len + 1, stride)\n",
    "    return np.stack([X[s:s+seq_len] for s in starts], axis=0)\n",
    "\n",
    "def build_dataset(csv_paths, seq_len=SEQ_LEN, stride=STRIDE):\n",
    "    \"\"\"\n",
    "    返り値:\n",
    "      X: (N, T, D)\n",
    "      y: (N,) 0/1\n",
    "      file_ids: (N,) 元ファイル名\n",
    "    \"\"\"\n",
    "    X_list, y_list, file_ids = [], [], []\n",
    "    used_kps_any = None\n",
    "\n",
    "    for p in csv_paths:\n",
    "        y_lab = infer_label_from_filename(p)  # 0=normal,1=ivdd\n",
    "\n",
    "        X_raw, used_kps = read_dlc_5kp_xy(\n",
    "            p, keypoints=KEYPOINTS,\n",
    "            use_likelihood=USE_LIKELIHOOD,\n",
    "            min_keep_likelihood=MIN_KEEP_LIKELIHOOD\n",
    "        )\n",
    "        if used_kps_any is None:\n",
    "            used_kps_any = used_kps\n",
    "\n",
    "        if X_raw.shape[1] != DIMS:\n",
    "            raise ValueError(f\"{os.path.basename(p)}: 次元 {X_raw.shape[1]} != 期待 {DIMS}\")\n",
    "\n",
    "        # ★ tail_set中心 + min-max 正規化\n",
    "        X_norm = normalize_tailset_minmax(X_raw, used_kps, ref_name=\"tail set\")\n",
    "\n",
    "        X_win = make_windows(X_norm, seq_len, stride)  # (M, T, D)\n",
    "        if X_win.shape[0] == 0:\n",
    "            print(f\"[WARN] {os.path.basename(p)}: フレーム不足（{seq_len}未満）でスキップ\")\n",
    "            continue\n",
    "\n",
    "        X_list.append(X_win)\n",
    "        y_list.append(np.full((X_win.shape[0],), y_lab, dtype=np.int64))\n",
    "        file_ids += [os.path.basename(p)] * X_win.shape[0]\n",
    "\n",
    "    if not X_list:\n",
    "        raise RuntimeError(\"データが作れませんでした。CSV名に 'ivdd' / 'normal' が含まれているか確認してください。\")\n",
    "\n",
    "    X = np.concatenate(X_list, axis=0)\n",
    "    y = np.concatenate(y_list, axis=0)\n",
    "    file_ids = np.array(file_ids)\n",
    "    print(f\"[INFO] 使用キーポイント実名: {used_kps_any}\")\n",
    "    return X, y, file_ids\n",
    "\n",
    "# ========= モデル定義（Functional; 構造は従来通り） =========\n",
    "def build_lstm_model(seq_len: int, dims: int, n_hidden: int, l2_lambda: float = 1e-4) -> keras.Model:\n",
    "    reg = keras.regularizers.l2(l2_lambda)\n",
    "    inp = keras.Input(shape=(seq_len, dims), name=\"input\")\n",
    "    td  = keras.layers.TimeDistributed(keras.layers.Dense(n_hidden, activation=\"relu\", kernel_regularizer=reg),\n",
    "                                       name=\"td_dense\")(inp)\n",
    "    x   = keras.layers.LSTM(n_hidden, return_sequences=True, kernel_regularizer=reg, name=\"lstm1\")(td)\n",
    "    x   = keras.layers.LSTM(n_hidden, kernel_regularizer=reg, name=\"lstm2\")(x)\n",
    "    out = keras.layers.Dense(1, kernel_regularizer=reg, name=\"logits\")(x)  # from_logits=True\n",
    "    model = keras.Model(inp, out, name=\"ivdd_lstm\")\n",
    "    return model\n",
    "\n",
    "# ========= データ読み込み =========\n",
    "csv_files = sorted(glob.glob(CSV_GLOB))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"CSV が見つかりません: {CSV_GLOB}\")\n",
    "\n",
    "X, y, file_ids = build_dataset(csv_files, SEQ_LEN, STRIDE)\n",
    "print(\"X:\", X.shape, \"y:\", y.shape, \"files:\", len(np.unique(file_ids)))\n",
    "\n",
    "# ========= 分割（ファイル単位推奨） =========\n",
    "if VAL_SPLIT_BY_FILE:\n",
    "    uniq = np.unique(file_ids)\n",
    "    tr_files, va_files = train_test_split(uniq, test_size=0.2, random_state=42, shuffle=True)\n",
    "    tr_mask = np.isin(file_ids, tr_files)\n",
    "    va_mask = np.isin(file_ids, va_files)\n",
    "    X_train, y_train = X[tr_mask], y[tr_mask]\n",
    "    X_val,   y_val   = X[va_mask], y[va_mask]\n",
    "else:\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"train:\", X_train.shape, \"val:\", X_val.shape)\n",
    "\n",
    "# ========= クラス不均衡対策 (0/1 の重み) =========\n",
    "# 片方しか存在しない場合は class_weight を None に\n",
    "unique_classes = np.unique(y_train)\n",
    "if set(unique_classes) == {0, 1}:\n",
    "    cls_w = compute_class_weight(class_weight=\"balanced\", classes=np.array([0,1]), y=y_train)\n",
    "    class_weight = {0: float(cls_w[0]), 1: float(cls_w[1])}\n",
    "else:\n",
    "    class_weight = None\n",
    "print(\"class_weight:\", class_weight)\n",
    "\n",
    "# ========= モデル準備・学習（EarlyStopping なし） =========\n",
    "model = build_lstm_model(SEQ_LEN, DIMS, N_HIDDEN, L2_LAMBDA)\n",
    "model.summary()\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=LR)\n",
    "loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = [keras.metrics.BinaryAccuracy(name=\"accuracy\")]\n",
    "model.compile(optimizer=opt, loss=loss_fn, metrics=metrics)\n",
    "\n",
    "# チェックポイント（best のみ）※要件: model/ にユニーク名で保存\n",
    "best_model_path  = os.path.join(MODEL_DIR, f\"ivdd_lstm_{RUN_ID}_best.keras\")\n",
    "final_model_path = os.path.join(MODEL_DIR, f\"ivdd_lstm_{RUN_ID}_final.keras\")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        best_model_path, monitor=\"val_accuracy\",\n",
    "        save_best_only=True, save_weights_only=False, mode=\"max\", verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", mode=\"min\",\n",
    "        factor=0.5, patience=5, min_lr=1e-5, verbose=1\n",
    "    ),\n",
    "    # ★ EarlyStopping は使わない（要件）\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_weight=class_weight,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# ========= 学習曲線を fig/ にユニーク名で保存 =========\n",
    "loss_png = os.path.join(FIG_DIR, f\"loss_{RUN_ID}.png\")\n",
    "acc_png  = os.path.join(FIG_DIR, f\"accuracy_{RUN_ID}.png\")\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history[\"loss\"], label=\"train\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val\")\n",
    "plt.title(\"Loss\")\n",
    "plt.legend(); plt.tight_layout()\n",
    "plt.savefig(loss_png, dpi=150); plt.close()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history[\"accuracy\"], label=\"train\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend(); plt.tight_layout()\n",
    "plt.savefig(acc_png, dpi=150); plt.close()\n",
    "\n",
    "print(\"[INFO] saved figs:\", loss_png, acc_png)\n",
    "\n",
    "# ========= 検証セットでの簡易評価（ウィンドウ単位） =========\n",
    "logits_val = model.predict(X_val, batch_size=64)\n",
    "y_pred = (tf.math.sigmoid(logits_val).numpy().ravel() >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n[Window-level] classification_report:\")\n",
    "print(classification_report(y_val, y_pred, target_names=CLASS_NAMES, digits=4))\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "print(\"[Window-level] confusion matrix:\\n\", cm)\n",
    "\n",
    "# ========= 最終モデルを保存（model/ にユニーク名） =========\n",
    "model.save(final_model_path)\n",
    "print(\"[INFO] saved final model to:\", final_model_path)\n",
    "\n",
    "# # （任意）学習に使った主要パラメータも一緒に保存しておくと後で便利\n",
    "# params_json = os.path.join(MODEL_DIR, f\"train_params_{RUN_ID}.json\")\n",
    "# with open(params_json, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump({\n",
    "#         \"RUN_ID\": RUN_ID,\n",
    "#         \"KEYPOINTS\": KEYPOINTS,\n",
    "#         \"USE_LIKELIHOOD\": USE_LIKELIHOOD,\n",
    "#         \"MIN_KEEP_LIKELIHOOD\": MIN_KEEP_LIKELIHOOD,\n",
    "#         \"SEQ_LEN\": SEQ_LEN,\n",
    "#         \"STRIDE\": STRIDE,\n",
    "#         \"DIMS\": DIMS,\n",
    "#         \"N_HIDDEN\": N_HIDDEN,\n",
    "#         \"L2_LAMBDA\": L2_LAMBDA,\n",
    "#         \"CLASS_NAMES\": CLASS_NAMES,\n",
    "#         \"VAL_SPLIT_BY_FILE\": VAL_SPLIT_BY_FILE,\n",
    "#         \"BATCH_SIZE\": BATCH_SIZE,\n",
    "#         \"EPOCHS\": EPOCHS,\n",
    "#         \"LR\": LR,\n",
    "#         \"DATA_DIR\": DATA_DIR,\n",
    "#         \"CSV_GLOB\": CSV_GLOB\n",
    "#     }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# print(\"[INFO] saved params to:\", params_json)\n",
    "print(\"[DONE] training complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLya",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
