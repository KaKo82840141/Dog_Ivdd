{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a638be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 収集画像数: 2422\n",
      "y\n",
      "normal    1494\n",
      "one        526\n",
      "two        402\n",
      "Name: count, dtype: int64\n",
      "[INFO] split -> train: 2058, val: 364\n",
      "[INFO] class_weight: {0: 1.534675615212528, 1: 2.0058479532163744, 2: 0.5405831363278172}\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\kanno\\envs\\MLya\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 215\u001b[39m\n\u001b[32m    206\u001b[39m callbacks = [\n\u001b[32m    207\u001b[39m     ReduceLROnPlateau(monitor=\u001b[33m\"\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m\"\u001b[39m, mode=\u001b[33m\"\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m\"\u001b[39m, factor=\u001b[32m0.5\u001b[39m, patience=\u001b[32m5\u001b[39m, min_lr=\u001b[32m1e-6\u001b[39m, verbose=\u001b[32m1\u001b[39m),\n\u001b[32m    208\u001b[39m     ModelCheckpoint(model_path, monitor=\u001b[33m\"\u001b[39m\u001b[33mval_accuracy\u001b[39m\u001b[33m\"\u001b[39m, mode=\u001b[33m\"\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m\"\u001b[39m, save_best_only=\u001b[38;5;28;01mTrue\u001b[39;00m, verbose=\u001b[32m1\u001b[39m),\n\u001b[32m    209\u001b[39m     CSVLogger(hist_csv, append=\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# 追加: エポック毎のloss/accをCSV保存\u001b[39;00m\n\u001b[32m    210\u001b[39m ]\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m    213\u001b[39m \u001b[38;5;66;03m#  学習\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ モデル保存: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    226\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[38;5;66;03m# 学習曲線保存\u001b[39;00m\n\u001b[32m    228\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\kanno\\envs\\MLya\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\kanno\\envs\\MLya\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\kanno\\envs\\MLya\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:219\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    221\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\kanno\\envs\\MLya\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\kanno\\envs\\MLya\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\kanno\\envs\\MLya\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:919\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    913\u001b[39m   \u001b[38;5;66;03m# If we did not create any variables the trace we have is good enough.\u001b[39;00m\n\u001b[32m    914\u001b[39m   filtered_flat_args = (\n\u001b[32m    915\u001b[39m       \u001b[38;5;28mself\u001b[39m._concrete_variable_creation_fn.function_type.unpack_inputs(\n\u001b[32m    916\u001b[39m           bound_args\n\u001b[32m    917\u001b[39m       )\n\u001b[32m    918\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    920\u001b[39m \u001b[43m      \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfn_with_cond\u001b[39m(inner_args, inner_kwds):\n\u001b[32m    925\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Conditionally runs initialization if it's needed.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\kanno\\envs\\MLya\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\kanno\\envs\\MLya\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\kanno\\envs\\MLya\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\kanno\\envs\\MLya\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\kanno\\envs\\MLya\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os, re, datetime, io\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import CSVLogger, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# =========================\n",
    "# パス設定（必要に応じて変更）\n",
    "# =========================\n",
    "# あなたのリポジトリ構成に合わせた既定値\n",
    "PROCESS_DIR = r\"C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\process\"\n",
    "\n",
    "# 使いたい軌跡サブフォルダ（例：正規化図のみ使うなら *_nor を指定）\n",
    "# 例) [\"tail_set_nor\", \"left_paw_nor\", \"right_paw_nor\"]\n",
    "# 例) 生の軌跡なら [\"tail_set\", \"left_paw\", \"right_paw\"]\n",
    "SRC_SUBDIRS = [\n",
    "    \"left_paw_nor\",\n",
    "    \"right_paw_nor\",\n",
    "    # \"left_tarsal_nor\",\n",
    "    # \"right_tarsal_nor\",\n",
    "]\n",
    "\n",
    "# 出力先\n",
    "MODEL_DIR = r\"C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\models_cnn_traj\"\n",
    "CURVE_DIR = r\"C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\output\\learning_curve_traj\"\n",
    "EVAL_DIR  = r\"C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\output\\learning_curve_traj\"\n",
    "\n",
    "# ★ 学習ログの保存先（任意のパスを指定）\n",
    "LOG_DIR   = r\"C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\output\\logs_traj\"\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(CURVE_DIR, exist_ok=True)\n",
    "os.makedirs(EVAL_DIR,  exist_ok=True)\n",
    "os.makedirs(LOG_DIR,   exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# ハイパパラメータ\n",
    "# =========================\n",
    "IMG_SIZE = (300, 300)\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 200\n",
    "VAL_RATIO = 0.15\n",
    "LEARNING_RATE = 5e-6\n",
    "AUGMENT = True  # 軽いDataAugを入れる場合は True\n",
    "\n",
    "# =========================\n",
    "# クラスとラベル推定\n",
    "# =========================\n",
    "# 3クラス分類：one, two, normal\n",
    "CLASSES = [\"one\", \"two\", \"normal\"]\n",
    "NAME2IDX = {c:i for i,c in enumerate(CLASSES)}\n",
    "\n",
    "def infer_label_from_path(p: Path):\n",
    "    \"\"\"\n",
    "    ファイル名と親フォルダ名をトークン化して判定\n",
    "    - 'one' -> one, 'two' -> two, 'normal' -> normal\n",
    "    \"\"\"\n",
    "    def tokens(s):\n",
    "        return [t for t in re.split(r\"[^a-z0-9]+\", s.lower()) if t]\n",
    "\n",
    "    toks = set(tokens(p.stem)) | set(tokens(p.name)) | set(tokens(p.parent.name))\n",
    "    if \"one\" in toks:    return NAME2IDX[\"one\"]\n",
    "    if \"two\" in toks:    return NAME2IDX[\"two\"]\n",
    "    if \"normal\" in toks: return NAME2IDX[\"normal\"]\n",
    "    return None  # 判定不能\n",
    "\n",
    "# =========================\n",
    "# 画像リスト収集（サブフォルダ別カウントも返す）\n",
    "# =========================\n",
    "def collect_images(process_dir: str, subdirs=None, exts=(\".png\", \".jpg\", \".jpeg\")):\n",
    "    proc = Path(process_dir)\n",
    "    paths, labels = [], []\n",
    "    per_dir_counts = {}  # {subdir_name: count}\n",
    "\n",
    "    subdirs = list(subdirs) if subdirs else []\n",
    "    if subdirs:\n",
    "        cand_dirs = [proc / sd for sd in subdirs]\n",
    "    else:\n",
    "        cand_dirs = [d for d in proc.iterdir() if d.is_dir()]\n",
    "\n",
    "    for d in cand_dirs:\n",
    "        if not d.exists():\n",
    "            print(f\"[WARN] ディレクトリが見つかりません: {d}\")\n",
    "            continue\n",
    "        before = len(paths)\n",
    "        for p in d.rglob(\"*\"):\n",
    "            if p.is_file() and p.suffix.lower() in exts:\n",
    "                y = infer_label_from_path(p)\n",
    "                if y is None:\n",
    "                    print(f\"[SKIP] ラベル不明: {p}\")\n",
    "                    continue\n",
    "                paths.append(str(p))\n",
    "                labels.append(y)\n",
    "        per_dir_counts[d.name] = len(paths) - before\n",
    "\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(\"画像が見つかりませんでした。SRC_SUBDIRS とファイル名ルール（one/two/normal）を確認してください。\")\n",
    "\n",
    "    df = pd.DataFrame({\"path\": paths, \"y\": labels})\n",
    "    return df, per_dir_counts\n",
    "\n",
    "df_all, per_dir_counts = collect_images(PROCESS_DIR, SRC_SUBDIRS)\n",
    "print(\"[INFO] 収集画像数:\", len(df_all))\n",
    "print(df_all[\"y\"].value_counts().rename(index={i:c for i,c in enumerate(CLASSES)}))\n",
    "\n",
    "# =========================\n",
    "# Stratified Train/Val 分割\n",
    "# =========================\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=VAL_RATIO, random_state=42)\n",
    "train_idx, val_idx = next(sss.split(df_all[\"path\"], df_all[\"y\"]))\n",
    "df_tr = df_all.iloc[train_idx].reset_index(drop=True)\n",
    "df_va = df_all.iloc[val_idx].reset_index(drop=True)\n",
    "print(f\"[INFO] split -> train: {len(df_tr)}, val: {len(df_va)}\")\n",
    "\n",
    "# =========================\n",
    "# tf.data パイプライン\n",
    "# =========================\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def decode_img(path):\n",
    "    img_bytes = tf.io.read_file(path)\n",
    "    img = tf.io.decode_image(img_bytes, channels=3, expand_animations=False)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    img = tf.clip_by_value(img/255.0, 0.0, 1.0)\n",
    "    return img\n",
    "\n",
    "def aug(img):\n",
    "    if AUGMENT:\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_brightness(img, max_delta=0.05)\n",
    "        img = tf.image.random_contrast(img, lower=0.9, upper=1.1)\n",
    "    return img\n",
    "\n",
    "def make_ds(paths, labels, training=True, batch_size=BATCH_SIZE):\n",
    "    ds_paths = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    def _load(path, y):\n",
    "        img = decode_img(path)\n",
    "        if training:\n",
    "            img = aug(img)\n",
    "        y_onehot = tf.one_hot(y, depth=len(CLASSES))\n",
    "        return img, y_onehot\n",
    "    ds = ds_paths.map(_load, num_parallel_calls=AUTOTUNE)\n",
    "    if training:\n",
    "        ds = ds.shuffle(buffer_size=min(2000, len(paths)))\n",
    "    ds = ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = make_ds(df_tr[\"path\"].values, df_tr[\"y\"].values, training=True)\n",
    "val_ds   = make_ds(df_va[\"path\"].values, df_va[\"y\"].values, training=False)\n",
    "\n",
    "# =========================\n",
    "# class_weight（不均衡対策）\n",
    "# =========================\n",
    "cls_w = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.array(list(range(len(CLASSES)))),\n",
    "    y=df_tr[\"y\"].values\n",
    ")\n",
    "class_weight = {i: float(cls_w[i]) for i in range(len(CLASSES))}\n",
    "print(\"[INFO] class_weight:\", class_weight)\n",
    "\n",
    "# =========================\n",
    "# CNN モデル（提示構成ベース）\n",
    "# =========================\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(CLASSES), activation='softmax')\n",
    "])\n",
    "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# =========================\n",
    "#  走行ID & 保存パス\n",
    "# =========================\n",
    "RUN_ID = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_path = os.path.join(MODEL_DIR, f\"traj_cnn_{RUN_ID}.h5\")\n",
    "curve_path = os.path.join(CURVE_DIR, f\"traj_learning_curve_{RUN_ID}.png\")\n",
    "pred_csv   = os.path.join(EVAL_DIR,  f\"traj_val_predictions_{RUN_ID}.csv\")\n",
    "cm_png     = os.path.join(EVAL_DIR,  f\"traj_cm_{RUN_ID}.png\")\n",
    "hist_csv   = os.path.join(LOG_DIR,   f\"history_{RUN_ID}.csv\")     # 追加: CSV履歴\n",
    "log_txt    = os.path.join(LOG_DIR,   f\"train_run_{RUN_ID}.txt\")   # 追加: テキストログ\n",
    "\n",
    "# =========================\n",
    "#  コールバック\n",
    "# =========================\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", mode=\"min\", factor=0.5, patience=5, min_lr=1e-6, verbose=1),\n",
    "    ModelCheckpoint(model_path, monitor=\"val_accuracy\", mode=\"max\", save_best_only=True, verbose=1),\n",
    "    CSVLogger(hist_csv, append=False)  # 追加: エポック毎のloss/accをCSV保存\n",
    "]\n",
    "\n",
    "# =========================\n",
    "#  学習\n",
    "# =========================\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    class_weight=class_weight,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "print(f\"✅ モデル保存: {model_path}\")\n",
    "\n",
    "# =========================\n",
    "# 学習曲線保存\n",
    "# =========================\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title(\"Loss\"); plt.legend(); plt.grid(True)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.title(\"Accuracy\"); plt.legend(); plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(curve_path, dpi=150)\n",
    "plt.close()\n",
    "print(f\"✅ 学習曲線保存: {curve_path}\")\n",
    "\n",
    "# =========================\n",
    "#  検証データでの詳細評価（CSV & 混同行列）\n",
    "# =========================\n",
    "val_paths  = df_va[\"path\"].values\n",
    "val_labels = df_va[\"y\"].values\n",
    "y_true, y_prob, y_pred = [], [], []\n",
    "\n",
    "for start in range(0, len(val_paths), BATCH_SIZE):\n",
    "    batch_paths = val_paths[start:start+BATCH_SIZE]\n",
    "    batch_imgs = np.stack([\n",
    "        tf.image.resize(\n",
    "            tf.io.decode_image(tf.io.read_file(p), channels=3, expand_animations=False),\n",
    "            IMG_SIZE\n",
    "        ).numpy()/255.0\n",
    "        for p in batch_paths\n",
    "    ], axis=0)\n",
    "    probs = model.predict(batch_imgs, verbose=0)\n",
    "    preds = probs.argmax(axis=1)\n",
    "    y_prob.append(probs)\n",
    "    y_pred.append(preds)\n",
    "    y_true.append(val_labels[start:start+BATCH_SIZE])\n",
    "\n",
    "y_true = np.concatenate(y_true)\n",
    "y_pred = np.concatenate(y_pred)\n",
    "y_prob = np.concatenate(y_prob)\n",
    "\n",
    "print(\"\\n[Validation] classification_report\")\n",
    "report_txt = classification_report(y_true, y_pred, target_names=CLASSES, digits=4)\n",
    "print(report_txt)\n",
    "\n",
    "# 予測CSV\n",
    "df_pred = pd.DataFrame({\n",
    "    \"path\": val_paths,\n",
    "    \"true\": [CLASSES[i] for i in y_true],\n",
    "    \"pred\": [CLASSES[i] for i in y_pred],\n",
    "    **{f\"p_{c}\": y_prob[:, i] for i, c in enumerate(CLASSES)}\n",
    "})\n",
    "df_pred.to_csv(pred_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"✅ 予測CSV保存: {pred_csv}\")\n",
    "\n",
    "# 混同行列PNG\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(len(CLASSES))))\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "im = ax.imshow(cm.astype(np.float32) / max(cm.sum(),1), cmap=\"Blues\")\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "ax.set_xticks(list(range(len(CLASSES)))); ax.set_yticks(list(range(len(CLASSES))))\n",
    "ax.set_xticklabels(CLASSES); ax.set_yticklabels(CLASSES)\n",
    "ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\n",
    "ax.set_title(\"Confusion Matrix (Validation)\")\n",
    "for i in range(len(CLASSES)):\n",
    "    for j in range(len(CLASSES)):\n",
    "        ax.text(j, i, int(cm[i,j]),\n",
    "                ha=\"center\",\n",
    "                color=\"white\" if cm[i,j] > cm.max()/2 else \"black\",\n",
    "                fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(cm_png, dpi=150); plt.close()\n",
    "print(f\"✅ 混同行列保存: {cm_png}\")\n",
    "\n",
    "# =========================\n",
    "#  テキストログ作成（★追加機能）\n",
    "# =========================\n",
    "# モデルsummaryを文字列で取得\n",
    "string_buf = io.StringIO()\n",
    "model.summary(print_fn=lambda s: string_buf.write(s + \"\\n\"))\n",
    "model_summary_str = string_buf.getvalue()\n",
    "\n",
    "# クラス別カウント\n",
    "def counts_named(series):\n",
    "    vc = series.value_counts().sort_index()\n",
    "    return {CLASSES[i]: int(vc.get(i, 0)) for i in range(len(CLASSES))}\n",
    "\n",
    "counts_all = counts_named(df_all[\"y\"])\n",
    "counts_tr  = counts_named(df_tr[\"y\"])\n",
    "counts_va  = counts_named(df_va[\"y\"])\n",
    "\n",
    "# エポック履歴を書き出し\n",
    "def epoch_lines(hist: dict) -> str:\n",
    "    n = len(hist[\"loss\"])\n",
    "    lines = [\"epoch,loss,accuracy,val_loss,val_accuracy\"]\n",
    "    for i in range(n):\n",
    "        lines.append(f\"{i+1},{hist['loss'][i]:.6f},{hist['accuracy'][i]:.6f},{hist['val_loss'][i]:.6f},{hist['val_accuracy'][i]:.6f}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "with open(log_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"# CNN training log (3-class)  RUN_ID={RUN_ID}\\n\\n\")\n",
    "    f.write(\"## Hyperparameters\\n\")\n",
    "    f.write(f\"IMG_SIZE       : {IMG_SIZE}\\n\")\n",
    "    f.write(f\"BATCH_SIZE     : {BATCH_SIZE}\\n\")\n",
    "    f.write(f\"EPOCHS         : {EPOCHS}\\n\")\n",
    "    f.write(f\"VAL_RATIO      : {VAL_RATIO}\\n\")\n",
    "    f.write(f\"LEARNING_RATE  : {LEARNING_RATE}\\n\")\n",
    "    f.write(f\"AUGMENT        : {AUGMENT}\\n\")\n",
    "    f.write(f\"CLASSES        : {CLASSES}\\n\")\n",
    "    f.write(f\"SRC_SUBDIRS    : {SRC_SUBDIRS}\\n\")\n",
    "    f.write(f\"class_weight   : {class_weight}\\n\\n\")\n",
    "\n",
    "    f.write(\"## Dataset summary\\n\")\n",
    "    f.write(f\"Total images   : {len(df_all)}\\n\")\n",
    "    f.write(f\"By class (all) : {counts_all}\\n\")\n",
    "    f.write(f\"By class (train): {counts_tr}\\n\")\n",
    "    f.write(f\"By class (val)  : {counts_va}\\n\")\n",
    "    f.write(f\"By subdir (all): {per_dir_counts}\\n\\n\")\n",
    "\n",
    "    f.write(\"## Paths\\n\")\n",
    "    f.write(f\"MODEL_DIR: {MODEL_DIR}\\nCURVE_DIR: {CURVE_DIR}\\nEVAL_DIR : {EVAL_DIR}\\nLOG_DIR  : {LOG_DIR}\\n\\n\")\n",
    "    f.write(f\"Saved model path     : {model_path}\\n\")\n",
    "    f.write(f\"Learning curve path  : {curve_path}\\n\")\n",
    "    f.write(f\"Predictions CSV path : {pred_csv}\\n\")\n",
    "    f.write(f\"Confusion matrix PNG : {cm_png}\\n\")\n",
    "    f.write(f\"History CSV path     : {hist_csv}\\n\\n\")\n",
    "\n",
    "    f.write(\"## Model summary\\n\")\n",
    "    f.write(model_summary_str + \"\\n\")\n",
    "\n",
    "    f.write(\"## Epoch history (train/val)\\n\")\n",
    "    f.write(epoch_lines(history.history) + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"## Validation report\\n\")\n",
    "    f.write(report_txt + \"\\n\")\n",
    "\n",
    "    f.write(\"## Confusion matrix (raw counts)\\n\")\n",
    "    f.write(pd.DataFrame(cm, index=CLASSES, columns=CLASSES).to_string() + \"\\n\")\n",
    "\n",
    "print(f\"✅ テキストログ保存: {log_txt}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLya",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
