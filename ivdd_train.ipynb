{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdc0d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "IVDD binary grading (2-class: ivdd / normal) from DeepLabCut CSV (3-level header)\n",
    "- 5 keypoints × (x,y) = 10 dims\n",
    "- Windowing: SEQ_LEN=30, STRIDE=5\n",
    "- Model: TimeDistributed(Dense->ReLU) -> LSTM -> LSTM -> Dense(2 logits)\n",
    "- ラベルは「ファイル名」に 'ivdd' または 'normal' を含むことで自動判定\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ========= 設定 =========\n",
    "DATA_DIR   = r\"./data_ivdd\"                 # CSV を置くフォルダ\n",
    "CSV_GLOB   = os.path.join(DATA_DIR, \"*.csv\")\n",
    "\n",
    "# ★ 指定の5点（ユーザー指定）\n",
    "KEYPOINTS = [\n",
    "    \"left back paw\",\n",
    "    \"right back paw\",\n",
    "    \"left front paw\",\n",
    "    \"right front paw\",\n",
    "    \"tail set\",\n",
    "]\n",
    "\n",
    "USE_LIKELIHOOD = False\n",
    "MIN_KEEP_LIKELIHOOD = 0.6\n",
    "\n",
    "SEQ_LEN = 60\n",
    "STRIDE  = 30\n",
    "DIMS    = 10                   # 5点×(x,y)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS     = 50\n",
    "LR         = 1e-3\n",
    "VAL_SPLIT_BY_FILE = True       # ファイル単位でtrain/val分割（リーク防止）\n",
    "\n",
    "# ---- 2クラス定義 ----\n",
    "CLASS_NAMES  = [\"ivdd\", \"normal\"]\n",
    "CLASS_TO_IDX = {c:i for i,c in enumerate(CLASS_NAMES)}\n",
    "N_CLASSES    = len(CLASS_NAMES)\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ========= ラベル判定（ファイル名から） =========\n",
    "def infer_label_from_filename(filename: str) -> int:\n",
    "    \"\"\"ファイル名（拡張子除く）に 'ivdd' or 'normal' が含まれる前提でインデックスを返す\"\"\"\n",
    "    base = os.path.basename(filename).lower()\n",
    "    is_ivdd   = \"ivdd\"   in base\n",
    "    is_normal = \"normal\" in base\n",
    "    if is_ivdd and not is_normal:\n",
    "        return CLASS_TO_IDX[\"ivdd\"]\n",
    "    if is_normal and not is_ivdd:\n",
    "        return CLASS_TO_IDX[\"normal\"]\n",
    "    raise ValueError(f\"ラベルを特定できません: {filename}（ivdd/normal が含まれている必要があります）\")\n",
    "\n",
    "# ========= 入出力ユーティリティ =========\n",
    "def _norm_name(s: str) -> str:\n",
    "    # 小文字化し、空白/アンダースコア/ハイフンを除去して比較\n",
    "    return \"\".join(ch for ch in s.lower() if ch not in \" _-\")\n",
    "\n",
    "def _resolve_keypoints(all_bodyparts: list[str], requested: list[str]) -> list[str]:\n",
    "    # DLC列に実在するbodypart名へマッピング（表記ゆれ吸収）\n",
    "    norm2orig = {}\n",
    "    for bp in all_bodyparts:\n",
    "        k = _norm_name(bp)\n",
    "        if k not in norm2orig:  # 衝突は先勝ち\n",
    "            norm2orig[k] = bp\n",
    "    resolved = []\n",
    "    missing = []\n",
    "    for req in requested:\n",
    "        k = _norm_name(req)\n",
    "        if k in norm2orig:\n",
    "            resolved.append(norm2orig[k])\n",
    "        else:\n",
    "            missing.append(req)\n",
    "    if missing:\n",
    "        raise ValueError(f\"指定キーポイントがCSVで見つかりません: {missing}\\n利用可能: {all_bodyparts}\")\n",
    "    return resolved\n",
    "\n",
    "def read_dlc_5kp_xy(csv_path: str,\n",
    "                    keypoints: list[str],\n",
    "                    use_likelihood=True,\n",
    "                    min_keep_likelihood=0.6) -> tuple[np.ndarray, list[str]]:\n",
    "    \"\"\"\n",
    "    DLCの3段ヘッダCSVを読み込み、指定5点の (x,y) だけを抽出して (N,10) を返す。\n",
    "    - keypoints: ユーザーが指定した5点（表記ゆれは自動解決）\n",
    "    - likelihood < 閾値 の (x,y) は NaN → 線形補間\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, header=[0,1,2], index_col=0)\n",
    "    bodyparts = list({bp for (_, bp, _) in df.columns})\n",
    "    use_kps = _resolve_keypoints(bodyparts, keypoints)\n",
    "\n",
    "    # (x,y) を列ごとに抽出\n",
    "    cols = {}\n",
    "    for bp in use_kps:\n",
    "        cols[f\"{bp}_x\"] = df.xs((bp, \"x\"), level=[1,2], axis=1)\n",
    "        cols[f\"{bp}_y\"] = df.xs((bp, \"y\"), level=[1,2], axis=1)\n",
    "    X_df = pd.concat(cols.values(), axis=1)\n",
    "    X_df.columns = list(cols.keys())\n",
    "\n",
    "    # likelihood で品質管理 → 低いときは (x,y) を NaN\n",
    "    if use_likelihood:\n",
    "        for bp in use_kps:\n",
    "            try:\n",
    "                lcol = df.xs((bp, \"likelihood\"), level=[1,2], axis=1).values.flatten()\n",
    "                low = lcol < min_keep_likelihood\n",
    "                for c in [f\"{bp}_x\", f\"{bp}_y\"]:\n",
    "                    vals = X_df[c].values\n",
    "                    vals[low] = np.nan\n",
    "                    X_df[c] = vals\n",
    "            except KeyError:\n",
    "                pass  # likelihood 列がない場合はそのまま\n",
    "\n",
    "    # 線形補間→前後補完→0埋め\n",
    "    X_df = X_df.interpolate(method=\"linear\", limit_direction=\"both\", axis=0)\n",
    "    X_df = X_df.fillna(method=\"bfill\").fillna(method=\"ffill\").fillna(0.0)\n",
    "\n",
    "    X = X_df.values.astype(np.float32)          # (N, 10)\n",
    "    return X, use_kps\n",
    "\n",
    "def zscore_per_file(X: np.ndarray, eps: float=1e-6) -> np.ndarray:\n",
    "    mu = X.mean(axis=0, keepdims=True)\n",
    "    sd = X.std(axis=0, keepdims=True)\n",
    "    return (X - mu) / (sd + eps)\n",
    "\n",
    "def make_windows(X: np.ndarray, seq_len: int, stride: int) -> np.ndarray:\n",
    "    n = X.shape[0]\n",
    "    if n < seq_len:\n",
    "        return np.empty((0, seq_len, X.shape[1]), dtype=X.dtype)\n",
    "    starts = range(0, n - seq_len + 1, stride)\n",
    "    return np.stack([X[s:s+seq_len] for s in starts], axis=0)\n",
    "\n",
    "def build_dataset(csv_paths, seq_len=SEQ_LEN, stride=STRIDE):\n",
    "    X_list, y_list, file_ids = [], [], []\n",
    "    used_kps_any = None\n",
    "\n",
    "    for p in csv_paths:\n",
    "        # ファイル名からラベルを決定\n",
    "        y_lab = infer_label_from_filename(p)\n",
    "\n",
    "        X_raw, used_kps = read_dlc_5kp_xy(\n",
    "            p,\n",
    "            keypoints=KEYPOINTS,\n",
    "            use_likelihood=USE_LIKELIHOOD,\n",
    "            min_keep_likelihood=MIN_KEEP_LIKELIHOOD\n",
    "        )\n",
    "        if X_raw.shape[1] != DIMS:\n",
    "            raise ValueError(f\"{os.path.basename(p)}: 取り出し次元 {X_raw.shape[1]} が期待の {DIMS} と違います\")\n",
    "\n",
    "        used_kps_any = used_kps  # 代表して最後に表示\n",
    "\n",
    "        X_raw = zscore_per_file(X_raw)\n",
    "        X_win = make_windows(X_raw, seq_len, stride)   # (M, T, D)\n",
    "        if X_win.shape[0] == 0:\n",
    "            print(f\"[WARN] {os.path.basename(p)}: フレーム不足（{seq_len}未満）でスキップ\")\n",
    "            continue\n",
    "\n",
    "        X_list.append(X_win)\n",
    "        y_list.append(np.full((X_win.shape[0],), y_lab, dtype=np.int64))\n",
    "        file_ids += [os.path.basename(p)] * X_win.shape[0]\n",
    "\n",
    "    if not X_list:\n",
    "        raise RuntimeError(\"データが作れませんでした。CSV と命名規則（ivdd/normal を含む）を確認してください。\")\n",
    "\n",
    "    X = np.concatenate(X_list, axis=0)\n",
    "    y = np.concatenate(y_list, axis=0)\n",
    "    file_ids = np.array(file_ids)\n",
    "    print(f\"[INFO] 使用キーポイント実名: {used_kps_any}\")\n",
    "    return X, y, file_ids\n",
    "\n",
    "# ========= モデル（構造そのまま） =========\n",
    "class LSTM_RNN(keras.Model):\n",
    "    def __init__(self, n_input, n_hidden, n_classes):\n",
    "        super().__init__()\n",
    "        self.input_dense = keras.layers.Dense(n_hidden, activation='relu')\n",
    "        self.time_dist   = keras.layers.TimeDistributed(self.input_dense)\n",
    "        self.lstm1 = keras.layers.LSTM(n_hidden, return_sequences=True)\n",
    "        self.lstm2 = keras.layers.LSTM(n_hidden)\n",
    "        self.out   = keras.layers.Dense(n_classes)  # logits\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.time_dist(x)\n",
    "        x = self.lstm1(x, training=training)\n",
    "        x = self.lstm2(x, training=training)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class LSTMWithL2(LSTM_RNN):\n",
    "    def __init__(self, n_input, n_hidden, n_classes, l2_lambda=1e-4):\n",
    "        super().__init__(n_input, n_hidden, n_classes)\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.loss_fn   = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        self.metric_acc  = keras.metrics.CategoricalAccuracy(name=\"accuracy\")\n",
    "        self.metric_loss = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    def compile(self, optimizer, **kwargs):\n",
    "        super().compile(optimizer=optimizer, **kwargs)\n",
    "\n",
    "    def _l2(self):\n",
    "        return self.l2_lambda * tf.add_n([tf.nn.l2_loss(v) for v in self.trainable_variables])\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self(x, training=True)\n",
    "            loss = self.loss_fn(y, logits) + self._l2()\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        self.metric_loss.update_state(loss)\n",
    "        self.metric_acc.update_state(y, logits)\n",
    "        return {\"loss\": self.metric_loss.result(), \"accuracy\": self.metric_acc.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        logits = self(x, training=False)\n",
    "        loss = self.loss_fn(y, logits) + self._l2()\n",
    "        self.metric_loss.update_state(loss)\n",
    "        self.metric_acc.update_state(y, logits)\n",
    "        return {\"loss\": self.metric_loss.result(), \"accuracy\": self.metric_acc.result()}\n",
    "\n",
    "# ========= データ読み込み & 学習 =========\n",
    "csv_files  = sorted(glob.glob(CSV_GLOB))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"CSV が見つかりません: {CSV_GLOB}\")\n",
    "\n",
    "X, y, file_ids = build_dataset(csv_files, SEQ_LEN, STRIDE)\n",
    "print(\"X:\", X.shape, \"y:\", y.shape, \"files:\", len(np.unique(file_ids)))\n",
    "\n",
    "# one-hot\n",
    "y_oh = keras.utils.to_categorical(y, num_classes=N_CLASSES)\n",
    "\n",
    "# ファイルリーク防止の分割\n",
    "if VAL_SPLIT_BY_FILE:\n",
    "    uniq = np.unique(file_ids)\n",
    "    tr_files, va_files = train_test_split(uniq, test_size=0.2, random_state=42, shuffle=True)\n",
    "    tr_mask = np.isin(file_ids, tr_files)\n",
    "    va_mask = np.isin(file_ids, va_files)\n",
    "    X_train, y_train = X[tr_mask], y_oh[tr_mask]\n",
    "    X_val,   y_val   = X[va_mask], y_oh[va_mask]\n",
    "else:\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y_oh, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# クラス不均衡対策（両クラスが存在しないとエラーになるのでガード）\n",
    "class_weight = None\n",
    "try:\n",
    "    cls_w = compute_class_weight(\"balanced\", classes=np.arange(N_CLASSES), y=np.argmax(y_train, axis=1))\n",
    "    class_weight = {int(c): float(w) for c, w in enumerate(cls_w)}\n",
    "except Exception as e:\n",
    "    print(\"[WARN] class_weight の計算に失敗（クラス数不足？）:\", e)\n",
    "\n",
    "n_hidden = 30\n",
    "model = LSTMWithL2(n_input=DIMS, n_hidden=n_hidden, n_classes=N_CLASSES, l2_lambda=1e-4)\n",
    "\n",
    "decay_steps = 100000  # 元コード準拠の大きめ\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=LR, decay_steps=decay_steps, decay_rate=0.96, staircase=True\n",
    ")\n",
    "opt = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "model.compile(optimizer=opt)\n",
    "\n",
    "ckpt_dir  = os.path.join(DATA_DIR, \"checkpoints\"); os.makedirs(ckpt_dir, exist_ok=True)\n",
    "ckpt_path = os.path.join(ckpt_dir, \"best.keras\")\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(ckpt_path, monitor=\"val_accuracy\", save_best_only=True, verbose=1),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=10, restore_best_weights=True, verbose=1),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-5, verbose=1),\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_weight=class_weight,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# ========= 評価 =========\n",
    "y_val_prob = model.predict(X_val, batch_size=BATCH_SIZE)\n",
    "y_val_pred = np.argmax(y_val_prob, axis=1)\n",
    "y_val_true = np.argmax(y_val, axis=1)\n",
    "\n",
    "print(\"\\nValidation report:\")\n",
    "print(classification_report(y_val_true, y_val_pred, target_names=CLASS_NAMES, digits=4))\n",
    "cm = confusion_matrix(y_val_true, y_val_pred)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "# 学習曲線\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1); plt.plot(history.history[\"loss\"]); plt.plot(history.history[\"val_loss\"]); plt.title(\"Loss\"); plt.legend([\"train\",\"val\"])\n",
    "plt.subplot(1,2,2); plt.plot(history.history[\"accuracy\"]); plt.plot(history.history[\"val_accuracy\"]); plt.title(\"Accuracy\"); plt.legend([\"train\",\"val\"])\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# 最終保存\n",
    "final_path = os.path.join(DATA_DIR, \"ivdd_binary_lstm_final.keras\")\n",
    "model.save(final_path)\n",
    "print(\"Saved model to:\", final_path)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
