{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bb9bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "IVDD binary evaluation with 3 keypoints (left back paw, right back paw, tail set)\n",
    "\n",
    "- 学習と同じ NORM_MODE を選択（\"zscore\" / \"tailset_minmax\"）\n",
    "- 入力: test/eval_csv/*.csv\n",
    "- 出力: test/eval_outputs/YYYYMMDD-HHMMSS/ 配下に一式\n",
    "- モデル出力が 1ロジットでも2ロジットでも自動で処理\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"-1\")\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "\n",
    "import re, glob\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TF:\", tf.__version__)\n",
    "tf.random.set_seed(42); np.random.set_seed(42)\n",
    "\n",
    "# ====== パス設定（必要に応じて変更） ======\n",
    "TEST_ROOT = r\"C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\test\"\n",
    "CSV_DIR   = os.path.join(TEST_ROOT, \"eval_csv\")\n",
    "CSV_GLOB  = os.path.join(CSV_DIR, \"*.csv\")\n",
    "\n",
    "OUT_BASE  = os.path.join(TEST_ROOT, \"eval_outputs\")\n",
    "RUN_ID    = datetime.now().strftime(\"%YMMDD-%H%M%S\").replace(\"M\",\"m\")  # YYYYMMDD-HHMMSS\n",
    "OUT_DIR   = os.path.join(OUT_BASE, RUN_ID)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# 学習済みモデルの .keras（上の学習スクリプトの出力を指定）\n",
    "CKPT_PATH = r\"C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\train3_model\\ivdd_lstm_20251211-120000_best.keras\"\n",
    "\n",
    "# ====== 設定 ======\n",
    "KEYPOINTS = [\n",
    "    \"left back paw\",\n",
    "    \"right back paw\",\n",
    "    \"tail set\",\n",
    "]\n",
    "USE_LIKELIHOOD      = False\n",
    "MIN_KEEP_LIKELIHOOD = 0.6\n",
    "\n",
    "SEQ_LEN = 60\n",
    "STRIDE  = 30\n",
    "DIMS    = 6\n",
    "\n",
    "# 学習と同じ正規化\n",
    "NORM_MODE = \"zscore\"           # train1 相当\n",
    "# NORM_MODE = \"tailset_minmax\" # train2 相当\n",
    "\n",
    "CLASS_NAMES  = [\"ivdd\", \"normal\"]    # 表示順（AUCや図は ivdd を陽性扱い）\n",
    "CLASS_TO_IDX = {\"ivdd\":0, \"normal\":1}\n",
    "NAME_TO_CLASSIDX = {\"ivdd\":0, \"normal\":1}\n",
    "\n",
    "# ====== 前処理系（3KP版） ======\n",
    "def infer_label_from_filename(path: str) -> int:\n",
    "    name = os.path.basename(path).lower()\n",
    "    stem = os.path.splitext(name)[0]\n",
    "    tokens = [t for t in re.split(r'[^a-z0-9]+', stem) if t]\n",
    "    token_set = set(tokens)\n",
    "    if ('ivdd' in token_set) and ('normal' not in token_set): return CLASS_TO_IDX[\"ivdd\"]\n",
    "    if ('normal' in token_set) and ('ivdd' not in token_set): return CLASS_TO_IDX[\"normal\"]\n",
    "    def looks_like(label, tok): return re.fullmatch(rf\"{label}\\d*\", tok) is not None\n",
    "    ivdd_like   = any(looks_like(\"ivdd\", t) for t in tokens)\n",
    "    normal_like = any(looks_like(\"normal\", t) for t in tokens)\n",
    "    if ivdd_like and not normal_like: return CLASS_TO_IDX[\"ivdd\"]\n",
    "    if normal_like and not ivdd_like: return CLASS_TO_IDX[\"normal\"]\n",
    "    if tokens and tokens[0] in CLASS_TO_IDX: return CLASS_TO_IDX[tokens[0]]\n",
    "    parent_tokens = [t for t in re.split(r'[^a-z0-9]+', os.path.dirname(path).lower()) if t]\n",
    "    pset = set(parent_tokens)\n",
    "    if ('ivdd' in pset) and ('normal' not in pset): return CLASS_TO_IDX[\"ivdd\"]\n",
    "    if ('normal' in pset) and ('ivdd' not in pset): return CLASS_TO_IDX[\"normal\"]\n",
    "    raise ValueError(f\"ラベル不明: {name}（'ivdd_' / 'normal_' または 'ivdd1_' 等の形式を推奨）\")\n",
    "\n",
    "def _norm_name(s: str) -> str:\n",
    "    return \"\".join(ch for ch in s.lower() if ch not in \" _-\")\n",
    "\n",
    "def _resolve_keypoints(all_bodyparts, requested):\n",
    "    norm2orig = {}\n",
    "    for bp in all_bodyparts:\n",
    "        k = _norm_name(bp)\n",
    "        norm2orig.setdefault(k, bp)\n",
    "    resolved, missing = [], []\n",
    "    for req in requested:\n",
    "        k = _norm_name(req)\n",
    "        if k in norm2orig: resolved.append(norm2orig[k])\n",
    "        else: missing.append(req)\n",
    "    if missing:\n",
    "        raise ValueError(f\"指定キーポイントがCSVで見つかりません: {missing}\\n利用可能: {all_bodyparts}\")\n",
    "    return resolved\n",
    "\n",
    "def read_dlc_3kp_xy(csv_path, keypoints, use_likelihood=True, min_keep_likelihood=0.6):\n",
    "    df = pd.read_csv(csv_path, header=[0,1,2], index_col=0)\n",
    "    bodyparts = list({bp for (_, bp, _) in df.columns})\n",
    "    use_kps = _resolve_keypoints(bodyparts, keypoints)\n",
    "    cols = {}\n",
    "    for bp in use_kps:\n",
    "        cols[f\"{bp}_x\"] = df.xs((bp,\"x\"), level=[1,2], axis=1)\n",
    "        cols[f\"{bp}_y\"] = df.xs((bp,\"y\"), level=[1,2], axis=1)\n",
    "    X_df = pd.concat(cols.values(), axis=1); X_df.columns = list(cols.keys())\n",
    "    if use_likelihood:\n",
    "        for bp in use_kps:\n",
    "            try:\n",
    "                lcol = df.xs((bp,\"likelihood\"), level=[1,2], axis=1).values.flatten()\n",
    "                low = lcol < min_keep_likelihood\n",
    "                for c in [f\"{bp}_x\", f\"{bp}_y\"]:\n",
    "                    v = X_df[c].values; v[low] = np.nan; X_df[c] = v\n",
    "            except KeyError: pass\n",
    "    X_df = X_df.interpolate(method=\"linear\", limit_direction=\"both\", axis=0)\n",
    "    X_df = X_df.bfill().ffill().fillna(0.0)\n",
    "    return X_df.values.astype(np.float32), use_kps\n",
    "\n",
    "def zscore_per_file(X: np.ndarray, eps: float=1e-6) -> np.ndarray:\n",
    "    mu = X.mean(axis=0, keepdims=True)\n",
    "    sd = X.std(axis=0, keepdims=True)\n",
    "    return (X - mu) / (sd + eps)\n",
    "\n",
    "def normalize_tailset_minmax(X: np.ndarray, used_kps: list[str], ref_name=\"tail set\", eps: float=1e-6):\n",
    "    low = [s.lower() for s in used_kps]\n",
    "    if ref_name.lower() not in low:\n",
    "        raise ValueError(f\"'{ref_name}' が used_kps に見つかりません: {used_kps}\")\n",
    "    ref_idx = low.index(ref_name.lower())\n",
    "    cx = X[:, 2*ref_idx]; cy = X[:, 2*ref_idx + 1]\n",
    "    Xc = X.copy()\n",
    "    for i in range(len(used_kps)):\n",
    "        Xc[:, 2*i]   -= cx\n",
    "        Xc[:, 2*i+1] -= cy\n",
    "    x_min = Xc.min(axis=0, keepdims=True)\n",
    "    x_max = Xc.max(axis=0, keepdims=True)\n",
    "    return (Xc - x_min) / (x_max - x_min + eps)\n",
    "\n",
    "def apply_normalization(X_raw, used_kps):\n",
    "    if NORM_MODE == \"zscore\":\n",
    "        return zscore_per_file(X_raw)\n",
    "    elif NORM_MODE == \"tailset_minmax\":\n",
    "        return normalize_tailset_minmax(X_raw, used_kps, ref_name=\"tail set\")\n",
    "    else:\n",
    "        raise ValueError(f\"未知の NORM_MODE: {NORM_MODE}\")\n",
    "\n",
    "def make_windows(X: np.ndarray, seq_len: int, stride: int):\n",
    "    n = X.shape[0]\n",
    "    if n < seq_len:\n",
    "        return np.empty((0, seq_len, X.shape[1]), dtype=X.dtype), []\n",
    "    starts = list(range(0, n - seq_len + 1, stride))\n",
    "    Xw = np.stack([X[s:s+seq_len] for s in starts], axis=0)\n",
    "    return Xw, starts\n",
    "\n",
    "def build_dataset(csv_paths, seq_len=SEQ_LEN, stride=STRIDE):\n",
    "    X_list, y_list, file_ids, starts_all = [], [], [], []\n",
    "    used_kps_any = None\n",
    "    for p in csv_paths:\n",
    "        y_lab = infer_label_from_filename(p)\n",
    "        X_raw, used_kps = read_dlc_3kp_xy(p, KEYPOINTS, USE_LIKELIHOOD, MIN_KEEP_LIKELIHOOD)\n",
    "        if used_kps_any is None: used_kps_any = used_kps\n",
    "        if X_raw.shape[1] != DIMS:\n",
    "            raise ValueError(f\"{os.path.basename(p)}: 次元 {X_raw.shape[1]} != 期待 {DIMS}\")\n",
    "        X_norm = apply_normalization(X_raw, used_kps)\n",
    "        X_win, starts = make_windows(X_norm, seq_len, stride)\n",
    "        if X_win.shape[0] == 0:\n",
    "            print(f\"[WARN] {os.path.basename(p)}: フレーム不足（{seq_len}未満）でスキップ\")\n",
    "            continue\n",
    "        X_list.append(X_win)\n",
    "        y_list.append(np.full((X_win.shape[0],), y_lab, dtype=np.int64))\n",
    "        file_ids += [os.path.basename(p)] * X_win.shape[0]\n",
    "        starts_all += starts\n",
    "    if not X_list:\n",
    "        raise RuntimeError(\"評価用CSVがありません。test/eval_csv に CSV を置き、命名規則（ivdd/normal）を確認してください。\")\n",
    "    X = np.concatenate(X_list, axis=0)\n",
    "    y = np.concatenate(y_list, axis=0)\n",
    "    return X, y, np.array(file_ids), np.array(starts_all)\n",
    "\n",
    "# ====== 入力収集 ======\n",
    "csv_files = sorted(glob.glob(CSV_GLOB))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"評価用CSVが見つかりません: {CSV_GLOB}\")\n",
    "\n",
    "X, y, file_ids, starts = build_dataset(csv_files, SEQ_LEN, STRIDE)\n",
    "print(\"X:\", X.shape, \"y:\", y.shape, \"files:\", len(np.unique(file_ids)))\n",
    "\n",
    "# ====== モデル読込 & 推論 ======\n",
    "model = keras.models.load_model(CKPT_PATH, compile=False)\n",
    "logits = np.asarray(model.predict(X, batch_size=64))\n",
    "\n",
    "# 1ロジットか2ロジットかを判別\n",
    "if logits.ndim == 1 or logits.shape[1] == 1:\n",
    "    p_ivdd   = tf.math.sigmoid(logits).numpy().reshape(-1)\n",
    "    p_normal = 1.0 - p_ivdd\n",
    "    probs    = np.stack([p_ivdd, p_normal], axis=1)  # 列順: [ivdd, normal]\n",
    "    y_pred   = (p_ivdd >= 0.5).astype(int)           # 1=ivdd, 0=normal\n",
    "else:\n",
    "    probs    = tf.nn.softmax(logits, axis=1).numpy()\n",
    "    p_ivdd   = probs[:, CLASS_TO_IDX[\"ivdd\"]]\n",
    "    p_normal = probs[:, CLASS_TO_IDX[\"normal\"]]\n",
    "    y_pred   = probs.argmax(axis=1)\n",
    "\n",
    "# ====== 保存パス ======\n",
    "paths = {\n",
    "    \"win_csv\":           os.path.join(OUT_DIR, f\"window_predictions_{RUN_ID}.csv\"),\n",
    "    \"roc_png\":           os.path.join(OUT_DIR, f\"roc_window_{RUN_ID}.png\"),\n",
    "    \"cm_window_png\":     os.path.join(OUT_DIR, f\"cm_window_{RUN_ID}.png\"),\n",
    "    \"file_pred_csv\":     os.path.join(OUT_DIR, f\"file_level_predictions_{RUN_ID}.csv\"),\n",
    "    \"file_errors_csv\":   os.path.join(OUT_DIR, f\"file_level_errors_{RUN_ID}.csv\"),\n",
    "    \"cm_file_major_png\": os.path.join(OUT_DIR, f\"cm_file_majority_{RUN_ID}.png\"),\n",
    "    \"cm_file_mean_png\":  os.path.join(OUT_DIR, f\"cm_file_meanprob_{RUN_ID}.png\"),\n",
    "}\n",
    "\n",
    "# ====== ウィンドウ単位出力 ======\n",
    "df_win = pd.DataFrame({\n",
    "    \"file\": file_ids, \"start\": starts, \"true\": y, \"pred\": y_pred,\n",
    "    \"p_ivdd\": p_ivdd, \"p_normal\": p_normal,\n",
    "}).sort_values([\"file\",\"start\"])\n",
    "df_win.to_csv(paths[\"win_csv\"], index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[INFO] 保存:\", paths[\"win_csv\"])\n",
    "\n",
    "print(\"\\n[Window-level] classification_report:\")\n",
    "print(classification_report(y, y_pred, target_names=[\"ivdd\",\"normal\"], digits=4))\n",
    "cm = confusion_matrix(y, y_pred, labels=[CLASS_TO_IDX[\"ivdd\"], CLASS_TO_IDX[\"normal\"]])\n",
    "print(\"[Window-level] confusion matrix:\\n\", cm)\n",
    "\n",
    "# ====== ROC（ivddを陽性） ======\n",
    "try:\n",
    "    y_pos = (y == CLASS_TO_IDX[\"ivdd\"]).astype(int)\n",
    "    auc_val = roc_auc_score(y_pos, p_ivdd)\n",
    "    fpr, tpr, _ = roc_curve(y_pos, p_ivdd, pos_label=1)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {auc_val:.3f}\")\n",
    "    plt.plot([0,1], [0,1], linestyle=\"--\")\n",
    "    plt.title(\"ROC (window-level)\")\n",
    "    plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend(); plt.tight_layout(); plt.savefig(paths[\"roc_png\"], dpi=150); plt.close()\n",
    "    print(\"[INFO] 保存:\", paths[\"roc_png\"])\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] ROC プロット失敗: {e}\")\n",
    "\n",
    "# ====== 可視化共通 ======\n",
    "def plot_cm(cm_mat, labels, title, path_png):\n",
    "    cmn = cm_mat.astype(np.float32) / (cm_mat.sum() + 1e-6)\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    im = ax.imshow(cmn, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(len(labels)), yticks=np.arange(len(labels)),\n",
    "           xticklabels=labels, yticklabels=labels,\n",
    "           ylabel='True label', xlabel='Predicted label', title=title)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    for i in range(cm_mat.shape[0]):\n",
    "        for j in range(cm_mat.shape[1]):\n",
    "            ax.text(j, i, int(cm_mat[i, j]), ha=\"center\",\n",
    "                    color=\"white\" if cmn[i, j] > cmn.max()/2 else \"black\", fontsize=12)\n",
    "    plt.tight_layout(); plt.savefig(path_png, dpi=150); plt.close()\n",
    "\n",
    "plot_cm(cm, [\"ivdd\",\"normal\"], \"Confusion Matrix (window-level)\", paths[\"cm_window_png\"])\n",
    "print(\"[INFO] 保存:\", paths[\"cm_window_png\"])\n",
    "\n",
    "# ====== ファイル単位集計 ======\n",
    "major_pred = df_win.groupby(\"file\")[\"pred\"].agg(lambda a: np.bincount(a).argmax())\n",
    "true_file  = df_win.groupby(\"file\")[\"true\"].first()\n",
    "mean_prob  = df_win.groupby(\"file\")[[\"p_ivdd\",\"p_normal\"]].mean()\n",
    "mean_pred  = mean_prob.values.argmax(axis=1)\n",
    "\n",
    "df_file = pd.DataFrame({\n",
    "    \"file\": mean_prob.index,\n",
    "    \"true\": [\"ivdd\" if t==0 else \"normal\" for t in true_file.values],  # y が [ivdd=0, normal=1]\n",
    "    \"pred_majority\": [\"ivdd\" if p==0 else \"normal\" for p in major_pred.values],\n",
    "    \"pred_meanprob\": [\"ivdd\" if p==0 else \"normal\" for p in mean_pred],\n",
    "    \"p_ivdd_mean\": mean_prob[\"p_ivdd\"].values,\n",
    "    \"p_normal_mean\": mean_prob[\"p_normal\"].values,\n",
    "})\n",
    "df_file.to_csv(paths[\"file_pred_csv\"], index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[INFO] 保存:\", paths[\"file_pred_csv\"])\n",
    "\n",
    "df_errors = df_file[df_file[\"true\"] != df_file[\"pred_meanprob\"]].copy()\n",
    "df_errors.to_csv(paths[\"file_errors_csv\"], index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[INFO] 保存:\", paths[\"file_errors_csv\"])\n",
    "\n",
    "# ファイル単位CM\n",
    "def _to_idx(series):\n",
    "    return series.map({\"ivdd\":0, \"normal\":1}).values\n",
    "cm_major = confusion_matrix(_to_idx(df_file[\"true\"]), _to_idx(df_file[\"pred_majority\"]), labels=[0,1])\n",
    "cm_mean  = confusion_matrix(_to_idx(df_file[\"true\"]), _to_idx(df_file[\"pred_meanprob\"]), labels=[0,1])\n",
    "plot_cm(cm_major, [\"ivdd\",\"normal\"], \"Confusion Matrix (File-level, Majority Vote)\", paths[\"cm_file_major_png\"])\n",
    "plot_cm(cm_mean,  [\"ivdd\",\"normal\"], \"Confusion Matrix (File-level, Mean Probability)\", paths[\"cm_file_mean_png\"])\n",
    "print(\"[INFO] 保存:\", paths[\"cm_file_major_png\"])\n",
    "print(\"[INFO] 保存:\", paths[\"cm_file_mean_png\"])\n",
    "\n",
    "print(\"\\n[Done] 出力先:\", OUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
