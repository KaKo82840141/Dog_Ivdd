{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "366ee208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\heel_hook1.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\heel_hook2.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\heel_hook3.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\heel_hook4.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\heel_hook5.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\heel_hook6.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\heel_hook7.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\heel_hook8.mp4\n",
      "Warning: Video C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\heel_hook8.mp4 ended before collecting 180 frames. Collected 0 frames.\n",
      "Warning: For video C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\heel_hook8.mp4, only 0/180 frames were saved to c:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\MP_Data_JSON\\heel_hook\\8.\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\deadpoint1.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\deadpoint2.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\deadpoint3.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\deadpoint4.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\deadpoint5.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\deadpoint6.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\deadpoint7.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\deadpoint8.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\dyno1.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\dyno2.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\dyno3.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\dyno4.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\dyno5.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\dyno6.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\dyno7.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\dyno8.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\cross_move1.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\cross_move2.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\cross_move3.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\cross_move4.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\cross_move5.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\cross_move6.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\cross_move7.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\cross_move8.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import json\n",
    "\n",
    "# 設定 (変更なし)\n",
    "DATA_PATH = os.path.join(os.getcwd(), 'MP_Data_JSON')\n",
    "actions = np.array(['heel_hook', 'deadpoint', 'dyno','cross_move'])\n",
    "no_videos = 8\n",
    "sequence_length = 180\n",
    "\n",
    "# Mediapipe Holisticモデルの初期化 (変更なし)\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "# 取得したい姿勢ランドマークのインデックスを指定\n",
    "DESIRED_POSE_LANDMARKS = [12, 14, 16, 18, 20, 22] # ユーザー指定のものを維持\n",
    "# 取得したい顔ランドマークのインデックスを指定 (空リストのままなら顔は処理されません)\n",
    "DESIRED_FACE_LANDMARKS = []\n",
    "# 取得したい左手ランドマークのインデックスを指定 (空リストのままなら左手は処理されません)\n",
    "DESIRED_LEFT_HAND_LANDMARKS = []\n",
    "# 取得したい右手ランドマークのインデックスを指定 (空リストのままなら右手は処理されません)\n",
    "DESIRED_RIGHT_HAND_LANDMARKS = []\n",
    "\n",
    "def multiple_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    image_height, image_width, _ = image.shape\n",
    "\n",
    "    # 姿勢のキーポイントの描画\n",
    "    if results.pose_landmarks:\n",
    "        # 点の描画\n",
    "        for idx in DESIRED_POSE_LANDMARKS: # グローバル変数を参照\n",
    "            if idx < len(results.pose_landmarks.landmark): # インデックス範囲チェック\n",
    "                landmark = results.pose_landmarks.landmark[idx]\n",
    "                # visibility が低いものは描画しないなどの条件も追加可能\n",
    "                # if landmark.visibility < 0.01: # 例: visibilityが極端に低いものはスキップ\n",
    "                #     continue\n",
    "                cx = int(landmark.x * image_width)\n",
    "                cy = int(landmark.y * image_height)\n",
    "                # ランドマーク（点）のスタイル: 青色、半径を調整\n",
    "                cv2.circle(image, (cx, cy), 3, (0, 0, 255), cv2.FILLED)\n",
    "\n",
    "        # 線の描画 (POSE_CONNECTIONS を元に、指定ランドマーク間のみ)\n",
    "        if mp_holistic.POSE_CONNECTIONS: # POSE_CONNECTIONS が定義されている場合\n",
    "            for connection in mp_holistic.POSE_CONNECTIONS:\n",
    "                start_idx = connection[0]\n",
    "                end_idx = connection[1]\n",
    "\n",
    "                # 接続する両方の点が DESIRED_POSE_LANDMARKS に含まれている場合のみ描画\n",
    "                if start_idx in DESIRED_POSE_LANDMARKS and end_idx in DESIRED_POSE_LANDMARKS:\n",
    "                    # ランドマークの存在確認\n",
    "                    if start_idx < len(results.pose_landmarks.landmark) and \\\n",
    "                       end_idx < len(results.pose_landmarks.landmark):\n",
    "\n",
    "                        start_landmark = results.pose_landmarks.landmark[start_idx]\n",
    "                        end_landmark = results.pose_landmarks.landmark[end_idx]\n",
    "\n",
    "                        # visibility が低いものは描画しないなどの条件も追加可能\n",
    "                        # if start_landmark.visibility < 0.01 or end_landmark.visibility < 0.01:\n",
    "                        #     continue\n",
    "\n",
    "                        start_point = (int(start_landmark.x * image_width), int(start_landmark.y * image_height))\n",
    "                        end_point = (int(end_landmark.x * image_width), int(end_landmark.y * image_height))\n",
    "                        # コネクション（線）のスタイル: 緑色、太さを調整\n",
    "                        cv2.line(image, start_point, end_point, (51, 255, 51), 2)\n",
    "\n",
    "    # 顔、左手、右手の描画は DESIRED_..._LANDMARKS が空なので、ここでは何もしません。\n",
    "    # もし特定部位を描画したい場合は、同様のロジックを DESIRED_FACE_LANDMARKS などを使って追加します。\n",
    "\n",
    "\n",
    "# ★★★ 変更点: JSON形式でキーポイントを抽出 ★★★\n",
    "def extract_keypoints(results):\n",
    "    keypoints_data = {}\n",
    "\n",
    "    # >>>>>>>>>> 変更点 3: 関数内の DESIRED_... リスト定義を削除 (グローバル変数を参照するため) <<<<<<<<<<<<\n",
    "    # DESIRED_POSE_LANDMARKS = [12 , 14 , 16 , 18 , 20 , 22] # これらの行を削除\n",
    "    # DESIRED_FACE_LANDMARKS = []\n",
    "    # DESIRED_LEFT_HAND_LANDMARKS = []\n",
    "    # DESIRED_RIGHT_HAND_LANDMARKS = []\n",
    "    # >>>>>>>>>> 変更点 3: ここまで <<<<<<<<<<<<\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        pose_landmarks = []\n",
    "        for i, res in enumerate(results.pose_landmarks.landmark):\n",
    "            if i in DESIRED_POSE_LANDMARKS: # グローバル変数を参照\n",
    "                pose_landmarks.append({\n",
    "                    \"id\": i,\n",
    "                    \"x\": res.x,\n",
    "                    \"y\": res.y,\n",
    "                    \"z\": res.z,\n",
    "                    \"visibility\": res.visibility\n",
    "                })\n",
    "        keypoints_data[\"pose\"] = pose_landmarks\n",
    "    else:\n",
    "        keypoints_data[\"pose\"] = []\n",
    "\n",
    "    if results.face_landmarks:\n",
    "        face_landmarks = []\n",
    "        if DESIRED_FACE_LANDMARKS: # グローバル変数を参照\n",
    "            for i, res in enumerate(results.face_landmarks.landmark):\n",
    "                if i in DESIRED_FACE_LANDMARKS:\n",
    "                    face_landmarks.append({\n",
    "                        \"id\": i, \"x\": res.x, \"y\": res.y, \"z\": res.z\n",
    "                    })\n",
    "        keypoints_data[\"face\"] = face_landmarks # DESIRED_FACE_LANDMARKS が空なら空リストが入る\n",
    "    else:\n",
    "        keypoints_data[\"face\"] = []\n",
    "\n",
    "    if results.left_hand_landmarks:\n",
    "        lh_landmarks = []\n",
    "        if DESIRED_LEFT_HAND_LANDMARKS: # グローバル変数を参照\n",
    "            for i, res in enumerate(results.left_hand_landmarks.landmark):\n",
    "                if i in DESIRED_LEFT_HAND_LANDMARKS:\n",
    "                    lh_landmarks.append({\n",
    "                        \"id\": i, \"x\": res.x, \"y\": res.y, \"z\": res.z\n",
    "                    })\n",
    "        keypoints_data[\"left_hand\"] = lh_landmarks\n",
    "    else:\n",
    "        keypoints_data[\"left_hand\"] = []\n",
    "\n",
    "    if results.right_hand_landmarks:\n",
    "        rh_landmarks = []\n",
    "        if DESIRED_RIGHT_HAND_LANDMARKS: # グローバル変数を参照\n",
    "            for i, res in enumerate(results.right_hand_landmarks.landmark):\n",
    "                if i in DESIRED_RIGHT_HAND_LANDMARKS:\n",
    "                    rh_landmarks.append({\n",
    "                        \"id\": i, \"x\": res.x, \"y\": res.y, \"z\": res.z\n",
    "                    })\n",
    "        keypoints_data[\"right_hand\"] = rh_landmarks\n",
    "    else:\n",
    "        keypoints_data[\"right_hand\"] = []\n",
    "\n",
    "    return keypoints_data\n",
    "\n",
    "# データの収集 (以降のコードは変更なし)\n",
    "# OpenCVのウィンドウ表示設定 (変更なし)\n",
    "cv2.namedWindow('OpenCV Feed', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('OpenCV Feed', (1280, 720))\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    for action in actions:\n",
    "        action_path = os.path.join(DATA_PATH, action)\n",
    "        if not os.path.exists(action_path):\n",
    "            os.makedirs(action_path)\n",
    "\n",
    "        for video_num in range(1, no_videos + 1):\n",
    "            video_file_path = os.path.join(r'C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos', action + str(video_num) + '.mp4')\n",
    "            cap = cv2.VideoCapture(video_file_path)\n",
    "\n",
    "            video_data_save_path = os.path.join(action_path, str(video_num))\n",
    "            if not os.path.exists(video_data_save_path):\n",
    "                os.makedirs(video_data_save_path)\n",
    "\n",
    "            print(f'Processing video: {video_file_path}')\n",
    "            frames_collected_for_current_video = 0\n",
    "\n",
    "            for frame_num in range(sequence_length):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(f\"Warning: Video {video_file_path} ended before collecting {sequence_length} frames. Collected {frame_num} frames.\")\n",
    "                    break\n",
    "\n",
    "                image, results = multiple_detection(frame, holistic)\n",
    "\n",
    "                draw_styled_landmarks(image, results) # 変更された関数が呼ばれる\n",
    "\n",
    "                cv2.putText(image, f'Collecting frames for {action} - Video {video_num} - Frame {frame_num + 1}/{sequence_length}',\n",
    "                            (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "                keypoints = extract_keypoints(results) # 変更された関数が呼ばれる\n",
    "                json_path = os.path.join(video_data_save_path, f'{frame_num}.json')\n",
    "                with open(json_path, 'w') as f:\n",
    "                    json.dump(keypoints, f, indent=4)\n",
    "\n",
    "                frames_collected_for_current_video +=1\n",
    "\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "            cap.release()\n",
    "            if frames_collected_for_current_video < sequence_length:\n",
    "                 print(f\"Warning: For video {video_file_path}, only {frames_collected_for_current_video}/{sequence_length} frames were saved to {video_data_save_path}.\")\n",
    "\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
