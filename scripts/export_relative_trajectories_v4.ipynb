{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ea3d5a7",
   "metadata": {},
   "source": [
    "# Export Relative & Normalized Trajectories (per keypoint folders)\n",
    "\n",
    "- **Relative plots** (x,y relative to `tail set`) are written to `process/<keypoint>/`  \n",
    "- **Normalized plots** (nx,ny) are written to **`process/<keypoint>_nor/`** ‚Üê (updated as requested)  \n",
    "- Points are **not** drawn (lines only).  \n",
    "- Axis ranges are configurable for both relative and normalized views.  \n",
    "- No likelihood masking or interpolation is applied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "626f497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Settings (edit here) ====\n",
    "IN_DIR   = r\"C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train_csv\"          # DLC 3-level header CSV directory\n",
    "OUT_DIR  = r\"C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\process\"                      # Output root (e.g., ...\\\\process)\n",
    "\n",
    "KEYPOINTS = [  # must include \"tail set\" for relative plots\n",
    "    \"tail set\",\n",
    "    \"right tarsal\",\n",
    "    \"right paw\",\n",
    "    \"left tarsal\",\n",
    "    \"left paw\",\n",
    "]\n",
    "\n",
    "WINDOW_LEN = 30\n",
    "STRIDE     = 15\n",
    "\n",
    "# Axis limits for relative coordinates (dx, dy) where origin is tail set\n",
    "X_LIM = (-300, 300)\n",
    "Y_LIM = (0, 300)\n",
    "\n",
    "# Axis limits for normalized coordinates (nx, ny)\n",
    "X_NORM_LIM = (-1.0, 1.0)\n",
    "Y_NORM_LIM = (-1.0, 1.0)\n",
    "\n",
    "INVERT_Y = True           # If True, invert y-axis for plotting\n",
    "EXPORT_NORMALIZED = True   # If True, output nx,ny plots to process/<keypoint>_nor/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9256714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _norm_name(s: str) -> str:\n",
    "    return ''.join(ch for ch in s.lower() if ch not in ' _-')\n",
    "\n",
    "def _resolve_keypoints(all_bodyparts, requested):\n",
    "    norm2orig = {}\n",
    "    for bp in all_bodyparts:\n",
    "        k = _norm_name(bp)\n",
    "        if k not in norm2orig:\n",
    "            norm2orig[k] = bp\n",
    "    resolved, missing = [], []\n",
    "    for req in requested:\n",
    "        k = _norm_name(req)\n",
    "        if k in norm2orig:\n",
    "            resolved.append(norm2orig[k])\n",
    "        else:\n",
    "            missing.append(req)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing keypoints in CSV: {missing}\\nAvailable: {all_bodyparts}\")\n",
    "    return resolved\n",
    "\n",
    "def read_dlc_xy(df: pd.DataFrame, bp: str):\n",
    "    x = df.xs((bp, 'x'), level=[1,2], axis=1).values.flatten()\n",
    "    y = df.xs((bp, 'y'), level=[1,2], axis=1).values.flatten()\n",
    "    return x.astype(np.float32), y.astype(np.float32)\n",
    "\n",
    "def read_dlc_nxny(df: pd.DataFrame, bp: str):\n",
    "    coords = set(df.columns.get_level_values(2).unique())\n",
    "    if not {'nx','ny'}.issubset(coords):\n",
    "        return None, None  # normalized columns not present\n",
    "    nx = df.xs((bp, 'nx'), level=[1,2], axis=1).values.flatten()\n",
    "    ny = df.xs((bp, 'ny'), level=[1,2], axis=1).values.flatten()\n",
    "    return nx.astype(np.float32), ny.astype(np.float32)\n",
    "\n",
    "def make_window_starts(T: int, win: int, stride: int):\n",
    "    if T < win:\n",
    "        return []\n",
    "    return list(range(0, T - win + 1, stride))\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9da0f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_trajectories(\n",
    "    in_dir: Path,\n",
    "    out_root: Path,\n",
    "    keypoints: list[str],\n",
    "    window_len: int = 60,\n",
    "    stride: int = 30,\n",
    "    x_lim=(-300,300),\n",
    "    y_lim=(0,300),\n",
    "    x_norm_lim=(-1.0, 1.0),\n",
    "    y_norm_lim=(-1.0, 1.0),\n",
    "    invert_y: bool = False,\n",
    "    export_normalized: bool = True,\n",
    "):\n",
    "    in_dir = Path(in_dir)\n",
    "    out_root = Path(out_root)\n",
    "    ensure_dir(out_root)\n",
    "\n",
    "    csv_paths = sorted(glob.glob(str(in_dir / '*.csv')))\n",
    "    if not csv_paths:\n",
    "        print(f\"[WARN] No CSV files in {in_dir}\")\n",
    "        return\n",
    "\n",
    "    for path in csv_paths:\n",
    "        try:\n",
    "            df = pd.read_csv(path, header=[0,1,2], index_col=0)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] cannot read {path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        bodyparts = list({bp for (_, bp, _) in df.columns})\n",
    "        try:\n",
    "            used = _resolve_keypoints(bodyparts, keypoints)\n",
    "        except ValueError as e:\n",
    "            print(f\"[WARN] {Path(path).name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # tail set index\n",
    "        low = [s.lower() for s in used]\n",
    "        if 'tail_set' not in low:\n",
    "            print(f\"[WARN] 'tail set' not found in requested keypoints for {Path(path).name}. Skipping.\")\n",
    "            continue\n",
    "        tidx = low.index('tail_set')\n",
    "        tail_name = used[tidx]\n",
    "\n",
    "        # read base (x, y)\n",
    "        try:\n",
    "            tx, ty = read_dlc_xy(df, tail_name)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] tail set columns missing in {Path(path).name}: {e}\")\n",
    "            continue\n",
    "        T = len(tx)\n",
    "        starts = make_window_starts(T, window_len, stride)\n",
    "        if not starts:\n",
    "            print(f\"[INFO] too short: {Path(path).name} (T={T})\")\n",
    "            continue\n",
    "\n",
    "        # For normalized columns availability check once\n",
    "        have_norm = False\n",
    "        if export_normalized:\n",
    "            nx_tail, ny_tail = read_dlc_nxny(df, tail_name)\n",
    "            have_norm = (nx_tail is not None) and (ny_tail is not None)\n",
    "\n",
    "        fname = Path(path).stem\n",
    "\n",
    "        for kpi, kp in enumerate(used):\n",
    "            if kp.lower() == 'tail_set':\n",
    "                continue  # no need to output tail itself\n",
    "\n",
    "            # relative: process/<kp>/\n",
    "            rel_dir = out_root / kp\n",
    "            ensure_dir(rel_dir)\n",
    "\n",
    "            # read kp raw x,y\n",
    "            try:\n",
    "                x, y = read_dlc_xy(df, kp)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] {kp} x,y missing in {Path(path).name}: {e}\")\n",
    "                continue\n",
    "\n",
    "            dx = x - tx\n",
    "            dy = y - ty\n",
    "\n",
    "            for s in starts:\n",
    "                eidx = s + window_len\n",
    "                segx = dx[s:eidx]\n",
    "                segy = dy[s:eidx]\n",
    "\n",
    "                fig, ax = plt.subplots(figsize=(6,6))\n",
    "                ax.plot(segx, segy)  # line only\n",
    "                ax.set_xlim(*x_lim)\n",
    "                ax.set_ylim(*y_lim)\n",
    "                if invert_y:\n",
    "                    ax.invert_yaxis()\n",
    "                ax.set_title(f\"{fname} | {kp} | {s}-{eidx}\")\n",
    "                ax.set_xlabel(\"dx (px)\")\n",
    "                ax.set_ylabel(\"dy (px)\")\n",
    "                fig.tight_layout()\n",
    "                out_path = rel_dir / f\"{fname}_{_norm_name(kp)}_{s}-{eidx}.png\"\n",
    "                fig.savefig(out_path, dpi=150)\n",
    "                plt.close(fig)\n",
    "\n",
    "            # normalized: process/<kp>_nor/\n",
    "            if export_normalized and have_norm:\n",
    "                nkx, nky = read_dlc_nxny(df, kp)\n",
    "                if nkx is None or nky is None:\n",
    "                    print(f\"[WARN] normalized columns missing for {kp} in {Path(path).name}\")\n",
    "                else:\n",
    "                    nor_dir = out_root / f\"{kp}_nor\"   # <-- per-keypoint normalized folder (updated)\n",
    "                    ensure_dir(nor_dir)\n",
    "                    for s in starts:\n",
    "                        eidx = s + window_len\n",
    "                        segx = nkx[s:eidx]\n",
    "                        segy = nky[s:eidx]\n",
    "\n",
    "                        fig, ax = plt.subplots(figsize=(6,6))\n",
    "                        ax.plot(segx, segy)  # line only\n",
    "                        ax.set_xlim(*x_norm_lim)\n",
    "                        ax.set_ylim(*y_norm_lim)\n",
    "                        if invert_y:\n",
    "                            ax.invert_yaxis()\n",
    "                        ax.set_title(f\"{fname} | {kp} (normalized) | {s}-{eidx}\")\n",
    "                        ax.set_xlabel(\"nx\")\n",
    "                        ax.set_ylabel(\"ny\")\n",
    "                        fig.tight_layout()\n",
    "                        out_path = nor_dir / f\"{fname}_{_norm_name(kp)}_{s}-{eidx}_norm.png\"\n",
    "                        fig.savefig(out_path, dpi=150)\n",
    "                        plt.close(fig)\n",
    "\n",
    "        print(f\"[OK] {Path(path).name} -> done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4109493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] normal_100DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_101DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_102DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_103DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_104DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_105DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_106DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_107DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_108DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_109DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_10DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_110DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_111DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_11DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_17DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_20DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_27DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_31DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_33DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_34DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_35DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_37DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_38DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_40DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_41DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_43DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_47DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_49DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_50DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_51DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_53DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_56DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_58DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_59DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_5DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_61DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_66DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_67DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_68DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_69DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_6DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_71DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_77DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_78DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_79DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_7DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_82DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_83DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_84DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_85DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_86DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_87DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_88DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_89DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_91DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_92DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_93DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_94DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_95DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_96DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_97DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_98DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_99DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] normal_9DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] one_10DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] one_11DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] one_12DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] one_15DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] one_17DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] one_19DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] one_20DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] one_21DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] one_22DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] one_23DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] one_27DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] one_2DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] one_30DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] one_3DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] one_4DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] one_5DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] one_case1DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] one_case2DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] one_case3DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] one_case4DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] one_case5DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] two_11DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] two_18DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] two_22DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] two_33DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] two_35DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] two_37DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] two_38DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] two_42DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] two_46DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] two_47DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] two_51DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] two_52DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] two_53DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] two_54DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] two_55DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] two_8DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] two_case10DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] two_case11DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] two_case12DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] two_case13DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] two_case14DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] two_case3DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] two_case7DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] two_case8DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n",
      "[OK] two_case9DLC_resnet152_sotuken1Dec17shuffle1_150000_proc.csv -> done.\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "def main():\n",
    "    ap = ArgumentParser()\n",
    "    ap.add_argument('--input',  type=str, default=IN_DIR)\n",
    "    ap.add_argument('--output', type=str, default=OUT_DIR)\n",
    "    ap.add_argument('--keypoints', type=str, default=','.join(KEYPOINTS))\n",
    "    ap.add_argument('--window-len', type=int, default=WINDOW_LEN)\n",
    "    ap.add_argument('--stride', type=int, default=STRIDE)\n",
    "    ap.add_argument('--invert-y', action='store_true', default=INVERT_Y)\n",
    "\n",
    "    ap.add_argument('--xlim', type=float, nargs=2, default=list(X_LIM))\n",
    "    ap.add_argument('--ylim', type=float, nargs=2, default=list(Y_LIM))\n",
    "    ap.add_argument('--xnorm', type=float, nargs=2, default=list(X_NORM_LIM))\n",
    "    ap.add_argument('--ynorm', type=float, nargs=2, default=list(Y_NORM_LIM))\n",
    "    ap.add_argument('--export-normalized', action='store_true' if EXPORT_NORMALIZED else 'store_false',\n",
    "                    default=EXPORT_NORMALIZED)\n",
    "\n",
    "    args, _ = ap.parse_known_args()\n",
    "\n",
    "    in_dir  = Path(args.input)\n",
    "    out_dir = Path(args.output)\n",
    "    kps = [s.strip() for s in args.keypoints.split(',') if s.strip()]\n",
    "\n",
    "    export_trajectories(\n",
    "        in_dir=in_dir,\n",
    "        out_root=out_dir,\n",
    "        keypoints=kps,\n",
    "        window_len=args.window_len,\n",
    "        stride=args.stride,\n",
    "        x_lim=tuple(args.xlim),\n",
    "        y_lim=tuple(args.ylim),\n",
    "        x_norm_lim=tuple(args.xnorm),\n",
    "        y_norm_lim=tuple(args.ynorm),\n",
    "        invert_y=args.invert_y,\n",
    "        export_normalized=args.export_normalized,\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLya",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
