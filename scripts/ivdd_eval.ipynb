{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a717eaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF: 2.19.0\n",
      "[WARN] ivdd1_case1DLC_resnet50_IvddOct30shuffle1_100000.csv: フレーム不足（60未満）でスキップ\n",
      "[INFO] 使用キーポイント: ['left back paw', 'right back paw', 'left front paw', 'right front paw', 'tail set']\n",
      "X: (231, 60, 10) y: (231,) files: 38\n",
      "[WARN] load_model 失敗: <class '__main__.LSTMWithL2'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n",
      "\n",
      "config={'module': None, 'class_name': 'LSTMWithL2', 'config': {'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}}, 'registered_name': 'LSTMWithL2', 'build_config': {'input_shape': [None, 60, 10]}, 'compile_config': None}.\n",
      "\n",
      "Exception encountered: Unable to revive model from config. When overriding the `get_config()` method, make sure that the returned config contains all items used as arguments in the  constructor to <class '__main__.LSTMWithL2'>, which is the default behavior. You can override this default behavior by defining a `from_config(cls, config)` class method to specify how to create an instance of LSTMWithL2 from its config.\n",
      "\n",
      "Received config={'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}}\n",
      "\n",
      "Error encountered during deserialization: LSTMWithL2.__init__() got an unexpected keyword argument 'trainable'\n",
      "[INFO] Loaded weights into fresh model from: C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train1_model\\ivdd_lstm_20251215-114108_best.keras\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step\n",
      "[INFO] 保存: C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\test\\eval_outputs\\20251215-115016\\window_predictions_20251215-115016.csv\n",
      "\n",
      "[Window-level] classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ivdd     0.5902    0.7742    0.6698        93\n",
      "      normal     0.8073    0.6377    0.7126       138\n",
      "\n",
      "    accuracy                         0.6926       231\n",
      "   macro avg     0.6988    0.7059    0.6912       231\n",
      "weighted avg     0.7199    0.6926    0.6953       231\n",
      "\n",
      "[Window-level] confusion matrix:\n",
      " [[72 21]\n",
      " [50 88]]\n",
      "[INFO] 保存: C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\test\\eval_outputs\\20251215-115016\\roc_window_20251215-115016.png\n",
      "[INFO] 保存: C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\test\\eval_outputs\\20251215-115016\\cm_window_20251215-115016.png\n",
      "[INFO] 保存: C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\test\\eval_outputs\\20251215-115016\\file_level_predictions_20251215-115016.csv\n",
      "[INFO] 保存: C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\test\\eval_outputs\\20251215-115016\\file_level_errors_20251215-115016.csv\n",
      "[INFO] 保存: C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\test\\eval_outputs\\20251215-115016\\cm_file_majority_20251215-115016.png\n",
      "[INFO] 保存: C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\test\\eval_outputs\\20251215-115016\\cm_file_meanprob_20251215-115016.png\n",
      "\n",
      "[Done] すべての出力は以下に保存されました：\n",
      "C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\test\\eval_outputs\\20251215-115016\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Binary IVDD validation (ivdd vs normal) from DeepLabCut CSV (3-level header)\n",
    "\n",
    "- 学習と同じ前処理を選択可能: NORM_MODE = \"zscore\" (train1) or \"tailset_minmax\" (train2)\n",
    "- 5 keypoints × (x,y) = 10 dims\n",
    "- 窓長/ストライドは学習と合わせる\n",
    "- 学習済み .keras を読み込んで、評価・混同行列・ROC・各種CSV出力\n",
    "- 出力は test/eval_outputs/YYYYMMDD-HHMMSS/ 配下に保存（eval_csv 配下には作られません）\n",
    "- 単一ロジット（sigmoid）/ 2ロジット（softmax）を自動判定\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "# 安定運用（任意）\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"-1\")\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "\n",
    "import re\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    roc_auc_score, roc_curve\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TF:\", tf.__version__)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ========= パラメータ =========\n",
    "# ルート（test 直下を指す）※ここだけ環境に合わせてください\n",
    "TEST_ROOT = r\"C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\test\"\n",
    "\n",
    "# 評価用CSV（eval_csv 配下）\n",
    "CSV_DIR   = os.path.join(TEST_ROOT, \"eval_csv\")\n",
    "CSV_GLOB  = os.path.join(CSV_DIR, \"*.csv\")\n",
    "\n",
    "# 出力先（eval_outputs/YYYYMMDD-HHMMSS 配下に全部出す）\n",
    "OUT_BASE  = os.path.join(TEST_ROOT, \"eval_outputs\")\n",
    "RUN_ID    = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "OUT_DIR   = os.path.join(OUT_BASE, RUN_ID)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# 学習済みモデルのパス（例：train2 の best）\n",
    "CKPT_PATH = r\"C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train1_model\\ivdd_lstm_20251215-114108_best.keras\"\n",
    "\n",
    "# キーポイント（順序が (x,y) の並び順に反映）\n",
    "KEYPOINTS = [\n",
    "    \"left back paw\",\n",
    "    \"right back paw\",\n",
    "    \"left front paw\",\n",
    "    \"right front paw\",\n",
    "    \"tail set\",\n",
    "]\n",
    "\n",
    "# 正規化のモード: \"zscore\"（train1） or \"tailset_minmax\"（train2）\n",
    "NORM_MODE = \"zscore\"          # ← train1 に合わせる場合\n",
    "# NORM_MODE = \"tailset_minmax\"  # ← train2 に合わせる場合\n",
    "\n",
    "# DLC likelihood 閾値（低信頼点の補間に使う）\n",
    "USE_LIKELIHOOD      = False\n",
    "MIN_KEEP_LIKELIHOOD = 0.6\n",
    "\n",
    "# 窓長とストライド（学習と合わせる）\n",
    "SEQ_LEN  = 60\n",
    "STRIDE   = 30\n",
    "\n",
    "# 次元・ネットワーク・ラベル\n",
    "DIMS          = 10  # 5点×(x,y)\n",
    "N_HIDDEN      = 30\n",
    "CLASS_NAMES   = [\"ivdd\", \"normal\"]           # 0=ivdd, 1=normal\n",
    "CLASS_TO_IDX  = {c: i for i, c in enumerate(CLASS_NAMES)}\n",
    "IDX_TO_CLASS  = {v: k for k, v in CLASS_TO_IDX.items()}\n",
    "\n",
    "# ========= ラベル推定（ファイル名から） =========\n",
    "def infer_label_from_filename(path: str) -> int:\n",
    "    \"\"\"\n",
    "    ファイル名からラベルを推定。\n",
    "    ・厳密一致 'ivdd' / 'normal' を最優先\n",
    "    ・次点で 'ivdd\\\\d*' / 'normal\\\\d*'（ivdd1, normal2 等）\n",
    "    ・未確定なら先頭トークンが 'ivdd' / 'normal'\n",
    "    ・さらに未確定なら親ディレクトリのトークンを見る（片方のみヒット）\n",
    "\n",
    "    'IvddOct30' のような文字付き接頭辞は ivdd と見なしません（誤検出回避）。\n",
    "    \"\"\"\n",
    "    name = os.path.basename(path).lower()\n",
    "    stem = os.path.splitext(name)[0]\n",
    "    tokens = [t for t in re.split(r'[^a-z0-9]+', stem) if t]\n",
    "    token_set = set(tokens)\n",
    "\n",
    "    # 1) 厳密一致\n",
    "    if ('ivdd' in token_set) and ('normal' not in token_set):\n",
    "        return CLASS_TO_IDX[\"ivdd\"]\n",
    "    if ('normal' in token_set) and ('ivdd' not in token_set):\n",
    "        return CLASS_TO_IDX[\"normal\"]\n",
    "\n",
    "    # 2) ivdd[digits]* / normal[digits]*\n",
    "    def looks_like(label: str, tok: str) -> bool:\n",
    "        # label + digits のみ許可（例: ivdd1, normal10 は可 / ivddoct は不可）\n",
    "        return re.fullmatch(rf\"{label}\\d*\", tok) is not None\n",
    "\n",
    "    ivdd_like   = any(looks_like(\"ivdd\", t) for t in tokens)\n",
    "    normal_like = any(looks_like(\"normal\", t) for t in tokens)\n",
    "    if ivdd_like and not normal_like:\n",
    "        return CLASS_TO_IDX[\"ivdd\"]\n",
    "    if normal_like and not ivdd_like:\n",
    "        return CLASS_TO_IDX[\"normal\"]\n",
    "\n",
    "    # 3) 先頭トークン\n",
    "    if tokens and tokens[0] in CLASS_TO_IDX:\n",
    "        return CLASS_TO_IDX[tokens[0]]\n",
    "\n",
    "    # 4) 親ディレクトリ\n",
    "    parent_tokens = [t for t in re.split(r'[^a-z0-9]+', os.path.dirname(path).lower()) if t]\n",
    "    pset = set(parent_tokens)\n",
    "    if ('ivdd' in pset) and ('normal' not in pset):\n",
    "        return CLASS_TO_IDX[\"ivdd\"]\n",
    "    if ('normal' in pset) and ('ivdd' not in pset):\n",
    "        return CLASS_TO_IDX[\"normal\"]\n",
    "\n",
    "    raise ValueError(f\"ラベル不明: {name}（先頭を ivdd_ / normal_ 推奨。'ivdd1_' 等も可）\")\n",
    "\n",
    "# ========= DLC 前処理 =========\n",
    "def _norm_name(s: str) -> str:\n",
    "    return \"\".join(ch for ch in s.lower() if ch not in \" _-\")\n",
    "\n",
    "def _resolve_keypoints(all_bodyparts, requested):\n",
    "    norm2orig = {}\n",
    "    for bp in all_bodyparts:\n",
    "        k = _norm_name(bp)\n",
    "        if k not in norm2orig:\n",
    "            norm2orig[k] = bp\n",
    "    resolved, missing = [], []\n",
    "    for req in requested:\n",
    "        k = _norm_name(req)\n",
    "        if k in norm2orig:\n",
    "            resolved.append(norm2orig[k])\n",
    "        else:\n",
    "            missing.append(req)\n",
    "    if missing:\n",
    "        raise ValueError(f\"指定キーポイントがCSVで見つかりません: {missing}\\n利用可能: {all_bodyparts}\")\n",
    "    return resolved\n",
    "\n",
    "def read_dlc_5kp_xy(csv_path: str,\n",
    "                    keypoints,\n",
    "                    use_likelihood=True,\n",
    "                    min_keep_likelihood=0.6):\n",
    "    \"\"\"\n",
    "    DLC(3段ヘッダ) -> 指定5点 (x,y) 抽出 -> 低likelihood NaN -> 補間 -> 0埋め\n",
    "    返: (T,10), used_kps(list[str])\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, header=[0,1,2], index_col=0)\n",
    "    bodyparts = list({bp for (_, bp, _) in df.columns})\n",
    "    use_kps = _resolve_keypoints(bodyparts, keypoints)\n",
    "\n",
    "    cols = {}\n",
    "    for bp in use_kps:\n",
    "        cols[f\"{bp}_x\"] = df.xs((bp, \"x\"), level=[1,2], axis=1)\n",
    "        cols[f\"{bp}_y\"] = df.xs((bp, \"y\"), level=[1,2], axis=1)\n",
    "    X_df = pd.concat(cols.values(), axis=1)\n",
    "    X_df.columns = list(cols.keys())\n",
    "\n",
    "    if use_likelihood:\n",
    "        for bp in use_kps:\n",
    "            try:\n",
    "                lcol = df.xs((bp, \"likelihood\"), level=[1,2], axis=1).values.flatten()\n",
    "                low = lcol < min_keep_likelihood\n",
    "                for c in [f\"{bp}_x\", f\"{bp}_y\"]:\n",
    "                    v = X_df[c].values\n",
    "                    v[low] = np.nan\n",
    "                    X_df[c] = v\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "    X_df = X_df.interpolate(method=\"linear\", limit_direction=\"both\", axis=0)\n",
    "    X_df = X_df.bfill().ffill().fillna(0.0)\n",
    "\n",
    "    X = X_df.values.astype(np.float32)\n",
    "    return X, use_kps\n",
    "\n",
    "# --- 正規化 ---\n",
    "def zscore_per_file(X: np.ndarray, eps: float=1e-6) -> np.ndarray:\n",
    "    mu = X.mean(axis=0, keepdims=True)\n",
    "    sd = X.std(axis=0, keepdims=True)\n",
    "    return (X - mu) / (sd + eps)\n",
    "\n",
    "def normalize_tailset_minmax(\n",
    "    X: np.ndarray, used_kps: list[str], ref_name: str = \"tail set\", eps: float = 1e-6\n",
    ") -> np.ndarray:\n",
    "    low = [s.lower() for s in used_kps]\n",
    "    if ref_name.lower() not in low:\n",
    "        raise ValueError(f\"'{ref_name}' が used_kps に見つかりません: {used_kps}\")\n",
    "    ref_idx = low.index(ref_name.lower())\n",
    "\n",
    "    # 平行移動（tail_set を原点へ）\n",
    "    cx = X[:, 2*ref_idx]\n",
    "    cy = X[:, 2*ref_idx + 1]\n",
    "    Xc = X.copy()\n",
    "    for i in range(len(used_kps)):\n",
    "        Xc[:, 2*i]   -= cx\n",
    "        Xc[:, 2*i+1] -= cy\n",
    "\n",
    "    # min-max（各次元独立）\n",
    "    x_min = Xc.min(axis=0, keepdims=True)\n",
    "    x_max = Xc.max(axis=0, keepdims=True)\n",
    "    Xn = (Xc - x_min) / (x_max - x_min + eps)\n",
    "    return Xn\n",
    "\n",
    "def apply_normalization(X_raw: np.ndarray, used_kps: list[str]) -> np.ndarray:\n",
    "    if NORM_MODE == \"zscore\":\n",
    "        return zscore_per_file(X_raw)\n",
    "    elif NORM_MODE == \"tailset_minmax\":\n",
    "        return normalize_tailset_minmax(X_raw, used_kps, ref_name=\"tail set\")\n",
    "    else:\n",
    "        raise ValueError(f\"未知の NORM_MODE: {NORM_MODE}\")\n",
    "\n",
    "# --- 窓切り ---\n",
    "def make_windows(X: np.ndarray, seq_len: int, stride: int):\n",
    "    n = X.shape[0]\n",
    "    if n < seq_len:\n",
    "        return np.empty((0, seq_len, X.shape[1]), dtype=X.dtype), []\n",
    "    starts = list(range(0, n - seq_len + 1, stride))\n",
    "    Xw = np.stack([X[s:s+seq_len] for s in starts], axis=0)\n",
    "    return Xw, starts\n",
    "\n",
    "def build_dataset(csv_paths, seq_len=SEQ_LEN, stride=STRIDE):\n",
    "    X_list, y_list, file_ids, starts_all = [], [], [], []\n",
    "    used_kps_any = None\n",
    "    for p in csv_paths:\n",
    "        y_lab = infer_label_from_filename(p)   # 0/1\n",
    "        X_raw, used_kps = read_dlc_5kp_xy(\n",
    "            p, keypoints=KEYPOINTS,\n",
    "            use_likelihood=USE_LIKELIHOOD, min_keep_likelihood=MIN_KEEP_LIKELIHOOD\n",
    "        )\n",
    "        if used_kps_any is None:\n",
    "            used_kps_any = used_kps\n",
    "        if X_raw.shape[1] != DIMS:\n",
    "            raise ValueError(f\"{os.path.basename(p)}: 次元 {X_raw.shape[1]} != 期待 {DIMS}\")\n",
    "\n",
    "        X_norm = apply_normalization(X_raw, used_kps)\n",
    "        X_win, starts = make_windows(X_norm, seq_len, stride)\n",
    "        if X_win.shape[0] == 0:\n",
    "            print(f\"[WARN] {os.path.basename(p)}: フレーム不足（{seq_len}未満）でスキップ\")\n",
    "            continue\n",
    "\n",
    "        X_list.append(X_win)\n",
    "        y_list.append(np.full((X_win.shape[0],), y_lab, dtype=np.int64))\n",
    "        file_ids += [os.path.basename(p)] * X_win.shape[0]\n",
    "        starts_all += starts\n",
    "\n",
    "    if not X_list:\n",
    "        raise RuntimeError(\"評価用データが作れませんでした。eval_csv に CSV があるか、命名規則（ivdd/normal）を確認してください。\")\n",
    "\n",
    "    X = np.concatenate(X_list, axis=0)\n",
    "    y = np.concatenate(y_list, axis=0)\n",
    "    file_ids = np.array(file_ids)\n",
    "    starts_all = np.array(starts_all)\n",
    "    print(f\"[INFO] 使用キーポイント: {used_kps_any}\")\n",
    "    return X, y, file_ids, starts_all\n",
    "\n",
    "# ========= モデル読込 =========\n",
    "class LSTM_RNN(keras.Model):\n",
    "    def __init__(self, n_input, n_hidden, n_classes):\n",
    "        super().__init__()\n",
    "        self.input_dense = keras.layers.Dense(n_hidden, activation='relu')\n",
    "        self.time_dist   = keras.layers.TimeDistributed(self.input_dense)\n",
    "        self.lstm1 = keras.layers.LSTM(n_hidden, return_sequences=True)\n",
    "        self.lstm2 = keras.layers.LSTM(n_hidden)\n",
    "        self.out   = keras.layers.Dense(n_classes)  # logits\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.time_dist(x)\n",
    "        x = self.lstm1(x, training=training)\n",
    "        x = self.lstm2(x, training=training)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class LSTMWithL2(LSTM_RNN):\n",
    "    def __init__(self, n_input, n_hidden, n_classes, l2_lambda=1e-4):\n",
    "        super().__init__(n_input, n_hidden, n_classes)\n",
    "        self.l2_lambda = l2_lambda\n",
    "\n",
    "def load_trained_model():\n",
    "    try:\n",
    "        m = keras.models.load_model(\n",
    "            CKPT_PATH,\n",
    "            custom_objects={\"LSTMWithL2\": LSTMWithL2, \"LSTM_RNN\": LSTM_RNN},\n",
    "            compile=False\n",
    "        )\n",
    "        print(f\"[INFO] Loaded full model: {CKPT_PATH}\")\n",
    "        return m\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] load_model 失敗: {e}\")\n",
    "        # weights のみ保存されていた場合のフォールバック\n",
    "        m = LSTMWithL2(n_input=DIMS, n_hidden=N_HIDDEN, n_classes=2)\n",
    "        _ = m(tf.zeros([1, SEQ_LEN, DIMS]))\n",
    "        try:\n",
    "            m.load_weights(CKPT_PATH)\n",
    "            print(f\"[INFO] Loaded weights into fresh model from: {CKPT_PATH}\")\n",
    "            return m\n",
    "        except Exception as e2:\n",
    "            raise RuntimeError(f\"モデルの読み込みに失敗しました。: {e2}\")\n",
    "\n",
    "# ========= 入力CSVの収集 =========\n",
    "csv_files = sorted(glob.glob(CSV_GLOB))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"評価用CSVが見つかりません: {CSV_GLOB}\")\n",
    "\n",
    "# ========= データ構築 =========\n",
    "X, y, file_ids, starts = build_dataset(csv_files, SEQ_LEN, STRIDE)\n",
    "print(\"X:\", X.shape, \"y:\", y.shape, \"files:\", len(np.unique(file_ids)))\n",
    "\n",
    "# ========= モデル & 推論 =========\n",
    "model = load_trained_model()\n",
    "logits = model.predict(X, batch_size=64)\n",
    "logits = np.asarray(logits)\n",
    "\n",
    "# 単一ロジット（sigmoid）/ 2ロジット（softmax）を自動判定\n",
    "if logits.ndim == 1 or logits.shape[1] == 1:\n",
    "    p_ivdd   = tf.math.sigmoid(logits).numpy().reshape(-1)\n",
    "    p_normal = 1.0 - p_ivdd\n",
    "    probs    = np.stack([p_ivdd, p_normal], axis=1)   # 列順を [\"ivdd\",\"normal\"] に合わせる\n",
    "    y_pred   = (p_ivdd >= 0.5).astype(int) * CLASS_TO_IDX[\"ivdd\"] + (p_ivdd < 0.5).astype(int) * CLASS_TO_IDX[\"normal\"]\n",
    "else:\n",
    "    probs    = tf.nn.softmax(logits, axis=1).numpy()\n",
    "    p_ivdd   = probs[:, CLASS_TO_IDX[\"ivdd\"]]\n",
    "    p_normal = probs[:, CLASS_TO_IDX[\"normal\"]]\n",
    "    y_pred   = probs.argmax(axis=1)\n",
    "\n",
    "# ========= 保存パス群（すべて RUN_ID 付きファイル名） =========\n",
    "paths = {\n",
    "    \"win_csv\":           os.path.join(OUT_DIR, f\"window_predictions_{RUN_ID}.csv\"),\n",
    "    \"roc_png\":           os.path.join(OUT_DIR, f\"roc_window_{RUN_ID}.png\"),\n",
    "    \"cm_window_png\":     os.path.join(OUT_DIR, f\"cm_window_{RUN_ID}.png\"),\n",
    "    \"file_pred_csv\":     os.path.join(OUT_DIR, f\"file_level_predictions_{RUN_ID}.csv\"),\n",
    "    \"file_errors_csv\":   os.path.join(OUT_DIR, f\"file_level_errors_{RUN_ID}.csv\"),\n",
    "    \"cm_file_major_png\": os.path.join(OUT_DIR, f\"cm_file_majority_{RUN_ID}.png\"),\n",
    "    \"cm_file_mean_png\":  os.path.join(OUT_DIR, f\"cm_file_meanprob_{RUN_ID}.png\"),\n",
    "}\n",
    "\n",
    "# ========= 予測CSV（ウィンドウ） =========\n",
    "df_win = pd.DataFrame({\n",
    "    \"file\": file_ids, \"start\": starts, \"true\": y, \"pred\": y_pred,\n",
    "    \"p_ivdd\": p_ivdd, \"p_normal\": p_normal,\n",
    "}).sort_values([\"file\",\"start\"])\n",
    "df_win.to_csv(paths[\"win_csv\"], index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[INFO] 保存:\", paths[\"win_csv\"])\n",
    "\n",
    "# ========= レポート（ウィンドウ単位） =========\n",
    "print(\"\\n[Window-level] classification_report:\")\n",
    "print(classification_report(y, y_pred, target_names=CLASS_NAMES, digits=4))\n",
    "cm = confusion_matrix(y, y_pred, labels=[CLASS_TO_IDX[\"ivdd\"], CLASS_TO_IDX[\"normal\"]])\n",
    "print(\"[Window-level] confusion matrix:\\n\", cm)\n",
    "\n",
    "# ========= ROC（ウィンドウ単位; ivdd を陽性） =========\n",
    "try:\n",
    "    y_ivdd = (y == CLASS_TO_IDX[\"ivdd\"]).astype(int)\n",
    "    auc_val = roc_auc_score(y_ivdd, p_ivdd)\n",
    "    fpr, tpr, _ = roc_curve(y_ivdd, p_ivdd, pos_label=1)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {auc_val:.3f}\")\n",
    "    plt.plot([0,1], [0,1], linestyle=\"--\")\n",
    "    plt.title(\"ROC (window-level)\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(paths[\"roc_png\"], dpi=150)\n",
    "    plt.close()\n",
    "    print(\"[INFO] 保存:\", paths[\"roc_png\"])\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] ROC プロット失敗: {e}\")\n",
    "\n",
    "# ========= 混同行列プロット（共通関数） =========\n",
    "def plot_cm(cm_mat, labels, title, path_png):\n",
    "    cmn = cm_mat.astype(np.float32) / (cm_mat.sum() + 1e-6)\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    im = ax.imshow(cmn, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(len(labels)), yticks=np.arange(len(labels)),\n",
    "           xticklabels=labels, yticklabels=labels,\n",
    "           ylabel='True label', xlabel='Predicted label',\n",
    "           title=title)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    # 生カウントを表示\n",
    "    for i in range(cm_mat.shape[0]):\n",
    "        for j in range(cm_mat.shape[1]):\n",
    "            ax.text(j, i, int(cm_mat[i, j]), ha=\"center\",\n",
    "                    color=\"white\" if cmn[i, j] > cmn.max()/2 else \"black\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# ウィンドウCM画像\n",
    "plot_cm(cm, CLASS_NAMES, \"Confusion Matrix (window-level)\", paths[\"cm_window_png\"])\n",
    "print(\"[INFO] 保存:\", paths[\"cm_window_png\"])\n",
    "\n",
    "# ========= ファイル単位の集計 =========\n",
    "# 多数決（pred の最頻）\n",
    "major_pred = df_win.groupby(\"file\")[\"pred\"].agg(lambda a: np.bincount(a).argmax())\n",
    "true_file  = df_win.groupby(\"file\")[\"true\"].first()\n",
    "\n",
    "# 平均確率 → argmax\n",
    "mean_prob = df_win.groupby(\"file\")[[\"p_ivdd\", \"p_normal\"]].mean()\n",
    "mean_pred = mean_prob.values.argmax(axis=1)\n",
    "\n",
    "# 予測CSV（ファイル単位）\n",
    "df_file = pd.DataFrame({\n",
    "    \"file\": mean_prob.index,\n",
    "    \"true\": [CLASS_NAMES[t] for t in true_file.values],\n",
    "    \"pred_majority\": [CLASS_NAMES[p] for p in major_pred.values],\n",
    "    \"pred_meanprob\": [CLASS_NAMES[p] for p in mean_pred],\n",
    "    \"p_ivdd_mean\": mean_prob[\"p_ivdd\"].values,\n",
    "    \"p_normal_mean\": mean_prob[\"p_normal\"].values,\n",
    "})\n",
    "df_file.to_csv(paths[\"file_pred_csv\"], index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[INFO] 保存:\", paths[\"file_pred_csv\"])\n",
    "\n",
    "# 誤分類リスト\n",
    "df_errors = df_file[df_file[\"true\"] != df_file[\"pred_meanprob\"]].copy()\n",
    "df_errors.to_csv(paths[\"file_errors_csv\"], index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[INFO] 保存:\", paths[\"file_errors_csv\"])\n",
    "\n",
    "# ========= ファイル単位の混同行列（画像保存） =========\n",
    "def _to_idx(series, name2idx={\"ivdd\":0, \"normal\":1}):\n",
    "    return series.map(name2idx).values\n",
    "\n",
    "y_true_f   = _to_idx(df_file[\"true\"])\n",
    "y_pred_maj = _to_idx(df_file[\"pred_majority\"])\n",
    "y_pred_mean= _to_idx(df_file[\"pred_meanprob\"])\n",
    "\n",
    "cm_major = confusion_matrix(y_true_f, y_pred_maj, labels=[0,1])\n",
    "cm_mean  = confusion_matrix(y_true_f, y_pred_mean, labels=[0,1])\n",
    "\n",
    "plot_cm(cm_major, CLASS_NAMES, \"Confusion Matrix (File-level, Majority Vote)\", paths[\"cm_file_major_png\"])\n",
    "plot_cm(cm_mean,  CLASS_NAMES, \"Confusion Matrix (File-level, Mean Probability)\", paths[\"cm_file_mean_png\"])\n",
    "print(\"[INFO] 保存:\", paths[\"cm_file_major_png\"])\n",
    "print(\"[INFO] 保存:\", paths[\"cm_file_mean_png\"])\n",
    "\n",
    "print(\"\\n[Done] すべての出力は以下に保存されました：\")\n",
    "print(OUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLya",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
