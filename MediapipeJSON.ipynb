{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "542a3a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\heel_hook1.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\heel_hook2.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\heel_hook3.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\heel_hook4.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\heel_hook5.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\heel_hook6.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\heel_hook7.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\heel_hook8.mp4\n",
      "Warning: Video C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\heel_hook8.mp4 ended before collecting 180 frames. Collected 0 frames.\n",
      "Warning: For video C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\heel_hook8.mp4, only 0/180 frames were saved to c:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\MP_Data_JSON\\heel_hook\\8.\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\deadpoint1.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\deadpoint2.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\deadpoint3.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\deadpoint4.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\deadpoint5.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\deadpoint6.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\deadpoint7.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\deadpoint8.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\dyno1.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\dyno2.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\dyno3.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\dyno4.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\dyno5.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\dyno6.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\dyno7.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\dyno8.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\cross_move1.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\cross_move2.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\cross_move3.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\cross_move4.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\cross_move5.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\cross_move6.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\cross_move7.mp4\n",
      "Processing video: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos\\cross_move8.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import json # 追加: jsonモジュールをインポート\n",
    "\n",
    "# 設定 (変更なし)\n",
    "DATA_PATH = os.path.join(os.getcwd(), 'MP_Data_JSON') # 保存先ディレクトリ名を変更\n",
    "# actions = np.array(['DrinkingWater', 'PushUps', 'Running', 'Squats', 'Walking', 'Standing'])\n",
    "actions = np.array(['heel_hook', 'deadpoint', 'dyno','cross_move']) # こちらを使用すると仮定\n",
    "no_videos = 8  # 各アクションの動画数\n",
    "sequence_length = 180  # 各動画のフレーム数 (1シーケンスあたりのフレーム数)\n",
    "\n",
    "# Mediapipe Holisticモデルの初期化 (変更なし)\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def multiple_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # 姿勢のキーポイントのみを描画\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=1),  # ランドマーク（点）のスタイル: 青色\n",
    "                             mp_drawing.DrawingSpec(color=(51, 255, 51), thickness=2, circle_radius=1)) # コネクション（線）のスタイル: 緑色\n",
    "\n",
    "    # --- 以下をコメントアウト ---\n",
    "    # 顔の描画は行わない\n",
    "    # mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "    #                          mp_drawing.DrawingSpec(color=(80, 110, 10), thickness=1, circle_radius=1),\n",
    "    #                          mp_drawing.DrawingSpec(color=(80, 256, 121), thickness=1, circle_radius=1))\n",
    "    # 左手の描画は行わない\n",
    "    # mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "    #                          mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=1),\n",
    "    #                          mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=1))\n",
    "    # 右手の描画は行わない\n",
    "    # mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "    #                          mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=1),\n",
    "    #                          mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=1))\n",
    "\n",
    "# ★★★ 変更点: JSON形式でキーポイントを抽出 ★★★\n",
    "def extract_keypoints(results):\n",
    "    keypoints_data = {}\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        pose_landmarks = []\n",
    "        for i, res in enumerate(results.pose_landmarks.landmark):\n",
    "            pose_landmarks.append({\n",
    "                \"id\": i,\n",
    "                \"x\": res.x,\n",
    "                \"y\": res.y,\n",
    "                \"z\": res.z,\n",
    "                \"visibility\": res.visibility\n",
    "            })\n",
    "        keypoints_data[\"pose\"] = pose_landmarks\n",
    "    else:\n",
    "        keypoints_data[\"pose\"] = [] # 空のリストとして保持\n",
    "\n",
    "    if results.face_landmarks:\n",
    "        face_landmarks = []\n",
    "        for i, res in enumerate(results.face_landmarks.landmark):\n",
    "            face_landmarks.append({\n",
    "                \"id\": i,\n",
    "                \"x\": res.x,\n",
    "                \"y\": res.y,\n",
    "                \"z\": res.z\n",
    "            })\n",
    "        keypoints_data[\"face\"] = face_landmarks\n",
    "    else:\n",
    "        keypoints_data[\"face\"] = []\n",
    "\n",
    "    if results.left_hand_landmarks:\n",
    "        lh_landmarks = []\n",
    "        for i, res in enumerate(results.left_hand_landmarks.landmark):\n",
    "            lh_landmarks.append({\n",
    "                \"id\": i,\n",
    "                \"x\": res.x,\n",
    "                \"y\": res.y,\n",
    "                \"z\": res.z\n",
    "            })\n",
    "        keypoints_data[\"left_hand\"] = lh_landmarks\n",
    "    else:\n",
    "        keypoints_data[\"left_hand\"] = []\n",
    "\n",
    "    if results.right_hand_landmarks:\n",
    "        rh_landmarks = []\n",
    "        for i, res in enumerate(results.right_hand_landmarks.landmark):\n",
    "            rh_landmarks.append({\n",
    "                \"id\": i,\n",
    "                \"x\": res.x,\n",
    "                \"y\": res.y,\n",
    "                \"z\": res.z\n",
    "            })\n",
    "        keypoints_data[\"right_hand\"] = rh_landmarks\n",
    "    else:\n",
    "        keypoints_data[\"right_hand\"] = []\n",
    "\n",
    "    return keypoints_data # 辞書を返す\n",
    "\n",
    "# データの収集\n",
    "# OpenCVのウィンドウ表示設定 (変更なし)\n",
    "cv2.namedWindow('OpenCV Feed', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('OpenCV Feed', (1280, 720))\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    for action in actions:\n",
    "        action_path = os.path.join(DATA_PATH, action) # アクションごとのディレクトリパス\n",
    "        if not os.path.exists(action_path):\n",
    "            os.makedirs(action_path)\n",
    "\n",
    "        for video_num in range(1, no_videos + 1):\n",
    "            video_file_path = os.path.join(r'C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\videos', action + str(video_num) + '.mp4') # 元の動画ファイルパス\n",
    "            cap = cv2.VideoCapture(video_file_path)\n",
    "\n",
    "            # 各動画(video_num)のデータを保存するディレクトリを作成\n",
    "            video_data_save_path = os.path.join(action_path, str(video_num))\n",
    "            if not os.path.exists(video_data_save_path):\n",
    "                os.makedirs(video_data_save_path)\n",
    "\n",
    "            print(f'Processing video: {video_file_path}')\n",
    "            frames_collected_for_current_video = 0 # この動画から収集したフレーム数をカウント\n",
    "\n",
    "            # 1つの動画から sequence_length (180) フレームを抽出 (1回だけ)\n",
    "            for frame_num in range(sequence_length):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(f\"Warning: Video {video_file_path} ended before collecting {sequence_length} frames. Collected {frame_num} frames.\")\n",
    "                    break\n",
    "\n",
    "                image, results = multiple_detection(frame, holistic)\n",
    "\n",
    "                draw_styled_landmarks(image, results)\n",
    "\n",
    "                # 収集状況を画面に表示 (変更なし)\n",
    "                cv2.putText(image, f'Collecting frames for {action} - Video {video_num} - Frame {frame_num + 1}/{sequence_length}',\n",
    "                            (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "                keypoints = extract_keypoints(results)\n",
    "                # ★★★ 変更点: JSONファイルとして保存 ★★★\n",
    "                json_path = os.path.join(video_data_save_path, f'{frame_num}.json') # .json 拡張子に変更\n",
    "                with open(json_path, 'w') as f:\n",
    "                    json.dump(keypoints, f, indent=4) # 見やすいようにindent=4を指定\n",
    "\n",
    "                frames_collected_for_current_video +=1\n",
    "\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "            cap.release()\n",
    "            if frames_collected_for_current_video < sequence_length:\n",
    "                 print(f\"Warning: For video {video_file_path}, only {frames_collected_for_current_video}/{sequence_length} frames were saved to {video_data_save_path}.\")\n",
    "\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
