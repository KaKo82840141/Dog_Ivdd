{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4356c5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: D:\\卒研手話\\video\\hikensya5\\ageru1.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\ageru2.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\ageru3.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\ageru4.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\ageru5.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\ageru6.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\ageru7.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\ageru8.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\ageru9.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\ageru10.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\understand1.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\understand2.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\understand3.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\understand4.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\understand5.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\understand6.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\understand7.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\understand8.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\understand9.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\understand10.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\annsinnsuru1.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\annsinnsuru2.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\annsinnsuru3.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\annsinnsuru4.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\annsinnsuru5.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\annsinnsuru6.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\annsinnsuru7.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\annsinnsuru8.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\annsinnsuru9.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\annsinnsuru10.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\heavy1.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\heavy2.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\heavy3.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\heavy4.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\heavy5.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\heavy6.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\heavy7.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\heavy8.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\heavy9.mp4\n",
      "Processing video: D:\\卒研手話\\video\\hikensya5\\heavy10.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import json\n",
    "\n",
    "# --- ★変更点: 描画するパーツを選択するフラグ ---\n",
    "# Trueに設定したパーツのランドマークが描画・保存されます\n",
    "DRAW_FACE = False\n",
    "DRAW_HANDS = True\n",
    "DRAW_POSE = False \n",
    "\n",
    "# 設定\n",
    "DATA_PATH = os.path.join(os.getcwd(), 'MP_Data_JSON')\n",
    "actions = np.array(['ageru', 'understand', 'annsinnsuru' , 'heavy'])\n",
    "no_videos = 10 #56\n",
    "sequence_length = 30\n",
    "\n",
    "# MediaPipe Holisticモデルの準備\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# 取得したい姿勢ランドマークのインデックスを指定\n",
    "# DRAW_POSEがTrueの場合に、ここで指定したインデックスのランドマークのみが処理されます。\n",
    "#DESIRED_POSE_LANDMARKS = [11, 12, 13, 14, 15, 16, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
    "# 顔と手は全てのランドマークを描画・保存対象としますが、\n",
    "# 特定のインデックスのみにしたい場合は、上記のようにリストを作成して利用することも可能です。\n",
    "\n",
    "def multiple_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    \"\"\"\n",
    "    指定されたフラグに基づいて、顔、手、ポーズのランドマークを描画する関数\n",
    "    \"\"\"\n",
    "    # 顔のランドマークを描画 (DRAW_FACEがTrueの場合)\n",
    "    if DRAW_FACE and results.face_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            results.face_landmarks,\n",
    "            mp_holistic.FACEMESH_TESSELATION, # 顔のメッシュ\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp_drawing.DrawingSpec(color=(200, 200, 200), thickness=1, circle_radius=1) # メッシュを薄い灰色で描画\n",
    "        )\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            results.face_landmarks,\n",
    "            mp_holistic.FACEMESH_CONTOURS, # 顔の輪郭\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp_drawing.DrawingSpec(color=(224, 224, 224), thickness=1, circle_radius=1) # 輪郭を少し濃い灰色で描画\n",
    "        )\n",
    "\n",
    "    # 手のランドマークを描画 (DRAW_HANDSがTrueの場合)\n",
    "    if DRAW_HANDS:\n",
    "        # 左手\n",
    "        if results.left_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.left_hand_landmarks,\n",
    "                mp_holistic.HAND_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2), # 点を赤色で描画\n",
    "                connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 100, 100), thickness=2) # 線を薄い赤色で描画\n",
    "            )\n",
    "        # 右手\n",
    "        if results.right_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.right_hand_landmarks,\n",
    "                mp_holistic.HAND_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=2), # 点を青色で描画\n",
    "                connection_drawing_spec=mp_drawing.DrawingSpec(color=(100, 100, 255), thickness=2) # 線を薄い青色で描画\n",
    "            )\n",
    "\n",
    "    # 姿勢のランドマークを描画 (DRAW_POSEがTrueの場合)\n",
    "    # この部分は元のコードのカスタム描画ロジックを維持\n",
    "    if DRAW_POSE and results.pose_landmarks:\n",
    "        image_height, image_width, _ = image.shape\n",
    "        # 点の描画\n",
    "        for idx in DESIRED_POSE_LANDMARKS:\n",
    "            if idx < len(results.pose_landmarks.landmark):\n",
    "                landmark = results.pose_landmarks.landmark[idx]\n",
    "                if landmark.visibility < 0.1:\n",
    "                    continue\n",
    "                cx = int(landmark.x * image_width)\n",
    "                cy = int(landmark.y * image_height)\n",
    "                cv2.circle(image, (cx, cy), 3, (0, 255, 0), cv2.FILLED) # ポーズの点は緑色\n",
    "\n",
    "        # 線の描画\n",
    "        if mp_holistic.POSE_CONNECTIONS:\n",
    "            for connection in mp_holistic.POSE_CONNECTIONS:\n",
    "                start_idx, end_idx = connection\n",
    "                if start_idx in DESIRED_POSE_LANDMARKS and end_idx in DESIRED_POSE_LANDMARKS:\n",
    "                    if start_idx < len(results.pose_landmarks.landmark) and end_idx < len(results.pose_landmarks.landmark):\n",
    "                        start_lm = results.pose_landmarks.landmark[start_idx]\n",
    "                        end_lm = results.pose_landmarks.landmark[end_idx]\n",
    "                        if start_lm.visibility < 0.1 or end_lm.visibility < 0.1:\n",
    "                            continue\n",
    "                        start_point = (int(start_lm.x * image_width), int(start_lm.y * image_height))\n",
    "                        end_point = (int(end_lm.x * image_width), int(end_lm.y * image_height))\n",
    "                        cv2.line(image, start_point, end_point, (51, 255, 51), 2)\n",
    "\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    \"\"\"\n",
    "    指定されたフラグに基づいて、顔、手、ポーズのキーポイントを抽出する関数\n",
    "    \"\"\"\n",
    "    keypoints_data = {}\n",
    "\n",
    "    # 顔のランドマークを抽出 (DRAW_FACEがTrueの場合)\n",
    "    if DRAW_FACE:\n",
    "        if results.face_landmarks:\n",
    "            face_landmarks_data = [{\"id\": i, \"x\": res.x, \"y\": res.y, \"z\": res.z} for i, res in enumerate(results.face_landmarks.landmark)]\n",
    "            keypoints_data[\"face\"] = face_landmarks_data\n",
    "        else:\n",
    "            keypoints_data[\"face\"] = []\n",
    "    \n",
    "    # 左手のランドマークを抽出 (DRAW_HANDSがTrueの場合)\n",
    "    if DRAW_HANDS:\n",
    "        if results.left_hand_landmarks:\n",
    "            left_hand_landmarks_data = [{\"id\": i, \"x\": res.x, \"y\": res.y, \"z\": res.z} for i, res in enumerate(results.left_hand_landmarks.landmark)]\n",
    "            keypoints_data[\"left_hand\"] = left_hand_landmarks_data\n",
    "        else:\n",
    "            keypoints_data[\"left_hand\"] = []\n",
    "\n",
    "    # 右手のランドマークを抽出 (DRAW_HANDSがTrueの場合)\n",
    "    if DRAW_HANDS:\n",
    "        if results.right_hand_landmarks:\n",
    "            right_hand_landmarks_data = [{\"id\": i, \"x\": res.x, \"y\": res.y, \"z\": res.z} for i, res in enumerate(results.right_hand_landmarks.landmark)]\n",
    "            keypoints_data[\"right_hand\"] = right_hand_landmarks_data\n",
    "        else:\n",
    "            keypoints_data[\"right_hand\"] = []\n",
    "\n",
    "    # 姿勢のランドマークを抽出 (DRAW_POSEがTrueの場合)\n",
    "    if DRAW_POSE:\n",
    "        if results.pose_landmarks:\n",
    "            pose_landmarks_data = []\n",
    "            for i, res in enumerate(results.pose_landmarks.landmark):\n",
    "                if i in DESIRED_POSE_LANDMARKS:\n",
    "                    pose_landmarks_data.append({\"id\": i, \"x\": res.x, \"y\": res.y, \"z\": res.z, \"visibility\": res.visibility})\n",
    "            keypoints_data[\"pose\"] = pose_landmarks_data\n",
    "        else:\n",
    "            keypoints_data[\"pose\"] = []\n",
    "    \n",
    "    return keypoints_data\n",
    "\n",
    "# データの収集\n",
    "cv2.namedWindow('OpenCV Feed', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('OpenCV Feed', (1280, 720))\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic_model:\n",
    "    for action in actions:\n",
    "        action_path = os.path.join(DATA_PATH, action)\n",
    "        if not os.path.exists(action_path):\n",
    "            os.makedirs(action_path)\n",
    "\n",
    "        for video_num in range(1, no_videos + 1):\n",
    "            #video_file_path = os.path.join(r\"C:\\Users\\admin\\Downloads\\syuwavideo\", action + str(video_num) + '.mp4')\n",
    "            video_file_path = os.path.join(r\"D:\\卒研手話\\video\\hikensya5\", action + str(video_num) + '.mp4')\n",
    "            cap = cv2.VideoCapture(video_file_path)\n",
    "\n",
    "            video_data_save_path = os.path.join(action_path, str(video_num))\n",
    "            if not os.path.exists(video_data_save_path):\n",
    "                os.makedirs(video_data_save_path)\n",
    "\n",
    "            print(f'Processing video: {video_file_path}')\n",
    "            frames_collected_for_current_video = 0\n",
    "\n",
    "            for frame_num in range(sequence_length):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(f\"Warning: Video {video_file_path} ended before collecting {sequence_length} frames. Collected {frame_num} frames.\")\n",
    "                    break\n",
    "\n",
    "                image, results = multiple_detection(frame, holistic_model)\n",
    "                draw_styled_landmarks(image, results)\n",
    "\n",
    "                cv2.putText(image, f'Collecting frames for {action} - Video {video_num} - Frame {frame_num + 1}/{sequence_length}',\n",
    "                            (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "                keypoints = extract_keypoints(results)\n",
    "                json_path = os.path.join(video_data_save_path, f'{frame_num}.json')\n",
    "                with open(json_path, 'w') as f:\n",
    "                    json.dump(keypoints, f, indent=4)\n",
    "\n",
    "                frames_collected_for_current_video += 1\n",
    "\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            \n",
    "            cap.release()\n",
    "            if frames_collected_for_current_video < sequence_length:\n",
    "                print(f\"Warning: For video {video_file_path}, only {frames_collected_for_current_video}/{sequence_length} frames were saved to {video_data_save_path}.\")\n",
    "\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
