{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f9dfcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Processing activity: heel_hook in C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\MP_Data_JSON\\heel_hook\n",
      "  Changed CWD to: C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\MP_Data_JSON\\heel_hook\n",
      "    Processing file: heel_hookl01_c01_s01_a01_r01_mp33.txt\n",
      "      Assigning to train set.\n",
      "      Warning: File heel_hookl01_c01_s01_a01_r01_mp33.txt has 174 frames, less than num_steps (180). Skipping.\n",
      "    Processing file: heel_hookl01_c01_s02_a01_r01_mp33.txt\n",
      "      Assigning to train set.\n",
      "      Total frames: 180, Num sequences possible: 1\n",
      "      Appended 1 sequences to C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\MP_Data_JSON\\X_train.txt\n",
      "      Appended 1 labels to C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\MP_Data_JSON\\Y_train.txt\n",
      "    Processing file: heel_hookl01_c01_s03_a01_r01_mp33.txt\n",
      "      Assigning to train set.\n",
      "      Total frames: 180, Num sequences possible: 1\n",
      "      Appended 1 sequences to C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\MP_Data_JSON\\X_train.txt\n",
      "      Appended 1 labels to C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\MP_Data_JSON\\Y_train.txt\n",
      "    Processing file: heel_hookl01_c01_s04_a01_r01_mp33.txt\n",
      "      Assigning to train set.\n",
      "      Total frames: 180, Num sequences possible: 1\n",
      "      Appended 1 sequences to C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\MP_Data_JSON\\X_train.txt\n",
      "      Appended 1 labels to C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\MP_Data_JSON\\Y_train.txt\n",
      "    Processing file: heel_hookl01_c01_s05_a01_r01_mp33.txt\n",
      "      Assigning to train set.\n",
      "      Total frames: 180, Num sequences possible: 1\n",
      "      Appended 1 sequences to C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\MP_Data_JSON\\X_train.txt\n",
      "      Appended 1 labels to C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\MP_Data_JSON\\Y_train.txt\n",
      "    Processing file: heel_hookl01_c01_s06_a01_r01_mp33.txt\n",
      "      Assigning to train set.\n",
      "      Total frames: 180, Num sequences possible: 1\n",
      "      Appended 1 sequences to C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\MP_Data_JSON\\X_train.txt\n",
      "      Appended 1 labels to C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\MP_Data_JSON\\Y_train.txt\n",
      "    Processing file: heel_hookl01_c01_s07_a01_r01_mp33.txt\n",
      "      Assigning to train set.\n",
      "      Total frames: 180, Num sequences possible: 1\n",
      "      Appended 1 sequences to C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\MP_Data_JSON\\X_train.txt\n",
      "      Appended 1 labels to C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\MP_Data_JSON\\Y_train.txt\n",
      "    Processing file: heel_hookl01_c01_s08_a01_r01_mp33.txt\n",
      "      Assigning to train set.\n",
      "      Total frames: 180, Num sequences possible: 1\n",
      "      Appended 1 sequences to C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\MP_Data_JSON\\X_train.txt\n",
      "      Appended 1 labels to C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\MP_Data_JSON\\Y_train.txt\n",
      "    Processing file: heel_hookl01_c01_s09_a01_r01_mp33.txt\n",
      "      Assigning to train set.\n",
      "      Total frames: 180, Num sequences possible: 1\n",
      "      Appended 1 sequences to C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\MP_Data_JSON\\X_train.txt\n",
      "      Appended 1 labels to C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\MP_Data_JSON\\Y_train.txt\n",
      "    Processing file: heel_hookl01_c01_s10_a01_r01_mp33.txt\n",
      "      Assigning to train set.\n",
      "      Total frames: 180, Num sequences possible: 1\n",
      "      Appended 1 sequences to C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\MP_Data_JSON\\X_train.txt\n",
      "      Appended 1 labels to C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\MP_Data_JSON\\Y_train.txt\n",
      "    Processing file: heel_hookl01_c01_s11_a01_r01_mp33.txt\n",
      "      Assigning to train set.\n"
     ]
    },
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEmptyDataError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 110\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m#     with open(file_name_in_activity_dir, 'r') as data_file_handle: # file_name_in_activity_dir is relative to CWD\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m#         file_text = data_file_handle.readlines()\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    104\u001b[39m \n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# --- ★★★ 線形補間のための新しいコードブロック ★★★ ---\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    107\u001b[39m     \u001b[38;5;66;03m# pandasのread_csvを使って直接データを読み込む\u001b[39;00m\n\u001b[32m    108\u001b[39m     \u001b[38;5;66;03m# ヘッダーがないので header=None を指定\u001b[39;00m\n\u001b[32m    109\u001b[39m     \u001b[38;5;66;03m# この時点で、0.0 の値は欠損値(NaN)として扱われるようにする\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     data_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name_in_activity_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m     \u001b[38;5;66;03m# 線形補間を実行\u001b[39;00m\n\u001b[32m    113\u001b[39m     \u001b[38;5;66;03m# DataFrame全体に対して一度に補間を適用できる\u001b[39;00m\n\u001b[32m    114\u001b[39m     data_df.interpolate(method=\u001b[33m'\u001b[39m\u001b[33mlinear\u001b[39m\u001b[33m'\u001b[39m, axis=\u001b[32m0\u001b[39m, limit_direction=\u001b[33m'\u001b[39m\u001b[33mboth\u001b[39m\u001b[33m'\u001b[39m, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\ML\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\ML\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\ML\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\ML\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\ML\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:581\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mEmptyDataError\u001b[39m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Creates database from converted MediaPipe output files (should now be in .txt format with _mp33 suffix)\n",
    "for use with RNN for Human Activity Recognition - 2D Pose Input (now 33*2D = 66 features)\n",
    "\n",
    "Adapted from original OpenPose script by Stuart Eiffert 13/12/2017\n",
    "Modifications for MediaPipe (33 points) by Your Name/AI DATE\n",
    "\n",
    "All code is provided under the MIT License (assuming original license applies)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# import json # Not used in this script\n",
    "# from pprint import pprint # Not used in this script\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Configuration for Output Files ---\n",
    "# These files will be created in the 'data_path' directory specified below.\n",
    "test_file_X = \"X_test.txt\"\n",
    "test_file_Y = \"Y_test.txt\"\n",
    "train_file_X = \"X_train.txt\"\n",
    "train_file_Y = \"Y_train.txt\"\n",
    "\n",
    "# --- Configuration based on your MediaPipe setup ---\n",
    "# MODIFIED: Path to the root directory where action folders (containing _mp33.txt files) are located.\n",
    "# This should be the 'MP_Data_JSON' directory.\n",
    "# If MP_Data_JSON is in the same directory as this script, a relative path is fine.\n",
    "data_path = r\"C:\\Users\\admin\\Downloads\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\MP_Data_JSON\"  # Example: \"MP_Data_JSON\" or \"/path/to/your/MP_Data_JSON\"\n",
    "# To use the script's current directory as a base for MP_Data_JSON:\n",
    "# data_path_abs = os.path.join(os.getcwd(), \"MP_Data_JSON\") # Use this if you prefer an absolute path from start\n",
    "\n",
    "# MODIFIED: Your list of actions from the MediaPipe script\n",
    "activity_list = ['heel_hook', 'deadpoint', 'dyno', 'cross_move']\n",
    "#activity_list = ['deadpoint', 'cross_move']\n",
    "# Original script's variables, kept for context\n",
    "# cluster_nums=4\n",
    "# camera_nums=1\n",
    "# subject_nums=12\n",
    "# activity_nums=len(activity_list)\n",
    "# repetition_nums=5\n",
    "\n",
    "# --- RNN Sequence Configuration ---\n",
    "num_steps = 180 #32  # Window width for RNN input (number of frames per sequence)\n",
    "test_train_split = 1.0  # Percentage of data for training\n",
    "split = False  # If True, a second stage of shuffling and splitting is performed.\n",
    "overlap = 0.8125  # Overlap when creating sequences. 0 = 0% overlap.\n",
    "\n",
    "# --- Ensure data_path exists ---\n",
    "if not os.path.isdir(data_path):\n",
    "    print(f\"Error: data_path '{data_path}' not found. Please check the path.\")\n",
    "    exit() # Exit if base data path doesn't exist\n",
    "\n",
    "# --- Clean up old train/test files if they exist ---\n",
    "# Output files are placed in the 'data_path' directory.\n",
    "files_to_remove = [test_file_X, test_file_Y, train_file_X, train_file_Y]\n",
    "for f_name in files_to_remove:\n",
    "    f_path = os.path.join(data_path, f_name)\n",
    "    if os.path.exists(f_path):\n",
    "        print(f\"Removing existing file: {f_path}\")\n",
    "        os.remove(f_path)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Store the original CWD to return to it if needed at the very end,\n",
    "# or to construct absolute paths if data_path is relative.\n",
    "initial_cwd = os.getcwd()\n",
    "abs_data_path = os.path.abspath(data_path) # Use absolute path for data_path internally\n",
    "\n",
    "# Process each activity\n",
    "for activity_idx, activity_name in enumerate(activity_list):\n",
    "    current_activity_path_relative_to_abs_data_path = activity_name # e.g., \"heel_hook\"\n",
    "    current_activity_full_path = os.path.join(abs_data_path, current_activity_path_relative_to_abs_data_path)\n",
    "    \n",
    "    print(f\"Processing activity: {activity_name} in {current_activity_full_path}\")\n",
    "\n",
    "    if not os.path.isdir(current_activity_full_path):\n",
    "        print(f\"Warning: Directory not found for activity {activity_name}, skipping: {current_activity_full_path}\")\n",
    "        continue\n",
    "\n",
    "    # Change directory into the activity's folder\n",
    "    try:\n",
    "        os.chdir(current_activity_full_path)\n",
    "        print(f\"  Changed CWD to: {os.getcwd()}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  Error: Could not change directory to {current_activity_full_path}, skipping activity.\")\n",
    "        # Change back to abs_data_path before continuing to next activity iteration\n",
    "        os.chdir(abs_data_path) # Go back to parent (e.g., MP_Data_JSON)\n",
    "        continue\n",
    "\n",
    "    # MODIFIED: Search for files ending with _mp33.txt within the current activity directory\n",
    "    for file_name_in_activity_dir in sorted(glob.glob(\"*_mp33.txt\")):\n",
    "        print(f\"    Processing file: {file_name_in_activity_dir}\")\n",
    "\n",
    "        is_train = np.random.rand() < test_train_split\n",
    "        print(f\"      Assigning to {'train' if is_train else 'test'} set.\")\n",
    "\n",
    "        # try:\n",
    "        #     with open(file_name_in_activity_dir, 'r') as data_file_handle: # file_name_in_activity_dir is relative to CWD\n",
    "        #         file_text = data_file_handle.readlines()\n",
    "        # except FileNotFoundError: # Should not happen if glob found it, but good practice\n",
    "        #     print(f\"      Error: File not found {file_name_in_activity_dir} within {os.getcwd()}, skipping.\")\n",
    "        #     continue\n",
    "\n",
    "        # --- ★★★ 線形補間のための新しいコードブロック ★★★ ---\n",
    "        try:\n",
    "            # pandasのread_csvを使って直接データを読み込む\n",
    "            # ヘッダーがないので header=None を指定\n",
    "            # この時点で、0.0 の値は欠損値(NaN)として扱われるようにする\n",
    "            data_df = pd.read_csv(file_name_in_activity_dir, header=None, na_values=0.0)\n",
    "\n",
    "            # 線形補間を実行\n",
    "            # DataFrame全体に対して一度に補間を適用できる\n",
    "            data_df.interpolate(method='linear', axis=0, limit_direction='both', inplace=True)\n",
    "\n",
    "            # 補間後も残っている可能性のあるNaNを0で埋める（例：動画の最初や最後が全て欠損の場合）\n",
    "            data_df.fillna(0.0, inplace=True)\n",
    "\n",
    "            # 後の処理で使えるように、DataFrameを元のテキスト形式（文字列のリスト）に変換し直す\n",
    "            # 各行をカンマ区切りの文字列に戻す\n",
    "            file_text = data_df.to_string(header=False, index=False).strip().split('\\n')\n",
    "            # to_string が余分なスペースを入れることがあるため、それを整形する\n",
    "            file_text = [line.strip().replace(' ', ',').replace(',,', ',0.0,') for line in file_text]\n",
    "            # 各行の末尾に改行文字を追加する（元のreadlines()の形式に合わせる）\n",
    "            file_text = [line + '\\\\n' for line in file_text]\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"      Error: File not found {file_name_in_activity_dir} within {os.getcwd()}, skipping.\")\n",
    "            continue\n",
    "        # --- ★★★ ここまでが新しいコードブロック ★★★ ---\n",
    "        \n",
    "        if not file_text:\n",
    "            print(f\"      Warning: File {file_name_in_activity_dir} is empty, skipping.\")\n",
    "            continue\n",
    "\n",
    "        num_frames = len(file_text)\n",
    "        if num_frames < num_steps:\n",
    "            print(f\"      Warning: File {file_name_in_activity_dir} has {num_frames} frames, less than num_steps ({num_steps}). Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        if 1 - overlap <= 1e-6: # Check for overlap close to 1 or 1\n",
    "            print(f\"      Error: Invalid overlap value ({overlap}) results in zero or negative step. Skipping file.\")\n",
    "            num_framesets = 0 \n",
    "        else:\n",
    "            num_framesets = int((num_frames - num_steps) / (num_steps * (1 - overlap))) + 1\n",
    "        \n",
    "        print(f\"      Total frames: {num_frames}, Num sequences possible: {num_framesets}\")\n",
    "\n",
    "        if num_framesets <= 0:\n",
    "            print(f\"      Not enough frames in {file_name_in_activity_dir} to create any sequences with current settings. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        if is_train:\n",
    "            output_file_X_basename = train_file_X\n",
    "            output_file_Y_basename = train_file_Y\n",
    "        else:\n",
    "            output_file_X_basename = test_file_X\n",
    "            output_file_Y_basename = test_file_Y\n",
    "\n",
    "        # Output files (X_train.txt etc.) are in the abs_data_path directory\n",
    "        x_output_full_path = os.path.join(abs_data_path, output_file_X_basename)\n",
    "        y_output_full_path = os.path.join(abs_data_path, output_file_Y_basename)\n",
    "\n",
    "        try:\n",
    "            with open(x_output_full_path, 'a') as x_file:\n",
    "                for frameset_idx in range(num_framesets):\n",
    "                    start_frame = int(frameset_idx * num_steps * (1 - overlap))\n",
    "                    end_frame = start_frame + num_steps\n",
    "                    \n",
    "                    # Ensure the slice is within bounds\n",
    "                    if start_frame < 0 or end_frame > num_frames or start_frame >= end_frame :\n",
    "                        print(f\"      Warning: Invalid frame slice [{start_frame}:{end_frame}] for num_frames {num_frames}. Skipping frameset_idx {frameset_idx}.\")\n",
    "                        continue\n",
    "\n",
    "                    for line_idx in range(start_frame, end_frame):\n",
    "                        x_file.write(file_text[line_idx])\n",
    "            print(f\"      Appended {num_framesets} sequences to {x_output_full_path}\")\n",
    "\n",
    "            with open(y_output_full_path, 'a') as y_file:\n",
    "                for _ in range(num_framesets):\n",
    "                    y_file.write(str(activity_idx + 1) + \"\\n\") # Y label is 1-based activity index\n",
    "            print(f\"      Appended {num_framesets} labels to {y_output_full_path}\")\n",
    "\n",
    "        except IOError as e:\n",
    "            print(f\"      Error writing to output files: {e}\")\n",
    "            # If there was an error writing, we should still try to chdir back\n",
    "    \n",
    "    # After processing all files in the current_activity_full_path,\n",
    "    # change CWD back to abs_data_path (e.g., MP_Data_JSON)\n",
    "    # so the next iteration of the activity loop correctly resolves paths.\n",
    "    try:\n",
    "        os.chdir(abs_data_path)\n",
    "        print(f\"  Returned CWD to: {os.getcwd()} (after processing {activity_name})\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error on os.chdir back to base data_path ({abs_data_path}) \" \\\n",
    "              f\"from within {activity_name} directory structure: {e}\")\n",
    "        print(f\"  Current CWD is: {os.getcwd()}. Subsequent paths might be incorrect.\")\n",
    "        # Potentially exit or handle more robustly if this critical chdir fails.\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# The second splitting block (if split:) from the original script\n",
    "# This block should now operate with CWD being abs_data_path (e.g., MP_Data_JSON)\n",
    "if split:\n",
    "    print(f\"Performing second stage split (if split=True) in CWD: {os.getcwd()}\")\n",
    "    \n",
    "    # This part's logic for choosing source_X/Y_to_resplit needs review if `split=True` is used.\n",
    "    # As per original, it seems to assume a single pair of X/Y files to resplit.\n",
    "    # If the first loop already created X_train/Y_train and X_test/Y_test, what should this do?\n",
    "    # For now, let's assume it tries to resplit the training data further, or a combined set if available.\n",
    "    # This is a placeholder, the exact files to read here depend on the desired workflow for `split=True`.\n",
    "    source_X_to_resplit = train_file_X # Example: re-split the training data\n",
    "    source_Y_to_resplit = train_file_Y # Example: re-split the training data\n",
    "\n",
    "    # Ensure full paths are used for reading, as CWD is now abs_data_path\n",
    "    path_X_to_resplit = os.path.join(abs_data_path, source_X_to_resplit)\n",
    "    path_Y_to_resplit = os.path.join(abs_data_path, source_Y_to_resplit)\n",
    "\n",
    "    X_data, Y_data = [], []\n",
    "    try:\n",
    "        if os.path.exists(path_X_to_resplit):\n",
    "            with open(path_X_to_resplit, 'r') as X_file_handle:\n",
    "                X_data = X_file_handle.readlines()\n",
    "        else:\n",
    "            print(f\"Error: Source file for X re-split not found: {path_X_to_resplit}\")\n",
    "\n",
    "        if os.path.exists(path_Y_to_resplit):\n",
    "            with open(path_Y_to_resplit, 'r') as Y_file_handle:\n",
    "                Y_data = Y_file_handle.readlines()\n",
    "        else:\n",
    "            print(f\"Error: Source file for Y re-split not found: {path_Y_to_resplit}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error reading files for second split: {e}. Skipping second split.\")\n",
    "\n",
    "    if X_data and Y_data:\n",
    "        print(f\"  Re-splitting {len(Y_data)} sequences from {source_X_to_resplit} and {source_Y_to_resplit}\")\n",
    "        # Remove old files again before re-writing test/train from this split\n",
    "        for f_name in files_to_remove:\n",
    "            f_path = os.path.join(abs_data_path, f_name)\n",
    "            if os.path.exists(f_path):\n",
    "                os.remove(f_path)\n",
    "\n",
    "        # Y_data contains one label per sequence created in the first loop\n",
    "        # msk should be the same length as the number of sequences in Y_data\n",
    "        num_sequences_to_resplit = len(Y_data)\n",
    "        msk = np.random.rand(num_sequences_to_resplit) < test_train_split\n",
    "        \n",
    "        current_X_sequence_lines = []\n",
    "        y_data_idx = 0 # To iterate through Y_data and msk\n",
    "\n",
    "        for i, x_line in enumerate(X_data):\n",
    "            current_X_sequence_lines.append(x_line)\n",
    "            if (i + 1) % num_steps == 0: # A full sequence for X has been collected\n",
    "                if y_data_idx < num_sequences_to_resplit:\n",
    "                    is_train_for_sequence = msk[y_data_idx]\n",
    "                    target_X_basename = train_file_X if is_train_for_sequence else test_file_X\n",
    "                    target_Y_basename = train_file_Y if is_train_for_sequence else test_file_Y\n",
    "\n",
    "                    x_out_f_path = os.path.join(abs_data_path, target_X_basename)\n",
    "                    y_out_f_path = os.path.join(abs_data_path, target_Y_basename)\n",
    "\n",
    "                    with open(x_out_f_path, 'a') as x_out_f:\n",
    "                        for line_in_seq in current_X_sequence_lines:\n",
    "                            x_out_f.write(line_in_seq)\n",
    "                    \n",
    "                    with open(y_out_f_path, 'a') as y_out_f:\n",
    "                        y_out_f.write(Y_data[y_data_idx]) # Y_data already has newlines\n",
    "                    \n",
    "                    y_data_idx += 1\n",
    "                current_X_sequence_lines = [] # Reset for next sequence\n",
    "        \n",
    "        print(\"  Second stage split completed.\")\n",
    "        # print(msk)\n",
    "    elif split: # Only print if split was true and data wasn't found\n",
    "        print(\"  Skipping second stage split due to missing data.\")\n",
    "\n",
    "\n",
    "# Optional: change back to the directory where the script was initially launched\n",
    "# os.chdir(initial_cwd)\n",
    "print(\"Database creation process finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
