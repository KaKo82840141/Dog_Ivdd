{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2840733c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.19.0\n",
      "[INFO] 使用キーポイント: ['left back paw', 'right back paw', 'tail set']\n",
      "X: (1460, 60, 6) y: (1460,) files: 198\n",
      "train: (1166, 60, 6) val: (294, 60, 6)\n",
      "class_weight: {0: 1.1, 1: 0.9166666666666666}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"ivdd_lstm_3kp\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"ivdd_lstm_3kp\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">210</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m6\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m30\u001b[0m)         │           \u001b[38;5;34m210\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m30\u001b[0m)         │         \u001b[38;5;34m7,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)             │         \u001b[38;5;34m7,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m31\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,881</span> (58.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,881\u001b[0m (58.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,881</span> (58.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,881\u001b[0m (58.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "\u001b[1m34/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4630 - loss: 0.7060\n",
      "Epoch 1: val_accuracy improved from -inf to 0.50000, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train2_model\\ivdd_lstm_3kp_20251211-124311_best.keras\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.4616 - loss: 0.7057 - val_accuracy: 0.5000 - val_loss: 0.7032 - learning_rate: 1.0000e-04\n",
      "Epoch 2/60\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4618 - loss: 0.7040\n",
      "Epoch 2: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4616 - loss: 0.7040 - val_accuracy: 0.5000 - val_loss: 0.7024 - learning_rate: 1.0000e-04\n",
      "Epoch 3/60\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4622 - loss: 0.7027\n",
      "Epoch 3: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4616 - loss: 0.7026 - val_accuracy: 0.5000 - val_loss: 0.7016 - learning_rate: 1.0000e-04\n",
      "Epoch 4/60\n",
      "\u001b[1m33/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4633 - loss: 0.7013\n",
      "Epoch 4: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4616 - loss: 0.7011 - val_accuracy: 0.5000 - val_loss: 0.7009 - learning_rate: 1.0000e-04\n",
      "Epoch 5/60\n",
      "\u001b[1m36/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4625 - loss: 0.6996\n",
      "Epoch 5: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4616 - loss: 0.6995 - val_accuracy: 0.5000 - val_loss: 0.7002 - learning_rate: 1.0000e-04\n",
      "Epoch 6/60\n",
      "\u001b[1m36/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4625 - loss: 0.6976\n",
      "Epoch 6: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4616 - loss: 0.6975 - val_accuracy: 0.5000 - val_loss: 0.6996 - learning_rate: 1.0000e-04\n",
      "Epoch 7/60\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4618 - loss: 0.6952\n",
      "Epoch 7: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4616 - loss: 0.6952 - val_accuracy: 0.5000 - val_loss: 0.6993 - learning_rate: 1.0000e-04\n",
      "Epoch 8/60\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4615 - loss: 0.6926\n",
      "Epoch 8: val_accuracy improved from 0.50000 to 0.50340, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train2_model\\ivdd_lstm_3kp_20251211-124311_best.keras\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4613 - loss: 0.6926 - val_accuracy: 0.5034 - val_loss: 0.6995 - learning_rate: 1.0000e-04\n",
      "Epoch 9/60\n",
      "\u001b[1m35/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4755 - loss: 0.6902\n",
      "Epoch 9: val_accuracy improved from 0.50340 to 0.52381, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train2_model\\ivdd_lstm_3kp_20251211-124311_best.keras\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4735 - loss: 0.6902 - val_accuracy: 0.5238 - val_loss: 0.7001 - learning_rate: 1.0000e-04\n",
      "Epoch 10/60\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4875 - loss: 0.6882\n",
      "Epoch 10: val_accuracy did not improve from 0.52381\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4872 - loss: 0.6882 - val_accuracy: 0.5238 - val_loss: 0.7008 - learning_rate: 1.0000e-04\n",
      "Epoch 11/60\n",
      "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5026 - loss: 0.6865\n",
      "Epoch 11: val_accuracy improved from 0.52381 to 0.52721, saving model to C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train2_model\\ivdd_lstm_3kp_20251211-124311_best.keras\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5019 - loss: 0.6866 - val_accuracy: 0.5272 - val_loss: 0.7013 - learning_rate: 1.0000e-04\n",
      "Epoch 12/60\n",
      "\u001b[1m35/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5127 - loss: 0.6849\n",
      "Epoch 12: val_accuracy did not improve from 0.52721\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5098 - loss: 0.6851 - val_accuracy: 0.5238 - val_loss: 0.7019 - learning_rate: 1.0000e-04\n",
      "Epoch 13/60\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5211 - loss: 0.6826\n",
      "Epoch 13: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5206 - loss: 0.6826 - val_accuracy: 0.5136 - val_loss: 0.7024 - learning_rate: 5.0000e-05\n",
      "Epoch 14/60\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5270 - loss: 0.6815\n",
      "Epoch 14: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5252 - loss: 0.6816 - val_accuracy: 0.5170 - val_loss: 0.7032 - learning_rate: 5.0000e-05\n",
      "Epoch 15/60\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5240 - loss: 0.6807\n",
      "Epoch 15: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5235 - loss: 0.6807 - val_accuracy: 0.5170 - val_loss: 0.7039 - learning_rate: 5.0000e-05\n",
      "Epoch 16/60\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5322 - loss: 0.6798\n",
      "Epoch 16: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5304 - loss: 0.6799 - val_accuracy: 0.5102 - val_loss: 0.7047 - learning_rate: 5.0000e-05\n",
      "Epoch 17/60\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5359 - loss: 0.6791\n",
      "Epoch 17: val_accuracy did not improve from 0.52721\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5344 - loss: 0.6791 - val_accuracy: 0.5102 - val_loss: 0.7055 - learning_rate: 5.0000e-05\n",
      "Epoch 18/60\n",
      "\u001b[1m34/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5370 - loss: 0.6777\n",
      "Epoch 18: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5334 - loss: 0.6779 - val_accuracy: 0.5102 - val_loss: 0.7059 - learning_rate: 2.5000e-05\n",
      "Epoch 19/60\n",
      "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5326 - loss: 0.6772\n",
      "Epoch 19: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5317 - loss: 0.6773 - val_accuracy: 0.5068 - val_loss: 0.7064 - learning_rate: 2.5000e-05\n",
      "Epoch 20/60\n",
      "\u001b[1m34/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5359 - loss: 0.6767\n",
      "Epoch 20: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5326 - loss: 0.6768 - val_accuracy: 0.5068 - val_loss: 0.7069 - learning_rate: 2.5000e-05\n",
      "Epoch 21/60\n",
      "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5344 - loss: 0.6764\n",
      "Epoch 21: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5336 - loss: 0.6764 - val_accuracy: 0.5034 - val_loss: 0.7074 - learning_rate: 2.5000e-05\n",
      "Epoch 22/60\n",
      "\u001b[1m35/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5363 - loss: 0.6759\n",
      "Epoch 22: val_accuracy did not improve from 0.52721\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5342 - loss: 0.6760 - val_accuracy: 0.5034 - val_loss: 0.7078 - learning_rate: 2.5000e-05\n",
      "Epoch 23/60\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5404 - loss: 0.6754\n",
      "Epoch 23: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5389 - loss: 0.6754 - val_accuracy: 0.5034 - val_loss: 0.7081 - learning_rate: 1.2500e-05\n",
      "Epoch 24/60\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5394 - loss: 0.6752\n",
      "Epoch 24: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5389 - loss: 0.6752 - val_accuracy: 0.5034 - val_loss: 0.7084 - learning_rate: 1.2500e-05\n",
      "Epoch 25/60\n",
      "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5396 - loss: 0.6749\n",
      "Epoch 25: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5386 - loss: 0.6749 - val_accuracy: 0.5034 - val_loss: 0.7086 - learning_rate: 1.2500e-05\n",
      "Epoch 26/60\n",
      "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5393 - loss: 0.6747\n",
      "Epoch 26: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5383 - loss: 0.6747 - val_accuracy: 0.5034 - val_loss: 0.7089 - learning_rate: 1.2500e-05\n",
      "Epoch 27/60\n",
      "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5412 - loss: 0.6745\n",
      "Epoch 27: val_accuracy did not improve from 0.52721\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5402 - loss: 0.6745 - val_accuracy: 0.5034 - val_loss: 0.7091 - learning_rate: 1.2500e-05\n",
      "Epoch 28/60\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5436 - loss: 0.6742\n",
      "Epoch 28: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5419 - loss: 0.6743 - val_accuracy: 0.5034 - val_loss: 0.7093 - learning_rate: 1.0000e-05\n",
      "Epoch 29/60\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5436 - loss: 0.6741\n",
      "Epoch 29: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5419 - loss: 0.6741 - val_accuracy: 0.5034 - val_loss: 0.7095 - learning_rate: 1.0000e-05\n",
      "Epoch 30/60\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5454 - loss: 0.6739\n",
      "Epoch 30: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5438 - loss: 0.6740 - val_accuracy: 0.5034 - val_loss: 0.7097 - learning_rate: 1.0000e-05\n",
      "Epoch 31/60\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5451 - loss: 0.6738\n",
      "Epoch 31: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5446 - loss: 0.6738 - val_accuracy: 0.5034 - val_loss: 0.7099 - learning_rate: 1.0000e-05\n",
      "Epoch 32/60\n",
      "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5457 - loss: 0.6736\n",
      "Epoch 32: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5446 - loss: 0.6737 - val_accuracy: 0.5068 - val_loss: 0.7101 - learning_rate: 1.0000e-05\n",
      "Epoch 33/60\n",
      "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5459 - loss: 0.6735\n",
      "Epoch 33: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5449 - loss: 0.6735 - val_accuracy: 0.5034 - val_loss: 0.7103 - learning_rate: 1.0000e-05\n",
      "Epoch 34/60\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5476 - loss: 0.6733\n",
      "Epoch 34: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5461 - loss: 0.6734 - val_accuracy: 0.5034 - val_loss: 0.7105 - learning_rate: 1.0000e-05\n",
      "Epoch 35/60\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5480 - loss: 0.6732\n",
      "Epoch 35: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5465 - loss: 0.6732 - val_accuracy: 0.5034 - val_loss: 0.7107 - learning_rate: 1.0000e-05\n",
      "Epoch 36/60\n",
      "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5489 - loss: 0.6731\n",
      "Epoch 36: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5480 - loss: 0.6731 - val_accuracy: 0.5000 - val_loss: 0.7108 - learning_rate: 1.0000e-05\n",
      "Epoch 37/60\n",
      "\u001b[1m35/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5507 - loss: 0.6728\n",
      "Epoch 37: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5478 - loss: 0.6730 - val_accuracy: 0.5000 - val_loss: 0.7110 - learning_rate: 1.0000e-05\n",
      "Epoch 38/60\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5472 - loss: 0.6728\n",
      "Epoch 38: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5467 - loss: 0.6728 - val_accuracy: 0.5000 - val_loss: 0.7112 - learning_rate: 1.0000e-05\n",
      "Epoch 39/60\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5484 - loss: 0.6726\n",
      "Epoch 39: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5479 - loss: 0.6727 - val_accuracy: 0.5000 - val_loss: 0.7114 - learning_rate: 1.0000e-05\n",
      "Epoch 40/60\n",
      "\u001b[1m35/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5515 - loss: 0.6724\n",
      "Epoch 40: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5487 - loss: 0.6725 - val_accuracy: 0.5000 - val_loss: 0.7116 - learning_rate: 1.0000e-05\n",
      "Epoch 41/60\n",
      "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5502 - loss: 0.6723\n",
      "Epoch 41: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5492 - loss: 0.6724 - val_accuracy: 0.5000 - val_loss: 0.7118 - learning_rate: 1.0000e-05\n",
      "Epoch 42/60\n",
      "\u001b[1m33/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5547 - loss: 0.6721\n",
      "Epoch 42: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5506 - loss: 0.6722 - val_accuracy: 0.5000 - val_loss: 0.7120 - learning_rate: 1.0000e-05\n",
      "Epoch 43/60\n",
      "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5517 - loss: 0.6720\n",
      "Epoch 43: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5508 - loss: 0.6721 - val_accuracy: 0.5000 - val_loss: 0.7122 - learning_rate: 1.0000e-05\n",
      "Epoch 44/60\n",
      "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5529 - loss: 0.6719\n",
      "Epoch 44: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5519 - loss: 0.6720 - val_accuracy: 0.5000 - val_loss: 0.7124 - learning_rate: 1.0000e-05\n",
      "Epoch 45/60\n",
      "\u001b[1m35/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5547 - loss: 0.6717\n",
      "Epoch 45: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5517 - loss: 0.6718 - val_accuracy: 0.5000 - val_loss: 0.7126 - learning_rate: 1.0000e-05\n",
      "Epoch 46/60\n",
      "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5528 - loss: 0.6716\n",
      "Epoch 46: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5517 - loss: 0.6717 - val_accuracy: 0.5000 - val_loss: 0.7128 - learning_rate: 1.0000e-05\n",
      "Epoch 47/60\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5535 - loss: 0.6714\n",
      "Epoch 47: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5519 - loss: 0.6715 - val_accuracy: 0.5034 - val_loss: 0.7130 - learning_rate: 1.0000e-05\n",
      "Epoch 48/60\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5522 - loss: 0.6714\n",
      "Epoch 48: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5517 - loss: 0.6714 - val_accuracy: 0.5000 - val_loss: 0.7132 - learning_rate: 1.0000e-05\n",
      "Epoch 49/60\n",
      "\u001b[1m36/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5539 - loss: 0.6712\n",
      "Epoch 49: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5517 - loss: 0.6712 - val_accuracy: 0.5000 - val_loss: 0.7134 - learning_rate: 1.0000e-05\n",
      "Epoch 50/60\n",
      "\u001b[1m36/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5584 - loss: 0.6710\n",
      "Epoch 50: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5559 - loss: 0.6711 - val_accuracy: 0.5000 - val_loss: 0.7136 - learning_rate: 1.0000e-05\n",
      "Epoch 51/60\n",
      "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5582 - loss: 0.6709\n",
      "Epoch 51: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5572 - loss: 0.6709 - val_accuracy: 0.5000 - val_loss: 0.7138 - learning_rate: 1.0000e-05\n",
      "Epoch 52/60\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5582 - loss: 0.6708\n",
      "Epoch 52: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5576 - loss: 0.6708 - val_accuracy: 0.5000 - val_loss: 0.7140 - learning_rate: 1.0000e-05\n",
      "Epoch 53/60\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5593 - loss: 0.6706\n",
      "Epoch 53: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5576 - loss: 0.6707 - val_accuracy: 0.5000 - val_loss: 0.7142 - learning_rate: 1.0000e-05\n",
      "Epoch 54/60\n",
      "\u001b[1m36/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5623 - loss: 0.6704\n",
      "Epoch 54: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5600 - loss: 0.6705 - val_accuracy: 0.5000 - val_loss: 0.7144 - learning_rate: 1.0000e-05\n",
      "Epoch 55/60\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5616 - loss: 0.6703\n",
      "Epoch 55: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5600 - loss: 0.6704 - val_accuracy: 0.5000 - val_loss: 0.7146 - learning_rate: 1.0000e-05\n",
      "Epoch 56/60\n",
      "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5611 - loss: 0.6702\n",
      "Epoch 56: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5600 - loss: 0.6702 - val_accuracy: 0.5000 - val_loss: 0.7148 - learning_rate: 1.0000e-05\n",
      "Epoch 57/60\n",
      "\u001b[1m34/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5614 - loss: 0.6699\n",
      "Epoch 57: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5579 - loss: 0.6701 - val_accuracy: 0.4966 - val_loss: 0.7150 - learning_rate: 1.0000e-05\n",
      "Epoch 58/60\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5608 - loss: 0.6699\n",
      "Epoch 58: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5592 - loss: 0.6699 - val_accuracy: 0.4966 - val_loss: 0.7152 - learning_rate: 1.0000e-05\n",
      "Epoch 59/60\n",
      "\u001b[1m35/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5625 - loss: 0.6696\n",
      "Epoch 59: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5597 - loss: 0.6698 - val_accuracy: 0.4966 - val_loss: 0.7155 - learning_rate: 1.0000e-05\n",
      "Epoch 60/60\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5616 - loss: 0.6696\n",
      "Epoch 60: val_accuracy did not improve from 0.52721\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5601 - loss: 0.6696 - val_accuracy: 0.4966 - val_loss: 0.7157 - learning_rate: 1.0000e-05\n",
      "[INFO] saved curve: C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\fig\\curve_20251211-124311.png\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step\n",
      "\n",
      "[Window-level] classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal     0.5118    0.7347    0.6034       147\n",
      "        ivdd     0.5301    0.2993    0.3826       147\n",
      "\n",
      "    accuracy                         0.5170       294\n",
      "   macro avg     0.5210    0.5170    0.4930       294\n",
      "weighted avg     0.5210    0.5170    0.4930       294\n",
      "\n",
      "[Window-level] confusion matrix:\n",
      " [[108  39]\n",
      " [103  44]]\n",
      "[INFO] saved misclassified csv: C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\val_misclassified\\val_misclassified_20251211-124311.csv\n",
      "[INFO] saved model: C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train2_model\\ivdd_lstm_3kp_20251211-124311_final.keras\n",
      "[DONE] 3KP training complete: tail_minmax\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Train (3 keypoints: left back paw, right back paw, tail set)\n",
    "- NORMALIZE_MODE = \"zscore\" (train1相当) or \"tail_minmax\" (train2相当)\n",
    "- outputs:\n",
    "  - data/train/fig/curve_<TS>.png\n",
    "  - data/train/val_misclassified/val_misclassified_<TS>.csv\n",
    "  - data/train/train1_model or train2_model / ivdd_lstm_3kp_<TS>_best.keras / _final.keras\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"-1\")\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "\n",
    "import re, glob\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ====== パス設定（あなたのリポジトリ最上位に合わせて修正可）======\n",
    "REPO_ROOT = r\"C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\"\n",
    "TRAIN_ROOT = os.path.join(REPO_ROOT, \"data\", \"train\")\n",
    "TEST_ROOT  = os.path.join(REPO_ROOT, \"data\", \"test\")\n",
    "\n",
    "TRAIN_CSV_DIR = os.path.join(TRAIN_ROOT, \"train_csv\")\n",
    "TRAIN_GLOB    = os.path.join(TRAIN_CSV_DIR, \"*.csv\")\n",
    "FIG_DIR       = os.path.join(TRAIN_ROOT, \"fig\")\n",
    "VALERR_DIR    = os.path.join(TRAIN_ROOT, \"val_misclassified\")\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "os.makedirs(VALERR_DIR, exist_ok=True)\n",
    "\n",
    "# ====== 実行設定 ======\n",
    "RUN_ID = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# 正規化選択: \"zscore\" (train1相当) / \"tail_minmax\" (train2相当)\n",
    "NORMALIZE_MODE = \"tail_minmax\"   # ←必要に応じて \"zscore\" に変更\n",
    "\n",
    "# モデル保存先はモードに応じて\n",
    "MODEL_DIR = os.path.join(TRAIN_ROOT, \"train1_model\" if NORMALIZE_MODE==\"zscore\" else \"train2_model\")\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# ====== データ設定 ======\n",
    "KEYPOINTS = [\"left back paw\", \"right back paw\", \"tail set\"]\n",
    "USE_LIKELIHOOD      = False\n",
    "MIN_KEEP_LIKELIHOOD = 0.6\n",
    "SEQ_LEN  = 60\n",
    "STRIDE   = 30\n",
    "DIMS     = 6  # 3keypoints × (x,y)\n",
    "\n",
    "# ====== 学習ハイパラ ======\n",
    "N_HIDDEN   = 30\n",
    "BATCH_SIZE = 30\n",
    "EPOCHS     = 60\n",
    "LR         = 1e-4\n",
    "L2_LAMBDA  = 1e-4\n",
    "\n",
    "VAL_SPLIT_BY_FILE = True\n",
    "\n",
    "# ラベル: 0=normal, 1=ivdd\n",
    "CLASS_NAMES  = [\"normal\", \"ivdd\"]\n",
    "NAME2IDX     = {\"normal\":0, \"ivdd\":1}\n",
    "\n",
    "# ====== ユーティリティ ======\n",
    "def _norm_name(s: str) -> str:\n",
    "    return \"\".join(ch for ch in s.lower() if ch not in \" _-\")\n",
    "\n",
    "def _resolve_keypoints(all_bodyparts, requested):\n",
    "    norm2orig = {}\n",
    "    for bp in all_bodyparts:\n",
    "        k = _norm_name(bp)\n",
    "        if k not in norm2orig:\n",
    "            norm2orig[k] = bp\n",
    "    resolved, missing = [], []\n",
    "    for req in requested:\n",
    "        k = _norm_name(req)\n",
    "        if k in norm2orig:\n",
    "            resolved.append(norm2orig[k])\n",
    "        else:\n",
    "            missing.append(req)\n",
    "    if missing:\n",
    "        raise ValueError(f\"指定KPが見つかりません: {missing}\\n利用可能: {all_bodyparts}\")\n",
    "    return resolved\n",
    "\n",
    "def infer_label_from_filename(path: str) -> int:\n",
    "    \"\"\"\n",
    "    'ivdd', 'ivdd1','ivdd2',... を ivdd と判定。'normal' は normal。\n",
    "    ファイル名→未決なら先頭トークン→親ディレクトリの順で決める。\n",
    "    \"\"\"\n",
    "    name = os.path.basename(path).lower()\n",
    "    stem = os.path.splitext(name)[0]\n",
    "    tokens = [t for t in re.split(r'[^a-z0-9]+', stem) if t]\n",
    "    token_set = set(tokens)\n",
    "\n",
    "    has_ivdd = any(t == \"ivdd\" or t.startswith(\"ivdd\") for t in tokens)\n",
    "    has_normal = \"normal\" in token_set\n",
    "\n",
    "    if has_ivdd and not has_normal:\n",
    "        return NAME2IDX[\"ivdd\"]\n",
    "    if has_normal and not has_ivdd:\n",
    "        return NAME2IDX[\"normal\"]\n",
    "\n",
    "    if tokens and tokens[0] in NAME2IDX:\n",
    "        return NAME2IDX[tokens[0]]\n",
    "\n",
    "    parent_tokens = [t for t in re.split(r'[^a-z0-9]+', os.path.dirname(path).lower()) if t]\n",
    "    p_has_ivdd   = any(t == \"ivdd\" or t.startswith(\"ivdd\") for t in parent_tokens)\n",
    "    p_has_normal = \"normal\" in set(parent_tokens)\n",
    "    if p_has_ivdd and not p_has_normal:\n",
    "        return NAME2IDX[\"ivdd\"]\n",
    "    if p_has_normal and not p_has_ivdd:\n",
    "        return NAME2IDX[\"normal\"]\n",
    "\n",
    "    raise ValueError(f\"ラベル不明: {name}（'ivdd' か 'normal' を含めてください）\")\n",
    "\n",
    "def read_dlc_3kp_xy(csv_path: str, keypoints, use_likelihood=True, min_keep_likelihood=0.6):\n",
    "    df = pd.read_csv(csv_path, header=[0,1,2], index_col=0)\n",
    "    bodyparts = list({bp for (_, bp, _) in df.columns})\n",
    "    use_kps = _resolve_keypoints(bodyparts, keypoints)\n",
    "\n",
    "    cols = {}\n",
    "    for bp in use_kps:\n",
    "        cols[f\"{bp}_x\"] = df.xs((bp, \"x\"), level=[1,2], axis=1)\n",
    "        cols[f\"{bp}_y\"] = df.xs((bp, \"y\"), level=[1,2], axis=1)\n",
    "    X_df = pd.concat(cols.values(), axis=1)\n",
    "    X_df.columns = list(cols.keys())\n",
    "\n",
    "    if use_likelihood:\n",
    "        for bp in use_kps:\n",
    "            try:\n",
    "                lcol = df.xs((bp, \"likelihood\"), level=[1,2], axis=1).values.flatten()\n",
    "                low = lcol < min_keep_likelihood\n",
    "                for c in [f\"{bp}_x\", f\"{bp}_y\"]:\n",
    "                    v = X_df[c].values\n",
    "                    v[low] = np.nan\n",
    "                    X_df[c] = v\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "    X_df = X_df.interpolate(method=\"linear\", limit_direction=\"both\", axis=0)\n",
    "    X_df = X_df.bfill().ffill().fillna(0.0)\n",
    "    return X_df.values.astype(np.float32), use_kps  # (T,6)\n",
    "\n",
    "def zscore_per_file(X: np.ndarray, eps=1e-6) -> np.ndarray:\n",
    "    mu = X.mean(axis=0, keepdims=True)\n",
    "    sd = X.std(axis=0, keepdims=True)\n",
    "    return (X - mu) / (sd + eps)\n",
    "\n",
    "def normalize_tailset_minmax(X: np.ndarray, used_kps: list[str], ref_name=\"tail set\", eps=1e-6) -> np.ndarray:\n",
    "    low = [s.lower() for s in used_kps]\n",
    "    if ref_name.lower() not in low:\n",
    "        raise ValueError(f\"'{ref_name}' が used_kps にありません: {used_kps}\")\n",
    "    r = low.index(ref_name.lower())\n",
    "\n",
    "    Xc = X.copy()\n",
    "    cx, cy = X[:, 2*r], X[:, 2*r+1]\n",
    "    for i in range(len(used_kps)):\n",
    "        Xc[:, 2*i]   -= cx\n",
    "        Xc[:, 2*i+1] -= cy\n",
    "\n",
    "    mn = Xc.min(axis=0, keepdims=True)\n",
    "    mx = Xc.max(axis=0, keepdims=True)\n",
    "    return (Xc - mn) / (mx - mn + eps)\n",
    "\n",
    "def make_windows(X: np.ndarray, seq_len: int, stride: int):\n",
    "    n = X.shape[0]\n",
    "    if n < seq_len:\n",
    "        return np.empty((0, seq_len, X.shape[1]), dtype=X.dtype), []\n",
    "    starts = list(range(0, n - seq_len + 1, stride))\n",
    "    Xw = np.stack([X[s:s+seq_len] for s in starts], axis=0)\n",
    "    return Xw, starts\n",
    "\n",
    "def build_dataset(csv_paths, seq_len=SEQ_LEN, stride=STRIDE):\n",
    "    Xs, ys, fids, starts_all = [], [], [], []\n",
    "    used_kps_any = None\n",
    "    for p in csv_paths:\n",
    "        y = infer_label_from_filename(p)\n",
    "        X_raw, used_kps = read_dlc_3kp_xy(p, KEYPOINTS, USE_LIKELIHOOD, MIN_KEEP_LIKELIHOOD)\n",
    "        if used_kps_any is None:\n",
    "            used_kps_any = used_kps\n",
    "        if X_raw.shape[1] != DIMS:\n",
    "            raise ValueError(f\"{os.path.basename(p)}: 次元{X_raw.shape[1]} != 期待{DIMS}\")\n",
    "\n",
    "        if NORMALIZE_MODE == \"zscore\":\n",
    "            Xn = zscore_per_file(X_raw)\n",
    "        elif NORMALIZE_MODE == \"tail_minmax\":\n",
    "            Xn = normalize_tailset_minmax(X_raw, used_kps)\n",
    "        else:\n",
    "            raise ValueError(\"NORMALIZE_MODE は 'zscore' or 'tail_minmax'\")\n",
    "\n",
    "        Xw, sidx = make_windows(Xn, seq_len, stride)\n",
    "        if Xw.shape[0] == 0:\n",
    "            print(f\"[WARN] {os.path.basename(p)}: フレーム不足でスキップ\")\n",
    "            continue\n",
    "\n",
    "        Xs.append(Xw)\n",
    "        ys.append(np.full((Xw.shape[0],), y, dtype=np.int64))\n",
    "        fids.extend([os.path.basename(p)]*Xw.shape[0])\n",
    "        starts_all.extend(sidx)\n",
    "\n",
    "    if not Xs:\n",
    "        raise RuntimeError(\"データが作れませんでした。CSVと命名（ivdd*/normal*）を確認してください。\")\n",
    "    X = np.concatenate(Xs, axis=0)\n",
    "    y = np.concatenate(ys, axis=0)\n",
    "    fids = np.array(fids)\n",
    "    starts_all = np.array(starts_all)\n",
    "    print(f\"[INFO] 使用キーポイント: {used_kps_any}\")\n",
    "    return X, y, fids, starts_all\n",
    "\n",
    "def build_model(seq_len: int, dims: int, n_hidden: int, l2_lambda: float=1e-4) -> keras.Model:\n",
    "    reg = keras.regularizers.l2(l2_lambda)\n",
    "    inp = keras.Input(shape=(seq_len, dims))\n",
    "    x   = keras.layers.TimeDistributed(keras.layers.Dense(n_hidden, activation=\"relu\", kernel_regularizer=reg))(inp)\n",
    "    x   = keras.layers.LSTM(n_hidden, return_sequences=True, kernel_regularizer=reg)(x)\n",
    "    x   = keras.layers.LSTM(n_hidden, kernel_regularizer=reg)(x)\n",
    "    out = keras.layers.Dense(1, kernel_regularizer=reg)(x)  # logits\n",
    "    return keras.Model(inp, out, name=\"ivdd_lstm_3kp\")\n",
    "\n",
    "# ====== データ読み込み ======\n",
    "csv_files = sorted(glob.glob(TRAIN_GLOB))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"学習CSVが見つかりません: {TRAIN_GLOB}\")\n",
    "\n",
    "X, y, file_ids, starts = build_dataset(csv_files, SEQ_LEN, STRIDE)\n",
    "print(\"X:\", X.shape, \"y:\", y.shape, \"files:\", len(np.unique(file_ids)))\n",
    "\n",
    "# ====== 分割 ======\n",
    "if VAL_SPLIT_BY_FILE:\n",
    "    uniq = np.unique(file_ids)\n",
    "    tr_files, va_files = train_test_split(uniq, test_size=0.2, random_state=42, shuffle=True)\n",
    "    tr_mask = np.isin(file_ids, tr_files)\n",
    "    va_mask = np.isin(file_ids, va_files)\n",
    "    X_train, y_train, starts_tr = X[tr_mask], y[tr_mask], starts[tr_mask]\n",
    "    X_val,   y_val,   starts_va = X[va_mask], y[va_mask], starts[va_mask]\n",
    "    file_ids_tr, file_ids_va = file_ids[tr_mask], file_ids[va_mask]\n",
    "else:\n",
    "    X_train, X_val, y_train, y_val, starts_tr, starts_va, file_ids_tr, file_ids_va = train_test_split(\n",
    "        X, y, starts, file_ids, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "print(\"train:\", X_train.shape, \"val:\", X_val.shape)\n",
    "\n",
    "# ====== class_weight ======\n",
    "if set(np.unique(y_train)) == {0,1}:\n",
    "    cw = compute_class_weight(\"balanced\", classes=np.array([0,1]), y=y_train)\n",
    "    class_weight = {0: float(cw[0]), 1: float(cw[1])}\n",
    "else:\n",
    "    class_weight = None\n",
    "print(\"class_weight:\", class_weight)\n",
    "\n",
    "# ====== モデル ======\n",
    "model = build_model(SEQ_LEN, DIMS, N_HIDDEN, L2_LAMBDA)\n",
    "opt = keras.optimizers.Adam(learning_rate=LR)\n",
    "model.compile(optimizer=opt, loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\")])\n",
    "model.summary()\n",
    "\n",
    "best_path  = os.path.join(MODEL_DIR, f\"ivdd_lstm_3kp_{RUN_ID}_best.keras\")\n",
    "final_path = os.path.join(MODEL_DIR, f\"ivdd_lstm_3kp_{RUN_ID}_final.keras\")\n",
    "\n",
    "cbs = [\n",
    "    keras.callbacks.ModelCheckpoint(best_path, monitor=\"val_accuracy\", mode=\"max\",\n",
    "                                    save_best_only=True, verbose=1),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", mode=\"min\",\n",
    "                                      factor=0.5, patience=5, min_lr=1e-5, verbose=1),\n",
    "]\n",
    "\n",
    "hist = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "    class_weight=class_weight, callbacks=cbs, verbose=1\n",
    ")\n",
    "\n",
    "# ====== 学習曲線（1枚） ======\n",
    "curve_png = os.path.join(FIG_DIR, f\"curve_{RUN_ID}.png\")\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1); plt.plot(hist.history[\"loss\"]); plt.plot(hist.history[\"val_loss\"]); plt.title(\"Loss\"); plt.legend([\"train\",\"val\"])\n",
    "plt.subplot(1,2,2); plt.plot(hist.history[\"accuracy\"]); plt.plot(hist.history[\"val_accuracy\"]); plt.title(\"Accuracy\"); plt.legend([\"train\",\"val\"])\n",
    "plt.tight_layout(); plt.savefig(curve_png, dpi=150); plt.close()\n",
    "print(\"[INFO] saved curve:\", curve_png)\n",
    "\n",
    "# ====== 検証(ウィンドウ)＆誤分類CSV ======\n",
    "logits_val = model.predict(X_val, batch_size=64)\n",
    "p_ivdd = tf.math.sigmoid(logits_val).numpy().ravel()\n",
    "y_pred = (p_ivdd >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n[Window-level] classification_report:\")\n",
    "print(classification_report(y_val, y_pred, target_names=CLASS_NAMES, digits=4))\n",
    "print(\"[Window-level] confusion matrix:\\n\", confusion_matrix(y_val, y_pred))\n",
    "\n",
    "# 誤分類ウィンドウの保存\n",
    "miss_mask = (y_val != y_pred)\n",
    "df_miss = pd.DataFrame({\n",
    "    \"file\": file_ids_va[miss_mask],\n",
    "    \"start\": starts_va[miss_mask],\n",
    "    \"true\": [CLASS_NAMES[t] for t in y_val[miss_mask]],\n",
    "    \"pred\": [CLASS_NAMES[p] for p in y_pred[miss_mask]],\n",
    "    \"p_ivdd\": p_ivdd[miss_mask],\n",
    "    \"p_normal\": 1.0 - p_ivdd[miss_mask],\n",
    "})\n",
    "miss_csv = os.path.join(VALERR_DIR, f\"val_misclassified_{RUN_ID}.csv\")\n",
    "df_miss.to_csv(miss_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[INFO] saved misclassified csv:\", miss_csv)\n",
    "\n",
    "# ====== 最終保存 ======\n",
    "model.save(final_path)\n",
    "print(\"[INFO] saved model:\", final_path)\n",
    "print(\"[DONE] 3KP training complete:\", NORMALIZE_MODE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLya",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
