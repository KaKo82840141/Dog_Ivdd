{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bdc0d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF: 2.19.0\n",
      "[INFO] 使用キーポイント実名: ['left back paw', 'right back paw', 'left front paw', 'right front paw', 'tail set']\n",
      "X: (1460, 60, 10) y: (1460,) files: 198\n",
      "train: (1166, 60, 10) val: (294, 60, 10)\n",
      "class_weight: {0: 1.1, 1: 0.9166666666666666}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"ivdd_lstm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"ivdd_lstm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ td_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ logits (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input (\u001b[38;5;33mInputLayer\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m10\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ td_dense (\u001b[38;5;33mTimeDistributed\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m30\u001b[0m)         │           \u001b[38;5;34m330\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm1 (\u001b[38;5;33mLSTM\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m30\u001b[0m)         │         \u001b[38;5;34m7,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm2 (\u001b[38;5;33mLSTM\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)             │         \u001b[38;5;34m7,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ logits (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m31\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,001</span> (58.60 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m15,001\u001b[0m (58.60 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,001</span> (58.60 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m15,001\u001b[0m (58.60 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4618 - loss: 0.7121\n",
      "Epoch 1: val_accuracy improved from -inf to 0.50000, saving model to c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train2_model\\ivdd_lstm_20251211-045755_best.keras\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.4616 - loss: 0.7119 - val_accuracy: 0.5000 - val_loss: 0.7052 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4622 - loss: 0.7061\n",
      "Epoch 2: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4616 - loss: 0.7059 - val_accuracy: 0.5000 - val_loss: 0.7034 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m36/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4625 - loss: 0.7043\n",
      "Epoch 3: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4616 - loss: 0.7042 - val_accuracy: 0.5000 - val_loss: 0.7029 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4622 - loss: 0.7033\n",
      "Epoch 4: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4616 - loss: 0.7032 - val_accuracy: 0.5000 - val_loss: 0.7025 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m36/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4625 - loss: 0.7025\n",
      "Epoch 5: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4616 - loss: 0.7024 - val_accuracy: 0.5000 - val_loss: 0.7020 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4620 - loss: 0.7014\n",
      "Epoch 6: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4616 - loss: 0.7014 - val_accuracy: 0.5000 - val_loss: 0.7015 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m36/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4625 - loss: 0.7003\n",
      "Epoch 7: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4616 - loss: 0.7002 - val_accuracy: 0.5000 - val_loss: 0.7009 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4622 - loss: 0.6987\n",
      "Epoch 8: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4616 - loss: 0.6986 - val_accuracy: 0.5000 - val_loss: 0.7002 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4622 - loss: 0.6966\n",
      "Epoch 9: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4616 - loss: 0.6965 - val_accuracy: 0.5000 - val_loss: 0.6993 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m36/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4625 - loss: 0.6935\n",
      "Epoch 10: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4616 - loss: 0.6934 - val_accuracy: 0.5000 - val_loss: 0.6980 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m36/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4625 - loss: 0.6888\n",
      "Epoch 11: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4616 - loss: 0.6887 - val_accuracy: 0.5000 - val_loss: 0.6959 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4657 - loss: 0.6815\n",
      "Epoch 12: val_accuracy improved from 0.50000 to 0.50340, saving model to c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train2_model\\ivdd_lstm_20251211-045755_best.keras\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4658 - loss: 0.6814 - val_accuracy: 0.5034 - val_loss: 0.6935 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4990 - loss: 0.6717\n",
      "Epoch 13: val_accuracy did not improve from 0.50340\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4998 - loss: 0.6717 - val_accuracy: 0.5000 - val_loss: 0.6980 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5442 - loss: 0.6627\n",
      "Epoch 14: val_accuracy improved from 0.50340 to 0.51361, saving model to c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train2_model\\ivdd_lstm_20251211-045755_best.keras\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5451 - loss: 0.6626 - val_accuracy: 0.5136 - val_loss: 0.7064 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5648 - loss: 0.6560\n",
      "Epoch 15: val_accuracy improved from 0.51361 to 0.52041, saving model to c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train2_model\\ivdd_lstm_20251211-045755_best.keras\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5660 - loss: 0.6557 - val_accuracy: 0.5204 - val_loss: 0.7094 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5885 - loss: 0.6512\n",
      "Epoch 16: val_accuracy did not improve from 0.52041\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5892 - loss: 0.6510 - val_accuracy: 0.5204 - val_loss: 0.7106 - learning_rate: 1.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5946 - loss: 0.6488\n",
      "Epoch 17: val_accuracy did not improve from 0.52041\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5954 - loss: 0.6484 - val_accuracy: 0.5204 - val_loss: 0.7110 - learning_rate: 1.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5854 - loss: 0.6484\n",
      "Epoch 18: val_accuracy improved from 0.52041 to 0.53401, saving model to c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train2_model\\ivdd_lstm_20251211-045755_best.keras\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5864 - loss: 0.6482 - val_accuracy: 0.5340 - val_loss: 0.6998 - learning_rate: 5.0000e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m36/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5887 - loss: 0.6476\n",
      "Epoch 19: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5893 - loss: 0.6474 - val_accuracy: 0.5306 - val_loss: 0.6999 - learning_rate: 5.0000e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5872 - loss: 0.6462\n",
      "Epoch 20: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5876 - loss: 0.6461 - val_accuracy: 0.5306 - val_loss: 0.7007 - learning_rate: 5.0000e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5897 - loss: 0.6452\n",
      "Epoch 21: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5904 - loss: 0.6449 - val_accuracy: 0.5272 - val_loss: 0.7017 - learning_rate: 5.0000e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5895 - loss: 0.6441\n",
      "Epoch 22: val_accuracy did not improve from 0.53401\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5897 - loss: 0.6441 - val_accuracy: 0.5272 - val_loss: 0.7026 - learning_rate: 5.0000e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5946 - loss: 0.6419\n",
      "Epoch 23: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5954 - loss: 0.6416 - val_accuracy: 0.5306 - val_loss: 0.7035 - learning_rate: 2.5000e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5934 - loss: 0.6413\n",
      "Epoch 24: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5942 - loss: 0.6409 - val_accuracy: 0.5306 - val_loss: 0.7044 - learning_rate: 2.5000e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m36/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5925 - loss: 0.6411\n",
      "Epoch 25: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5935 - loss: 0.6406 - val_accuracy: 0.5272 - val_loss: 0.7052 - learning_rate: 2.5000e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5958 - loss: 0.6406\n",
      "Epoch 26: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5966 - loss: 0.6402 - val_accuracy: 0.5306 - val_loss: 0.7058 - learning_rate: 2.5000e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5963 - loss: 0.6403\n",
      "Epoch 27: val_accuracy did not improve from 0.53401\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5970 - loss: 0.6399 - val_accuracy: 0.5306 - val_loss: 0.7064 - learning_rate: 2.5000e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m35/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5926 - loss: 0.6395\n",
      "Epoch 28: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5937 - loss: 0.6388 - val_accuracy: 0.5238 - val_loss: 0.7064 - learning_rate: 1.2500e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5948 - loss: 0.6385\n",
      "Epoch 29: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5950 - loss: 0.6384 - val_accuracy: 0.5204 - val_loss: 0.7067 - learning_rate: 1.2500e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m34/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5934 - loss: 0.6391\n",
      "Epoch 30: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5944 - loss: 0.6382 - val_accuracy: 0.5238 - val_loss: 0.7070 - learning_rate: 1.2500e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5934 - loss: 0.6382\n",
      "Epoch 31: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5937 - loss: 0.6381 - val_accuracy: 0.5238 - val_loss: 0.7072 - learning_rate: 1.2500e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5934 - loss: 0.6380\n",
      "Epoch 32: val_accuracy did not improve from 0.53401\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5937 - loss: 0.6379 - val_accuracy: 0.5238 - val_loss: 0.7075 - learning_rate: 1.2500e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5929 - loss: 0.6381\n",
      "Epoch 33: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5937 - loss: 0.6376 - val_accuracy: 0.5238 - val_loss: 0.7076 - learning_rate: 1.0000e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5934 - loss: 0.6376\n",
      "Epoch 34: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5937 - loss: 0.6374 - val_accuracy: 0.5238 - val_loss: 0.7078 - learning_rate: 1.0000e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5929 - loss: 0.6378\n",
      "Epoch 35: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5937 - loss: 0.6373 - val_accuracy: 0.5238 - val_loss: 0.7079 - learning_rate: 1.0000e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5929 - loss: 0.6376\n",
      "Epoch 36: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5937 - loss: 0.6372 - val_accuracy: 0.5238 - val_loss: 0.7081 - learning_rate: 1.0000e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m34/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5932 - loss: 0.6379\n",
      "Epoch 37: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5945 - loss: 0.6370 - val_accuracy: 0.5238 - val_loss: 0.7082 - learning_rate: 1.0000e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m37/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5936 - loss: 0.6374\n",
      "Epoch 38: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5945 - loss: 0.6369 - val_accuracy: 0.5238 - val_loss: 0.7084 - learning_rate: 1.0000e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5939 - loss: 0.6371\n",
      "Epoch 39: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5945 - loss: 0.6368 - val_accuracy: 0.5238 - val_loss: 0.7085 - learning_rate: 1.0000e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5939 - loss: 0.6370\n",
      "Epoch 40: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5945 - loss: 0.6367 - val_accuracy: 0.5238 - val_loss: 0.7086 - learning_rate: 1.0000e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5940 - loss: 0.6367\n",
      "Epoch 41: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5943 - loss: 0.6365 - val_accuracy: 0.5272 - val_loss: 0.7088 - learning_rate: 1.0000e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5938 - loss: 0.6367\n",
      "Epoch 42: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5943 - loss: 0.6364 - val_accuracy: 0.5272 - val_loss: 0.7089 - learning_rate: 1.0000e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m34/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5932 - loss: 0.6371\n",
      "Epoch 43: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5943 - loss: 0.6363 - val_accuracy: 0.5272 - val_loss: 0.7090 - learning_rate: 1.0000e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5948 - loss: 0.6363\n",
      "Epoch 44: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5951 - loss: 0.6362 - val_accuracy: 0.5272 - val_loss: 0.7091 - learning_rate: 1.0000e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m36/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5941 - loss: 0.6367\n",
      "Epoch 45: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5952 - loss: 0.6360 - val_accuracy: 0.5272 - val_loss: 0.7092 - learning_rate: 1.0000e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5967 - loss: 0.6362\n",
      "Epoch 46: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5972 - loss: 0.6359 - val_accuracy: 0.5272 - val_loss: 0.7093 - learning_rate: 1.0000e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5967 - loss: 0.6361\n",
      "Epoch 47: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5972 - loss: 0.6358 - val_accuracy: 0.5272 - val_loss: 0.7094 - learning_rate: 1.0000e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5971 - loss: 0.6358\n",
      "Epoch 48: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5973 - loss: 0.6357 - val_accuracy: 0.5272 - val_loss: 0.7095 - learning_rate: 1.0000e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m36/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5963 - loss: 0.6362\n",
      "Epoch 49: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5973 - loss: 0.6355 - val_accuracy: 0.5272 - val_loss: 0.7095 - learning_rate: 1.0000e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5968 - loss: 0.6357\n",
      "Epoch 50: val_accuracy did not improve from 0.53401\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5973 - loss: 0.6354 - val_accuracy: 0.5272 - val_loss: 0.7096 - learning_rate: 1.0000e-05\n",
      "[INFO] saved curve: c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\fig\\curve_20251211-045755.png\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step\n",
      "\n",
      "[Window-level] classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal     0.5297    0.7891    0.6339       147\n",
      "        ivdd     0.5867    0.2993    0.3964       147\n",
      "\n",
      "    accuracy                         0.5442       294\n",
      "   macro avg     0.5582    0.5442    0.5151       294\n",
      "weighted avg     0.5582    0.5442    0.5151       294\n",
      "\n",
      "[Window-level] confusion matrix:\n",
      " [[116  31]\n",
      " [103  44]]\n",
      "[INFO] saved misclassified windows: c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\val_misclassified\\val_misclassified_20251211-045755.csv  (rows=134)\n",
      "[INFO] saved final model to: c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train2_model\\ivdd_lstm_20251211-045755_final.keras\n",
      "[DONE] training complete.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "IVDD binary training (ivdd vs normal) - train2 (tail_set-centered + per-file min-max)\n",
    "変更点:\n",
    "- プロジェクトルートを自動検出（scripts配下にdataが出来ない）\n",
    "- 学習曲線: loss/acc を1枚のPNGで保存 (fig/curve_YYYYMMDD.png)\n",
    "- 学習モデル: train2_model に YYYYMMDD ベース名で best/final を保存\n",
    "- Val誤分類ウィンドウ一覧を train/val_misclassified/val_misclassified_YYYYMMDD.csv で保存\n",
    "- 既存の正規化（tail_set原点 + 各次元min-max）はそのまま\n",
    "\"\"\"\n",
    "\n",
    "# ===== 安定運用: GPU無効/ログ控えめ/スレッド抑制（必要なら） =====\n",
    "import os\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"-1\")\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "\n",
    "import re, glob, datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TF:\", tf.__version__)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ========= ルート自動検出（scripts直下で実行してもOK） =========\n",
    "def detect_project_root() -> Path:\n",
    "    env = os.environ.get(\"IVDD_PROJECT_ROOT\")\n",
    "    if env:\n",
    "        p = Path(env).expanduser().resolve()\n",
    "        if (p / \"data\").is_dir() or (p / \"scripts\").is_dir():\n",
    "            return p\n",
    "    try:\n",
    "        here = Path(__file__).resolve()\n",
    "        if here.parent.name == \"scripts\":\n",
    "            cand = here.parent.parent\n",
    "            if (cand / \"data\").is_dir() or (cand / \"scripts\").is_dir():\n",
    "                return cand\n",
    "        if (here.parent / \"data\").is_dir() or (here.parent / \"scripts\").is_dir():\n",
    "            return here.parent\n",
    "    except NameError:\n",
    "        pass\n",
    "    cwd = Path.cwd()\n",
    "    for cand in [cwd] + list(cwd.parents):\n",
    "        if (cand / \"data\").is_dir() and (cand / \"scripts\").is_dir():\n",
    "            return cand\n",
    "    return cwd.parent if cwd.name == \"scripts\" else cwd\n",
    "\n",
    "PROJ_ROOT   = detect_project_root()\n",
    "TRAIN_DIR   = PROJ_ROOT / \"data\" / \"train\"\n",
    "TRAIN_CSV_DIR = TRAIN_DIR / \"train_csv\"\n",
    "FIG_DIR     = TRAIN_DIR / \"fig\"\n",
    "MODEL_DIR   = TRAIN_DIR / \"train2_model\"          # ← train2 のモデル置き場\n",
    "VALERR_DIR  = TRAIN_DIR / \"val_misclassified\"     # ← 新設: 誤分類CSV置き場\n",
    "for d in [FIG_DIR, MODEL_DIR, VALERR_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CSV_GLOB = str(TRAIN_CSV_DIR / \"*.csv\")\n",
    "\n",
    "# ========= パラメータ =========\n",
    "KEYPOINTS = [\n",
    "    \"left back paw\",\n",
    "    \"right back paw\",\n",
    "    \"left front paw\",\n",
    "    \"right front paw\",\n",
    "    \"tail set\",\n",
    "]\n",
    "\n",
    "USE_LIKELIHOOD      = False\n",
    "MIN_KEEP_LIKELIHOOD = 0.6\n",
    "\n",
    "SEQ_LEN  = 60\n",
    "STRIDE   = 30\n",
    "DIMS      = 10\n",
    "N_HIDDEN  = 30\n",
    "\n",
    "BATCH_SIZE = 30\n",
    "EPOCHS     = 50\n",
    "LR         = 1e-4\n",
    "L2_LAMBDA  = 1e-4\n",
    "\n",
    "VAL_SPLIT_BY_FILE = True\n",
    "\n",
    "# バイナリ: 0=normal, 1=ivdd\n",
    "CLASS_NAMES  = [\"normal\", \"ivdd\"]\n",
    "CLASS_TO_IDX = {\"normal\": 0, \"ivdd\": 1}\n",
    "\n",
    "# ランごとユニークID（日時）\n",
    "DATE_STR = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "\n",
    "# ========= ラベル推定 =========\n",
    "def infer_label_from_filename(path: str) -> int:\n",
    "    base = os.path.basename(path).lower()\n",
    "    stem = os.path.splitext(base)[0]\n",
    "    tokens = [t for t in re.split(r'[^a-z0-9]+', stem) if t]\n",
    "\n",
    "    # 1) 先頭トークンが ivdd/normal + 任意の数字 を許容（例: ivdd1, normal20）\n",
    "    if tokens:\n",
    "        head = tokens[0]\n",
    "        if re.fullmatch(r'ivdd\\d*', head):\n",
    "            return CLASS_TO_IDX[\"ivdd\"]\n",
    "        if re.fullmatch(r'normal\\d*', head):\n",
    "            return CLASS_TO_IDX[\"normal\"]\n",
    "\n",
    "    # 2) 他トークンに厳密一致（両方ヒットは避ける）\n",
    "    if (\"ivdd\" in tokens) and (\"normal\" not in tokens):\n",
    "        return CLASS_TO_IDX[\"ivdd\"]\n",
    "    if (\"normal\" in tokens) and (\"ivdd\" not in tokens):\n",
    "        return CLASS_TO_IDX[\"normal\"]\n",
    "\n",
    "    # 3) 親ディレクトリ名に基づくフォールバック（サブフォルダ運用時）\n",
    "    parent_tokens = [t for t in re.split(r'[^a-z0-9]+', os.path.dirname(path).lower()) if t]\n",
    "    if (\"ivdd\" in parent_tokens) and (\"normal\" not in parent_tokens):\n",
    "        return CLASS_TO_IDX[\"ivdd\"]\n",
    "    if (\"normal\" in parent_tokens) and (\"ivdd\" not in parent_tokens):\n",
    "        return CLASS_TO_IDX[\"normal\"]\n",
    "\n",
    "    # 4) 最後の手段: どこかで ivdd/normal の直後が数字なら採用（IvddOct は除外）\n",
    "    if re.search(r'(?<![a-z])ivdd(?=\\d)', stem):\n",
    "        return CLASS_TO_IDX[\"ivdd\"]\n",
    "    if re.search(r'(?<![a-z])normal(?=\\d)', stem):\n",
    "        return CLASS_TO_IDX[\"normal\"]\n",
    "\n",
    "    raise ValueError(f\"ラベル不明: {base}（先頭を ivdd_ / normal_ または ivdd1_ / normal1_ にするか、上の判定を調整）\")\n",
    "\n",
    "# ========= DLC 読み取り & 正規化 =========\n",
    "def _norm_name(s: str) -> str:\n",
    "    return \"\".join(ch for ch in s.lower() if ch not in \" _-\")\n",
    "\n",
    "def _resolve_keypoints(all_bodyparts, requested):\n",
    "    norm2orig = {}\n",
    "    for bp in all_bodyparts:\n",
    "        k = _norm_name(bp)\n",
    "        if k not in norm2orig:\n",
    "            norm2orig[k] = bp\n",
    "    resolved, missing = [], []\n",
    "    for req in requested:\n",
    "        k = _norm_name(req)\n",
    "        if k in norm2orig:\n",
    "            resolved.append(norm2orig[k])\n",
    "        else:\n",
    "            missing.append(req)\n",
    "    if missing:\n",
    "        raise ValueError(f\"指定キーポイントがCSVに見つかりません: {missing}\\n利用可能: {all_bodyparts}\")\n",
    "    return resolved\n",
    "\n",
    "def read_dlc_5kp_xy(csv_path: str, keypoints, use_likelihood=True, min_keep_likelihood=0.6):\n",
    "    df = pd.read_csv(csv_path, header=[0,1,2], index_col=0)\n",
    "    bodyparts = list({bp for (_, bp, _) in df.columns})\n",
    "    use_kps = _resolve_keypoints(bodyparts, keypoints)\n",
    "\n",
    "    cols = {}\n",
    "    for bp in use_kps:\n",
    "        cols[f\"{bp}_x\"] = df.xs((bp, \"x\"), level=[1,2], axis=1)\n",
    "        cols[f\"{bp}_y\"] = df.xs((bp, \"y\"), level=[1,2], axis=1)\n",
    "    X_df = pd.concat(cols.values(), axis=1)\n",
    "    X_df.columns = list(cols.keys())\n",
    "\n",
    "    if use_likelihood:\n",
    "        for bp in use_kps:\n",
    "            try:\n",
    "                lcol = df.xs((bp, \"likelihood\"), level=[1,2], axis=1).values.flatten()\n",
    "                low = lcol < min_keep_likelihood\n",
    "                for c in [f\"{bp}_x\", f\"{bp}_y\"]:\n",
    "                    v = X_df[c].values\n",
    "                    v[low] = np.nan\n",
    "                    X_df[c] = v\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "    X_df = X_df.interpolate(method=\"linear\", limit_direction=\"both\", axis=0)\n",
    "    X_df = X_df.bfill().ffill().fillna(0.0)\n",
    "    X = X_df.values.astype(np.float32)  # (T,10)\n",
    "    return X, use_kps\n",
    "\n",
    "def normalize_tailset_minmax(X: np.ndarray, used_kps, ref_name: str = \"tail set\", eps: float = 1e-6) -> np.ndarray:\n",
    "    low = [s.lower() for s in used_kps]\n",
    "    if ref_name.lower() not in low:\n",
    "        raise ValueError(f\"'{ref_name}' が used_kps に見つかりません: {used_kps}\")\n",
    "    ref_idx = low.index(ref_name.lower())\n",
    "\n",
    "    # 原点平行移動\n",
    "    cx = X[:, 2*ref_idx]\n",
    "    cy = X[:, 2*ref_idx + 1]\n",
    "    Xc = X.copy()\n",
    "    for i in range(len(used_kps)):\n",
    "        Xc[:, 2*i]   -= cx\n",
    "        Xc[:, 2*i+1] -= cy\n",
    "\n",
    "    # 各次元 min-max（ファイル単位）\n",
    "    x_min = Xc.min(axis=0, keepdims=True)\n",
    "    x_max = Xc.max(axis=0, keepdims=True)\n",
    "    Xn = (Xc - x_min) / (x_max - x_min + eps)\n",
    "    return Xn\n",
    "\n",
    "def make_windows(X: np.ndarray, seq_len: int, stride: int):\n",
    "    n = X.shape[0]\n",
    "    if n < seq_len:\n",
    "        return np.empty((0, seq_len, X.shape[1]), dtype=X.dtype), []\n",
    "    starts = list(range(0, n - seq_len + 1, stride))\n",
    "    Xw = np.stack([X[s:s+seq_len] for s in starts], axis=0) if starts else np.empty((0, seq_len, X.shape[1]), dtype=X.dtype)\n",
    "    return Xw, starts\n",
    "\n",
    "def build_dataset(csv_paths, seq_len=SEQ_LEN, stride=STRIDE):\n",
    "    \"\"\"\n",
    "    戻り:\n",
    "      X: (N, T, D)\n",
    "      y: (N,) 0/1\n",
    "      file_ids: (N,) 元CSVファイル名\n",
    "      starts: (N,) ウィンドウ開始フレーム\n",
    "    \"\"\"\n",
    "    X_list, y_list, file_ids, starts_all = [], [], [], []\n",
    "    used_kps_any = None\n",
    "\n",
    "    for p in csv_paths:\n",
    "        y_lab = infer_label_from_filename(p)\n",
    "        X_raw, used_kps = read_dlc_5kp_xy(\n",
    "            p, keypoints=KEYPOINTS, use_likelihood=USE_LIKELIHOOD, min_keep_likelihood=MIN_KEEP_LIKELIHOOD\n",
    "        )\n",
    "        if used_kps_any is None:\n",
    "            used_kps_any = used_kps\n",
    "        if X_raw.shape[1] != DIMS:\n",
    "            raise ValueError(f\"{Path(p).name}: 次元 {X_raw.shape[1]} != 期待 {DIMS}\")\n",
    "\n",
    "        # ★ tail_set中心 + min-max 正規化（維持）\n",
    "        X_norm = normalize_tailset_minmax(X_raw, used_kps, ref_name=\"tail set\")\n",
    "\n",
    "        X_win, starts = make_windows(X_norm, seq_len, stride)\n",
    "        if X_win.shape[0] == 0:\n",
    "            print(f\"[WARN] {Path(p).name}: フレーム不足（{seq_len}未満）でスキップ\")\n",
    "            continue\n",
    "\n",
    "        X_list.append(X_win)\n",
    "        y_list.append(np.full((X_win.shape[0],), y_lab, dtype=np.int64))\n",
    "        file_ids += [Path(p).name] * X_win.shape[0]\n",
    "        starts_all += starts\n",
    "\n",
    "    if not X_list:\n",
    "        raise RuntimeError(\"データが作れませんでした。CSV名に 'ivdd' / 'normal' を含めてください。\")\n",
    "\n",
    "    X = np.concatenate(X_list, axis=0)\n",
    "    y = np.concatenate(y_list, axis=0)\n",
    "    file_ids = np.array(file_ids)\n",
    "    starts_all = np.array(starts_all, dtype=int)\n",
    "    print(f\"[INFO] 使用キーポイント実名: {used_kps_any}\")\n",
    "    return X, y, file_ids, starts_all\n",
    "\n",
    "# ========= モデル =========\n",
    "def build_lstm_model(seq_len: int, dims: int, n_hidden: int, l2_lambda: float = 1e-4) -> keras.Model:\n",
    "    reg = keras.regularizers.l2(l2_lambda)\n",
    "    inp = keras.Input(shape=(seq_len, dims), name=\"input\")\n",
    "    td  = keras.layers.TimeDistributed(\n",
    "        keras.layers.Dense(n_hidden, activation=\"relu\", kernel_regularizer=reg),\n",
    "        name=\"td_dense\"\n",
    "    )(inp)\n",
    "    x   = keras.layers.LSTM(n_hidden, return_sequences=True, kernel_regularizer=reg, name=\"lstm1\")(td)\n",
    "    x   = keras.layers.LSTM(n_hidden, kernel_regularizer=reg, name=\"lstm2\")(x)\n",
    "    out = keras.layers.Dense(1, kernel_regularizer=reg, name=\"logits\")(x)  # from_logits=True\n",
    "    return keras.Model(inp, out, name=\"ivdd_lstm\")\n",
    "\n",
    "# ========= データ読み込み =========\n",
    "csv_files = sorted(glob.glob(CSV_GLOB))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"CSV が見つかりません: {CSV_GLOB}\")\n",
    "\n",
    "X, y, file_ids, starts = build_dataset(csv_files, SEQ_LEN, STRIDE)\n",
    "print(\"X:\", X.shape, \"y:\", y.shape, \"files:\", len(np.unique(file_ids)))\n",
    "\n",
    "# ========= 分割（ファイル単位推奨） =========\n",
    "if VAL_SPLIT_BY_FILE:\n",
    "    uniq = np.unique(file_ids)\n",
    "    tr_files, va_files = train_test_split(uniq, test_size=0.2, random_state=42, shuffle=True)\n",
    "    tr_mask = np.isin(file_ids, tr_files)\n",
    "    va_mask = np.isin(file_ids, va_files)\n",
    "    X_train, y_train = X[tr_mask], y[tr_mask]\n",
    "    X_val,   y_val   = X[va_mask], y[va_mask]\n",
    "    val_file_ids     = file_ids[va_mask]\n",
    "    val_starts       = starts[va_mask]\n",
    "else:\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    # ファイル名/開始は未知になるのでダミー\n",
    "    val_file_ids = np.array([\"unknown.csv\"]*len(y_val))\n",
    "    val_starts   = np.arange(len(y_val))\n",
    "\n",
    "print(\"train:\", X_train.shape, \"val:\", X_val.shape)\n",
    "\n",
    "# ========= クラス不均衡対策 =========\n",
    "unique_classes = np.unique(y_train)\n",
    "if set(unique_classes) == {0, 1}:\n",
    "    cls_w = compute_class_weight(class_weight=\"balanced\", classes=np.array([0,1]), y=y_train)\n",
    "    class_weight = {0: float(cls_w[0]), 1: float(cls_w[1])}\n",
    "else:\n",
    "    class_weight = None\n",
    "print(\"class_weight:\", class_weight)\n",
    "\n",
    "# ========= 学習（EarlyStoppingなし） =========\n",
    "model = build_lstm_model(SEQ_LEN, DIMS, N_HIDDEN, L2_LAMBDA)\n",
    "model.summary()\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=LR)\n",
    "loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = [keras.metrics.BinaryAccuracy(name=\"accuracy\")]\n",
    "model.compile(optimizer=opt, loss=loss_fn, metrics=metrics)\n",
    "\n",
    "best_model_path  = MODEL_DIR / f\"ivdd_lstm_{DATE_STR}_best.keras\"\n",
    "final_model_path = MODEL_DIR / f\"ivdd_lstm_{DATE_STR}_final.keras\"\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        str(best_model_path), monitor=\"val_accuracy\",\n",
    "        save_best_only=True, save_weights_only=False, mode=\"max\", verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", mode=\"min\",\n",
    "        factor=0.5, patience=5, min_lr=1e-5, verbose=1\n",
    "    ),\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_weight=class_weight,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# ========= 学習曲線（loss/acc を1枚で保存） =========\n",
    "curve_png = FIG_DIR / f\"curve_{DATE_STR}.png\"\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1); plt.plot(history.history[\"loss\"], label=\"train\"); plt.plot(history.history[\"val_loss\"], label=\"val\")\n",
    "plt.title(\"Loss\"); plt.legend()\n",
    "plt.subplot(1,2,2); plt.plot(history.history[\"accuracy\"], label=\"train\"); plt.plot(history.history[\"val_accuracy\"], label=\"val\")\n",
    "plt.title(\"Accuracy\"); plt.legend()\n",
    "plt.tight_layout(); plt.savefig(curve_png, dpi=150); plt.close()\n",
    "print(f\"[INFO] saved curve: {curve_png}\")\n",
    "\n",
    "# ========= Val評価 & 誤分類CSV =========\n",
    "logits_val = model.predict(X_val, batch_size=64)\n",
    "p_ivdd = tf.math.sigmoid(logits_val).numpy().ravel()   # ivdd=1 の確率\n",
    "y_pred = (p_ivdd >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n[Window-level] classification_report:\")\n",
    "print(classification_report(y_val, y_pred, target_names=CLASS_NAMES, digits=4))\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "print(\"[Window-level] confusion matrix:\\n\", cm)\n",
    "\n",
    "# 誤分類ウィンドウを書き出し（要件: train配下の新フォルダにYYYYMMDD名）\n",
    "val_df = pd.DataFrame({\n",
    "    \"file\": val_file_ids,\n",
    "    \"win_start\": val_starts,           # ウィンドウ開始フレーム\n",
    "    \"true\": y_val,\n",
    "    \"pred\": y_pred,\n",
    "    \"p_ivdd\": p_ivdd,\n",
    "    \"p_normal\": 1.0 - p_ivdd,\n",
    "})\n",
    "errors_df = val_df[val_df[\"true\"] != val_df[\"pred\"]].copy()\n",
    "err_csv = VALERR_DIR / f\"val_misclassified_{DATE_STR}.csv\"\n",
    "errors_df.to_csv(err_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[INFO] saved misclassified windows: {err_csv}  (rows={len(errors_df)})\")\n",
    "\n",
    "# ========= 最終モデル保存 =========\n",
    "model.save(str(final_model_path))\n",
    "print(f\"[INFO] saved final model to: {final_model_path}\")\n",
    "\n",
    "print(\"[DONE] training complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLya",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
