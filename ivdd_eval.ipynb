{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a717eaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] ivdd1_case1DLC_resnet50_IvddOct30shuffle1_100000.csv: フレーム不足（60未満）でスキップ\n",
      "[INFO] 使用キーポイント: ['left back paw', 'right back paw', 'left front paw', 'right front paw', 'tail set']\n",
      "X: (288, 60, 10) y: (288,) files: 42\n",
      "[WARN] load_model 失敗: <class '__main__.LSTMWithL2'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n",
      "\n",
      "config={'module': None, 'class_name': 'LSTMWithL2', 'config': {'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}}, 'registered_name': 'LSTMWithL2', 'build_config': {'input_shape': [None, 60, 10]}, 'compile_config': None}.\n",
      "\n",
      "Exception encountered: Unable to revive model from config. When overriding the `get_config()` method, make sure that the returned config contains all items used as arguments in the  constructor to <class '__main__.LSTMWithL2'>, which is the default behavior. You can override this default behavior by defining a `from_config(cls, config)` class method to specify how to create an instance of LSTMWithL2 from its config.\n",
      "\n",
      "Received config={'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}}\n",
      "\n",
      "Error encountered during deserialization: LSTMWithL2.__init__() got an unexpected keyword argument 'trainable'\n",
      "[INFO] Loaded weights into fresh model from: C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\checkpoints\\ivdd_lstm_final copy.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step\n",
      "\n",
      "[Window-level] classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ivdd     1.0000    0.9186    0.9576       172\n",
      "      normal     0.8923    1.0000    0.9431       116\n",
      "\n",
      "    accuracy                         0.9514       288\n",
      "   macro avg     0.9462    0.9593    0.9503       288\n",
      "weighted avg     0.9566    0.9514    0.9517       288\n",
      "\n",
      "[Window-level] confusion matrix:\n",
      " [[158  14]\n",
      " [  0 116]]\n",
      "[INFO] 保存: window_predictions.csv\n",
      "\n",
      "[File-level Majority Vote] classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ivdd     1.0000    0.9583    0.9787        24\n",
      "      normal     0.9474    1.0000    0.9730        18\n",
      "\n",
      "    accuracy                         0.9762        42\n",
      "   macro avg     0.9737    0.9792    0.9758        42\n",
      "weighted avg     0.9774    0.9762    0.9763        42\n",
      "\n",
      "[File-level Majority Vote] confusion matrix:\n",
      " [[23  1]\n",
      " [ 0 18]]\n",
      "\n",
      "[File-level Mean Probability] classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ivdd     1.0000    0.9583    0.9787        24\n",
      "      normal     0.9474    1.0000    0.9730        18\n",
      "\n",
      "    accuracy                         0.9762        42\n",
      "   macro avg     0.9737    0.9792    0.9758        42\n",
      "weighted avg     0.9774    0.9762    0.9763        42\n",
      "\n",
      "[File-level Mean Probability] confusion matrix:\n",
      " [[23  1]\n",
      " [ 0 18]]\n",
      "[INFO] 保存: roc_window.png\n",
      "[INFO] 保存: cm_window.png\n",
      "[INFO] 保存: file_level_predictions.csv\n",
      "\n",
      "[Done] 検証が完了しました。出力は: C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\test\\eval_outputs\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Binary IVDD validation (ivdd vs normal) from DeepLabCut CSV (3-level header)\n",
    "- trainで使ったのと同じ前処理/ネットワーク構造\n",
    "- 5 keypoints × (x,y) = 10 dims\n",
    "- 窓長 SEQ_LEN=30, STRIDE=5（要件通り）\n",
    "- 学習済み .keras モデルを読み込んで、評価・混同行列・ROC・各種CSV出力\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "# 安定運用（GPU隠蔽/ログ控えめ/スレッド制御は任意）\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"-1\")\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    roc_auc_score, roc_curve, auc\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ========= パラメータ =========\n",
    "# 学習時と同じ値を使ってください\n",
    "DATA_DIR   = r\"C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\test\"          # 評価用CSVを置くフォルダ（例：test用）\n",
    "CSV_GLOB   = os.path.join(DATA_DIR, \"*.csv\")\n",
    "\n",
    "# 学習時に保存したベストモデル\n",
    "CKPT_DIR   = r\"C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\\checkpoints\"\n",
    "CKPT_PATH  = os.path.join(CKPT_DIR, \"ivdd_lstm_final copy.keras\")    # ivdd_train.ipynb と同じパスに合わせる\n",
    "\n",
    "# 5点（あなたが指定した名称）\n",
    "KEYPOINTS = [\n",
    "    \"left back paw\",\n",
    "    \"right back paw\",\n",
    "    \"left front paw\",\n",
    "    \"right front paw\",\n",
    "    \"tail set\",\n",
    "]\n",
    "\n",
    "USE_LIKELIHOOD       = True\n",
    "MIN_KEEP_LIKELIHOOD  = 0.6\n",
    "\n",
    "SEQ_LEN  = 60\n",
    "STRIDE   = 30\n",
    "DIMS     = 10    # 5点×(x,y)\n",
    "N_HIDDEN = 30    # 学習時と一致\n",
    "CLASS_NAMES  = [\"ivdd\", \"normal\"]\n",
    "CLASS_TO_IDX = {c:i for i,c in enumerate(CLASS_NAMES)}\n",
    "N_CLASSES    = 2\n",
    "\n",
    "# 可視化保存先\n",
    "OUT_DIR = os.path.join(DATA_DIR, \"eval_outputs\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ========= ラベル推定（ファイル名から） =========\n",
    "def infer_label_from_filename(path: str) -> int:\n",
    "    \"\"\"\n",
    "    ファイル名からラベルを推定する。\n",
    "    1) 拡張子を除いたファイル名を非英数字で分割 → トークンに 'ivdd' / 'normal' が「厳密一致」したら採用\n",
    "    2) 1)で一意に決まらなければ、先頭トークンが 'ivdd' / 'normal' なら採用\n",
    "    3) まだ決まらなければ、親ディレクトリ名のトークンを参照\n",
    "    \"\"\"\n",
    "    name = os.path.basename(path).lower()\n",
    "    stem = os.path.splitext(name)[0]\n",
    "\n",
    "    # 非英数字で分割してトークン化（例: \"normal_10dlc_resnet50_ivddoct30\" -> [\"normal\",\"10dlc\",\"resnet50\",\"ivddoct30\"]）\n",
    "    tokens = [t for t in re.split(r'[^a-z0-9]+', stem) if t]\n",
    "    token_set = set(tokens)\n",
    "\n",
    "    has_ivdd_exact   = ('ivdd' in token_set)\n",
    "    has_normal_exact = ('normal' in token_set)\n",
    "\n",
    "    if has_ivdd_exact and not has_normal_exact:\n",
    "        return CLASS_TO_IDX[\"ivdd\"]\n",
    "    if has_normal_exact and not has_ivdd_exact:\n",
    "        return CLASS_TO_IDX[\"normal\"]\n",
    "\n",
    "    # 先頭トークン優先のフォールバック\n",
    "    if tokens:\n",
    "        if tokens[0] in CLASS_TO_IDX:\n",
    "            return CLASS_TO_IDX[tokens[0]]\n",
    "\n",
    "    # 親ディレクトリにも 'ivdd' / 'normal' が入っていればそれを採用（片方のみヒット時）\n",
    "    parent_tokens = [t for t in re.split(r'[^a-z0-9]+', os.path.dirname(path).lower()) if t]\n",
    "    pset = set(parent_tokens)\n",
    "    p_has_ivdd   = ('ivdd' in pset)\n",
    "    p_has_normal = ('normal' in pset)\n",
    "    if p_has_ivdd and not p_has_normal:\n",
    "        return CLASS_TO_IDX[\"ivdd\"]\n",
    "    if p_has_normal and not p_has_ivdd:\n",
    "        return CLASS_TO_IDX[\"normal\"]\n",
    "\n",
    "    # それでも決まらない場合は例外\n",
    "    raise ValueError(\n",
    "        f\"ラベルを特定できません: {name} \"\n",
    "        f\"(推奨: ファイル名の先頭を 'ivdd_' か 'normal_' にしてください)\"\n",
    "    )\n",
    "\n",
    "# ========= 前処理（train と同じ流儀） =========\n",
    "def _norm_name(s: str) -> str:\n",
    "    return \"\".join(ch for ch in s.lower() if ch not in \" _-\")\n",
    "\n",
    "def _resolve_keypoints(all_bodyparts, requested):\n",
    "    norm2orig = {}\n",
    "    for bp in all_bodyparts:\n",
    "        k = _norm_name(bp)\n",
    "        if k not in norm2orig:\n",
    "            norm2orig[k] = bp\n",
    "    resolved, missing = [], []\n",
    "    for req in requested:\n",
    "        k = _norm_name(req)\n",
    "        if k in norm2orig:\n",
    "            resolved.append(norm2orig[k])\n",
    "        else:\n",
    "            missing.append(req)\n",
    "    if missing:\n",
    "        raise ValueError(f\"指定キーポイントがCSVで見つかりません: {missing}\\n利用可能: {all_bodyparts}\")\n",
    "    return resolved\n",
    "\n",
    "def read_dlc_5kp_xy(csv_path: str,\n",
    "                    keypoints,\n",
    "                    use_likelihood=True,\n",
    "                    min_keep_likelihood=0.6) -> tuple[np.ndarray, list[str]]:\n",
    "    \"\"\"\n",
    "    DLCの3段ヘッダCSVを読み込み、指定5点の (x,y) だけを抽出して (N,10) を返す。\n",
    "    低likelihoodはNaN→補間（線形/前方/後方）→0埋め。\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, header=[0,1,2], index_col=0)\n",
    "    bodyparts = list({bp for (_, bp, _) in df.columns})\n",
    "    use_kps = _resolve_keypoints(bodyparts, keypoints)\n",
    "\n",
    "    # (x,y) 抽出\n",
    "    cols = {}\n",
    "    for bp in use_kps:\n",
    "        cols[f\"{bp}_x\"] = df.xs((bp, \"x\"), level=[1,2], axis=1)\n",
    "        cols[f\"{bp}_y\"] = df.xs((bp, \"y\"), level=[1,2], axis=1)\n",
    "    X_df = pd.concat(cols.values(), axis=1)\n",
    "    X_df.columns = list(cols.keys())\n",
    "\n",
    "    # likelihood によるマスク\n",
    "    if use_likelihood:\n",
    "        for bp in use_kps:\n",
    "            try:\n",
    "                lcol = df.xs((bp, \"likelihood\"), level=[1,2], axis=1).values.flatten()\n",
    "                low = lcol < min_keep_likelihood\n",
    "                for c in [f\"{bp}_x\", f\"{bp}_y\"]:\n",
    "                    vals = X_df[c].values\n",
    "                    vals[low] = np.nan\n",
    "                    X_df[c] = vals\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "    # 線形補間 → 前方/後方補完 → 0埋め（FutureWarning対策: bfill()/ffill() を使用）\n",
    "    X_df = X_df.interpolate(method=\"linear\", limit_direction=\"both\", axis=0)\n",
    "    X_df = X_df.bfill().ffill().fillna(0.0)\n",
    "\n",
    "    X = X_df.values.astype(np.float32)  # (T,10)\n",
    "    return X, use_kps\n",
    "\n",
    "def zscore_per_file(X: np.ndarray, eps: float=1e-6) -> np.ndarray:\n",
    "    mu = X.mean(axis=0, keepdims=True)\n",
    "    sd = X.std(axis=0, keepdims=True)\n",
    "    return (X - mu) / (sd + eps)\n",
    "\n",
    "def make_windows(X: np.ndarray, seq_len: int, stride: int) -> np.ndarray:\n",
    "    n = X.shape[0]\n",
    "    if n < seq_len:\n",
    "        return np.empty((0, seq_len, X.shape[1]), dtype=X.dtype)\n",
    "    starts = range(0, n - seq_len + 1, stride)\n",
    "    return np.stack([X[s:s+seq_len] for s in starts], axis=0)\n",
    "\n",
    "def build_dataset(csv_paths, seq_len=SEQ_LEN, stride=STRIDE):\n",
    "    X_list, y_list, file_ids, starts = [], [], [], []\n",
    "    used_kps_any = None\n",
    "    for p in csv_paths:\n",
    "        y_lab = infer_label_from_filename(p)\n",
    "\n",
    "        X_raw, used_kps = read_dlc_5kp_xy(\n",
    "            p,\n",
    "            keypoints=KEYPOINTS,\n",
    "            use_likelihood=USE_LIKELIHOOD,\n",
    "            min_keep_likelihood=MIN_KEEP_LIKELIHOOD\n",
    "        )\n",
    "        if used_kps_any is None:\n",
    "            used_kps_any = used_kps\n",
    "\n",
    "        if X_raw.shape[1] != DIMS:\n",
    "            raise ValueError(f\"{os.path.basename(p)}: 取り出し次元 {X_raw.shape[1]} != 期待 {DIMS}\")\n",
    "\n",
    "        X_raw = zscore_per_file(X_raw)\n",
    "        X_win = make_windows(X_raw, seq_len, stride)  # (M, T, D)\n",
    "        if X_win.shape[0] == 0:\n",
    "            print(f\"[WARN] {os.path.basename(p)}: フレーム不足（{seq_len}未満）でスキップ\")\n",
    "            continue\n",
    "\n",
    "        X_list.append(X_win)\n",
    "        y_list.append(np.full((X_win.shape[0],), y_lab, dtype=np.int64))\n",
    "        file_ids += [os.path.basename(p)]*X_win.shape[0]\n",
    "        starts  += list(range(0, X_raw.shape[0] - seq_len + 1, stride))\n",
    "\n",
    "    if not X_list:\n",
    "        raise RuntimeError(\"評価用データが作れませんでした。CSV名に 'ivdd' または 'normal' を含めてください。\")\n",
    "\n",
    "    X = np.concatenate(X_list, axis=0)\n",
    "    y = np.concatenate(y_list, axis=0)\n",
    "    file_ids = np.array(file_ids)\n",
    "    starts = np.array(starts)\n",
    "    print(f\"[INFO] 使用キーポイント: {used_kps_any}\")\n",
    "    return X, y, file_ids, starts\n",
    "\n",
    "# ========= モデル定義（学習時と同一） =========\n",
    "class LSTM_RNN(keras.Model):\n",
    "    def __init__(self, n_input, n_hidden, n_classes):\n",
    "        super().__init__()\n",
    "        self.input_dense = keras.layers.Dense(n_hidden, activation='relu')\n",
    "        self.time_dist   = keras.layers.TimeDistributed(self.input_dense)\n",
    "        self.lstm1 = keras.layers.LSTM(n_hidden, return_sequences=True)\n",
    "        self.lstm2 = keras.layers.LSTM(n_hidden)\n",
    "        self.out   = keras.layers.Dense(n_classes)  # logits\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.time_dist(x)\n",
    "        x = self.lstm1(x, training=training)\n",
    "        x = self.lstm2(x, training=training)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class LSTMWithL2(LSTM_RNN):\n",
    "    def __init__(self, n_input, n_hidden, n_classes, l2_lambda=1e-4):\n",
    "        super().__init__(n_input, n_hidden, n_classes)\n",
    "        self.l2_lambda = l2_lambda\n",
    "\n",
    "# ========= データ読み込み =========\n",
    "csv_files = sorted(glob.glob(CSV_GLOB))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"評価用CSVが見つかりません: {CSV_GLOB}\")\n",
    "\n",
    "X, y, file_ids, starts = build_dataset(csv_files, SEQ_LEN, STRIDE)\n",
    "print(\"X:\", X.shape, \"y:\", y.shape, \"files:\", len(np.unique(file_ids)))\n",
    "\n",
    "# ========= モデル読込 =========\n",
    "def load_trained_model():\n",
    "    # まずは .keras の「フルモデル」読み込み（最も簡単）\n",
    "    try:\n",
    "        m = keras.models.load_model(\n",
    "            CKPT_PATH,\n",
    "            custom_objects={\"LSTMWithL2\": LSTMWithL2, \"LSTM_RNN\": LSTM_RNN},\n",
    "            compile=False\n",
    "        )\n",
    "        print(f\"[INFO] Loaded full model: {CKPT_PATH}\")\n",
    "        return m\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] load_model 失敗: {e}\")\n",
    "        # フォールバック: 同構造モデルを作ってから load_weights（もしweightsのみ保存していた場合）\n",
    "        m = LSTMWithL2(n_input=DIMS, n_hidden=N_HIDDEN, n_classes=N_CLASSES, l2_lambda=1e-4)\n",
    "        _ = m(tf.zeros([1, SEQ_LEN, DIMS]))  # build\n",
    "        try:\n",
    "            m.load_weights(CKPT_PATH)\n",
    "            print(f\"[INFO] Loaded weights into fresh model from: {CKPT_PATH}\")\n",
    "            return m\n",
    "        except Exception as e2:\n",
    "            raise RuntimeError(f\"モデルの読み込みに失敗しました。: {e2}\")\n",
    "\n",
    "model = load_trained_model()\n",
    "\n",
    "# ========= 推論（ウィンドウ単位） =========\n",
    "logits = model.predict(X, batch_size=64)\n",
    "# Kerasの出力が logits のはず（Dense(n_classes)）\n",
    "probs = tf.nn.softmax(logits, axis=1).numpy()\n",
    "y_pred = probs.argmax(axis=1)\n",
    "\n",
    "# ========= レポート（ウィンドウ単位） =========\n",
    "print(\"\\n[Window-level] classification_report:\")\n",
    "print(classification_report(y, y_pred, target_names=CLASS_NAMES, digits=4))\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "print(\"[Window-level] confusion matrix:\\n\", cm)\n",
    "\n",
    "# ========= ファイル単位（多数決/平均確率） =========\n",
    "df_win = pd.DataFrame({\n",
    "    \"file\": file_ids,\n",
    "    \"start\": starts,\n",
    "    \"true\": y,\n",
    "    \"pred\": y_pred,\n",
    "    \"p_ivdd\": probs[:, CLASS_TO_IDX[\"ivdd\"]],\n",
    "    \"p_normal\": probs[:, CLASS_TO_IDX[\"normal\"]],\n",
    "})\n",
    "df_win.sort_values([\"file\",\"start\"], inplace=True)\n",
    "df_win.to_csv(os.path.join(OUT_DIR, \"window_predictions.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[INFO] 保存: window_predictions.csv\")\n",
    "\n",
    "# 多数決\n",
    "major_pred = df_win.groupby(\"file\")[\"pred\"].agg(lambda a: np.bincount(a).argmax())\n",
    "true_file  = df_win.groupby(\"file\")[\"true\"].first()  # 同一ファイルは同一ラベルの想定\n",
    "print(\"\\n[File-level Majority Vote] classification_report:\")\n",
    "print(classification_report(true_file.values, major_pred.values, target_names=CLASS_NAMES, digits=4))\n",
    "print(\"[File-level Majority Vote] confusion matrix:\\n\", confusion_matrix(true_file.values, major_pred.values))\n",
    "\n",
    "# 平均確率→argmax\n",
    "mean_prob = df_win.groupby(\"file\")[[\"p_ivdd\", \"p_normal\"]].mean()\n",
    "mean_pred = mean_prob.values.argmax(axis=1)\n",
    "print(\"\\n[File-level Mean Probability] classification_report:\")\n",
    "print(classification_report(true_file.values, mean_pred, target_names=CLASS_NAMES, digits=4))\n",
    "print(\"[File-level Mean Probability] confusion matrix:\\n\", confusion_matrix(true_file.values, mean_pred))\n",
    "\n",
    "# ========= ROC（ウィンドウ単位） =========\n",
    "try:\n",
    "    y_score_ivdd = probs[:, CLASS_TO_IDX[\"ivdd\"]]\n",
    "    auc_val = roc_auc_score(y, y_score_ivdd)\n",
    "    fpr, tpr, _ = roc_curve(y, y_score_ivdd, pos_label=CLASS_TO_IDX[\"ivdd\"])\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {auc_val:.3f}\")\n",
    "    plt.plot([0,1], [0,1], linestyle=\"--\")\n",
    "    plt.title(\"ROC (window-level)\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, \"roc_window.png\"), dpi=150)\n",
    "    plt.close()\n",
    "    print(\"[INFO] 保存: roc_window.png\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] ROC プロット失敗: {e}\")\n",
    "\n",
    "# ========= 混同行列プロット（ウィンドウ単位） =========\n",
    "def plot_cm(cm, labels, title, path_png):\n",
    "    cmn = cm.astype(np.float32) / (cm.sum() + 1e-6)\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    im = ax.imshow(cmn, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(len(labels)), yticks=np.arange(len(labels)),\n",
    "           xticklabels=labels, yticklabels=labels,\n",
    "           ylabel='True label', xlabel='Predicted label',\n",
    "           title=title)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    # 枠内の数字は生のカウントで\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, int(cm[i, j]), ha=\"center\",\n",
    "                    color=\"white\" if cmn[i, j] > cmn.max()/2 else \"black\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "plot_cm(cm, CLASS_NAMES, \"Confusion Matrix (window-level)\",\n",
    "        os.path.join(OUT_DIR, \"cm_window.png\"))\n",
    "print(\"[INFO] 保存: cm_window.png\")\n",
    "\n",
    "# ========= 予測CSV（ファイル集約） =========\n",
    "df_file = pd.DataFrame({\n",
    "    \"file\": mean_prob.index,\n",
    "    \"true\": [CLASS_NAMES[t] for t in true_file.values],\n",
    "    \"pred_majority\": [CLASS_NAMES[p] for p in major_pred.values],\n",
    "    \"pred_meanprob\": [CLASS_NAMES[p] for p in mean_pred],\n",
    "    \"p_ivdd_mean\": mean_prob[\"p_ivdd\"].values,\n",
    "    \"p_normal_mean\": mean_prob[\"p_normal\"].values,\n",
    "})\n",
    "df_file.to_csv(os.path.join(OUT_DIR, \"file_level_predictions.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[INFO] 保存: file_level_predictions.csv\")\n",
    "\n",
    "print(\"\\n[Done] 検証が完了しました。出力は:\", OUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
