{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 110,
=======
   "execution_count": 80,
>>>>>>> parent of e433e32 (cnn追加、正規化軌跡出力、csv前処理追加)
   "id": "56f9794f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.19.0\n",
      "[INFO] Project root: c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\n",
      "[INFO] Train CSV dir: c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train_csv\n",
      "[INFO] 使用キーポイント（CSV実名）: ['left_tarsal', 'left_paw']\n",
      "X: (1211, 30, 4) y: (1211,) unique files: 110\n",
      "class_weight: {0: 1.4183673469387754, 1: 0.7722222222222223}\n",
      "Epoch 1/50\n",
      "\u001b[1m94/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5052 - loss: 0.7013\n",
      "Epoch 1: val_accuracy improved from -inf to 0.54202, saving model to c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train1_model\\ivdd_lstm_20260108-140344_best.keras\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5048 - loss: 0.7011 - val_accuracy: 0.5420 - val_loss: 0.6916 - learning_rate: 7.0000e-05\n",
      "Epoch 2/50\n",
      "\u001b[1m94/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5260 - loss: 0.6963\n",
      "Epoch 2: val_accuracy improved from 0.54202 to 0.57563, saving model to c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train1_model\\ivdd_lstm_20260108-140344_best.keras\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5264 - loss: 0.6961 - val_accuracy: 0.5756 - val_loss: 0.6890 - learning_rate: 7.0000e-05\n",
      "Epoch 3/50\n",
      "\u001b[1m95/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5584 - loss: 0.6939\n",
      "Epoch 3: val_accuracy did not improve from 0.57563\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5586 - loss: 0.6937 - val_accuracy: 0.5630 - val_loss: 0.6867 - learning_rate: 7.0000e-05\n",
      "Epoch 4/50\n",
      "\u001b[1m95/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6082 - loss: 0.6916\n",
      "Epoch 4: val_accuracy did not improve from 0.57563\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6074 - loss: 0.6914 - val_accuracy: 0.5714 - val_loss: 0.6841 - learning_rate: 7.0000e-05\n",
      "Epoch 5/50\n",
      "\u001b[1m91/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6207 - loss: 0.6890\n",
      "Epoch 5: val_accuracy did not improve from 0.57563\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6191 - loss: 0.6886 - val_accuracy: 0.5294 - val_loss: 0.6808 - learning_rate: 7.0000e-05\n",
      "Epoch 6/50\n",
      "\u001b[1m97/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6073 - loss: 0.6850\n",
      "Epoch 6: val_accuracy did not improve from 0.57563\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6071 - loss: 0.6849 - val_accuracy: 0.5588 - val_loss: 0.6761 - learning_rate: 7.0000e-05\n",
      "Epoch 7/50\n",
      "\u001b[1m86/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6038 - loss: 0.6805\n",
      "Epoch 7: val_accuracy improved from 0.57563 to 0.57983, saving model to c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train1_model\\ivdd_lstm_20260108-140344_best.keras\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6037 - loss: 0.6798 - val_accuracy: 0.5798 - val_loss: 0.6693 - learning_rate: 7.0000e-05\n",
      "Epoch 8/50\n",
      "\u001b[1m87/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5974 - loss: 0.6730\n",
      "Epoch 8: val_accuracy improved from 0.57983 to 0.59664, saving model to c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train1_model\\ivdd_lstm_20260108-140344_best.keras\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5983 - loss: 0.6723 - val_accuracy: 0.5966 - val_loss: 0.6590 - learning_rate: 7.0000e-05\n",
      "Epoch 9/50\n",
      "\u001b[1m86/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5992 - loss: 0.6622\n",
      "Epoch 9: val_accuracy improved from 0.59664 to 0.60504, saving model to c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train1_model\\ivdd_lstm_20260108-140344_best.keras\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6013 - loss: 0.6612 - val_accuracy: 0.6050 - val_loss: 0.6442 - learning_rate: 7.0000e-05\n",
      "Epoch 10/50\n",
      "\u001b[1m87/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6091 - loss: 0.6473\n",
      "Epoch 10: val_accuracy improved from 0.60504 to 0.61765, saving model to c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train1_model\\ivdd_lstm_20260108-140344_best.keras\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6117 - loss: 0.6462 - val_accuracy: 0.6176 - val_loss: 0.6269 - learning_rate: 7.0000e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m89/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6426 - loss: 0.6309\n",
      "Epoch 11: val_accuracy improved from 0.61765 to 0.64286, saving model to c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train1_model\\ivdd_lstm_20260108-140344_best.keras\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6437 - loss: 0.6297 - val_accuracy: 0.6429 - val_loss: 0.6113 - learning_rate: 7.0000e-05\n",
      "Epoch 12/50\n",
      "\u001b[1m87/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6647 - loss: 0.6158\n",
      "Epoch 12: val_accuracy improved from 0.64286 to 0.65546, saving model to c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train1_model\\ivdd_lstm_20260108-140344_best.keras\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6650 - loss: 0.6143 - val_accuracy: 0.6555 - val_loss: 0.5993 - learning_rate: 7.0000e-05\n",
      "Epoch 13/50\n",
      "\u001b[1m94/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6672 - loss: 0.6021\n",
      "Epoch 13: val_accuracy did not improve from 0.65546\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6675 - loss: 0.6014 - val_accuracy: 0.6471 - val_loss: 0.5906 - learning_rate: 7.0000e-05\n",
      "Epoch 14/50\n",
      "\u001b[1m90/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6763 - loss: 0.5924\n",
      "Epoch 14: val_accuracy did not improve from 0.65546\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6771 - loss: 0.5910 - val_accuracy: 0.6345 - val_loss: 0.5843 - learning_rate: 7.0000e-05\n",
      "Epoch 15/50\n",
      "\u001b[1m86/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6737 - loss: 0.5844\n",
      "Epoch 15: val_accuracy did not improve from 0.65546\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6755 - loss: 0.5823 - val_accuracy: 0.6471 - val_loss: 0.5792 - learning_rate: 7.0000e-05\n",
      "Epoch 16/50\n",
      "\u001b[1m93/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6855 - loss: 0.5757\n",
      "Epoch 16: val_accuracy improved from 0.65546 to 0.66387, saving model to c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train1_model\\ivdd_lstm_20260108-140344_best.keras\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6862 - loss: 0.5747 - val_accuracy: 0.6639 - val_loss: 0.5749 - learning_rate: 7.0000e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m89/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6957 - loss: 0.5693\n",
      "Epoch 17: val_accuracy did not improve from 0.66387\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6963 - loss: 0.5677 - val_accuracy: 0.6639 - val_loss: 0.5710 - learning_rate: 7.0000e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m90/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7000 - loss: 0.5626\n",
      "Epoch 18: val_accuracy did not improve from 0.66387\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7005 - loss: 0.5613 - val_accuracy: 0.6555 - val_loss: 0.5676 - learning_rate: 7.0000e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m89/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6953 - loss: 0.5566\n",
      "Epoch 19: val_accuracy did not improve from 0.66387\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6961 - loss: 0.5552 - val_accuracy: 0.6555 - val_loss: 0.5645 - learning_rate: 7.0000e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m90/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7005 - loss: 0.5507\n",
      "Epoch 20: val_accuracy did not improve from 0.66387\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7015 - loss: 0.5494 - val_accuracy: 0.6597 - val_loss: 0.5617 - learning_rate: 7.0000e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m93/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7045 - loss: 0.5448\n",
      "Epoch 21: val_accuracy did not improve from 0.66387\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7053 - loss: 0.5439 - val_accuracy: 0.6597 - val_loss: 0.5594 - learning_rate: 7.0000e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m87/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7129 - loss: 0.5404\n",
      "Epoch 22: val_accuracy did not improve from 0.66387\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7139 - loss: 0.5386 - val_accuracy: 0.6639 - val_loss: 0.5575 - learning_rate: 7.0000e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m95/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7194 - loss: 0.5343\n",
      "Epoch 23: val_accuracy improved from 0.66387 to 0.66807, saving model to c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train1_model\\ivdd_lstm_20260108-140344_best.keras\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7198 - loss: 0.5336 - val_accuracy: 0.6681 - val_loss: 0.5561 - learning_rate: 7.0000e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m92/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7275 - loss: 0.5300\n",
      "Epoch 24: val_accuracy improved from 0.66807 to 0.68067, saving model to c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train1_model\\ivdd_lstm_20260108-140344_best.keras\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7282 - loss: 0.5289 - val_accuracy: 0.6807 - val_loss: 0.5552 - learning_rate: 7.0000e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m88/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7236 - loss: 0.5261\n",
      "Epoch 25: val_accuracy improved from 0.68067 to 0.68487, saving model to c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train1_model\\ivdd_lstm_20260108-140344_best.keras\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7250 - loss: 0.5244 - val_accuracy: 0.6849 - val_loss: 0.5547 - learning_rate: 7.0000e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m97/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7322 - loss: 0.5205\n",
      "Epoch 26: val_accuracy improved from 0.68487 to 0.69748, saving model to c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train1_model\\ivdd_lstm_20260108-140344_best.keras\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7323 - loss: 0.5202 - val_accuracy: 0.6975 - val_loss: 0.5543 - learning_rate: 7.0000e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m88/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7266 - loss: 0.5180\n",
      "Epoch 27: val_accuracy improved from 0.69748 to 0.70588, saving model to c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train1_model\\ivdd_lstm_20260108-140344_best.keras\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7282 - loss: 0.5162 - val_accuracy: 0.7059 - val_loss: 0.5540 - learning_rate: 7.0000e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7341 - loss: 0.5126\n",
      "Epoch 28: val_accuracy improved from 0.70588 to 0.71849, saving model to c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train1_model\\ivdd_lstm_20260108-140344_best.keras\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7342 - loss: 0.5125 - val_accuracy: 0.7185 - val_loss: 0.5537 - learning_rate: 7.0000e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m91/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7376 - loss: 0.5104\n",
      "Epoch 29: val_accuracy did not improve from 0.71849\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7387 - loss: 0.5089 - val_accuracy: 0.7185 - val_loss: 0.5533 - learning_rate: 7.0000e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m91/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7332 - loss: 0.5070\n",
      "Epoch 30: val_accuracy did not improve from 0.71849\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7349 - loss: 0.5055 - val_accuracy: 0.7185 - val_loss: 0.5527 - learning_rate: 7.0000e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7370 - loss: 0.5024\n",
      "Epoch 31: val_accuracy improved from 0.71849 to 0.72689, saving model to c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train1_model\\ivdd_lstm_20260108-140344_best.keras\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7372 - loss: 0.5022 - val_accuracy: 0.7269 - val_loss: 0.5519 - learning_rate: 7.0000e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m90/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7360 - loss: 0.5007\n",
      "Epoch 32: val_accuracy improved from 0.72689 to 0.73109, saving model to c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train1_model\\ivdd_lstm_20260108-140344_best.keras\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7381 - loss: 0.4990 - val_accuracy: 0.7311 - val_loss: 0.5512 - learning_rate: 7.0000e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m89/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7395 - loss: 0.4976\n",
      "Epoch 33: val_accuracy did not improve from 0.73109\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7418 - loss: 0.4957 - val_accuracy: 0.7269 - val_loss: 0.5506 - learning_rate: 7.0000e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m94/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7409 - loss: 0.4933\n",
      "Epoch 34: val_accuracy did not improve from 0.73109\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7420 - loss: 0.4923 - val_accuracy: 0.7311 - val_loss: 0.5501 - learning_rate: 7.0000e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7495 - loss: 0.4892\n",
      "Epoch 35: val_accuracy improved from 0.73109 to 0.73529, saving model to c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train1_model\\ivdd_lstm_20260108-140344_best.keras\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7497 - loss: 0.4890 - val_accuracy: 0.7353 - val_loss: 0.5495 - learning_rate: 7.0000e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7524 - loss: 0.4859\n",
      "Epoch 36: val_accuracy improved from 0.73529 to 0.73950, saving model to c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train1_model\\ivdd_lstm_20260108-140344_best.keras\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7525 - loss: 0.4857 - val_accuracy: 0.7395 - val_loss: 0.5488 - learning_rate: 7.0000e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7538 - loss: 0.4827\n",
      "Epoch 37: val_accuracy did not improve from 0.73950\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7539 - loss: 0.4825 - val_accuracy: 0.7353 - val_loss: 0.5482 - learning_rate: 7.0000e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m94/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7541 - loss: 0.4804\n",
      "Epoch 38: val_accuracy did not improve from 0.73950\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7548 - loss: 0.4794 - val_accuracy: 0.7395 - val_loss: 0.5477 - learning_rate: 7.0000e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m91/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7626 - loss: 0.4778\n",
      "Epoch 39: val_accuracy improved from 0.73950 to 0.74370, saving model to c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train1_model\\ivdd_lstm_20260108-140344_best.keras\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7635 - loss: 0.4764 - val_accuracy: 0.7437 - val_loss: 0.5472 - learning_rate: 7.0000e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m87/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7644 - loss: 0.4756\n",
      "Epoch 40: val_accuracy did not improve from 0.74370\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7655 - loss: 0.4735 - val_accuracy: 0.7437 - val_loss: 0.5467 - learning_rate: 7.0000e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m93/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7728 - loss: 0.4717\n",
      "Epoch 41: val_accuracy did not improve from 0.74370\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7731 - loss: 0.4707 - val_accuracy: 0.7437 - val_loss: 0.5462 - learning_rate: 7.0000e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m87/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7761 - loss: 0.4699\n",
      "Epoch 42: val_accuracy improved from 0.74370 to 0.74790, saving model to c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train1_model\\ivdd_lstm_20260108-140344_best.keras\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7764 - loss: 0.4679 - val_accuracy: 0.7479 - val_loss: 0.5458 - learning_rate: 7.0000e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m84/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7740 - loss: 0.4680\n",
      "Epoch 43: val_accuracy did not improve from 0.74790\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7746 - loss: 0.4652 - val_accuracy: 0.7479 - val_loss: 0.5454 - learning_rate: 7.0000e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m92/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7743 - loss: 0.4638\n",
      "Epoch 44: val_accuracy did not improve from 0.74790\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7747 - loss: 0.4626 - val_accuracy: 0.7479 - val_loss: 0.5450 - learning_rate: 7.0000e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m89/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7782 - loss: 0.4617\n",
      "Epoch 45: val_accuracy did not improve from 0.74790\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7786 - loss: 0.4601 - val_accuracy: 0.7479 - val_loss: 0.5447 - learning_rate: 7.0000e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m92/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7778 - loss: 0.4588\n",
      "Epoch 46: val_accuracy did not improve from 0.74790\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7780 - loss: 0.4577 - val_accuracy: 0.7479 - val_loss: 0.5444 - learning_rate: 7.0000e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m92/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7772 - loss: 0.4565\n",
      "Epoch 47: val_accuracy did not improve from 0.74790\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7776 - loss: 0.4554 - val_accuracy: 0.7479 - val_loss: 0.5443 - learning_rate: 7.0000e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m87/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7784 - loss: 0.4550\n",
      "Epoch 48: val_accuracy improved from 0.74790 to 0.75210, saving model to c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train1_model\\ivdd_lstm_20260108-140344_best.keras\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7791 - loss: 0.4531 - val_accuracy: 0.7521 - val_loss: 0.5445 - learning_rate: 7.0000e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m93/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7791 - loss: 0.4517\n",
      "Epoch 49: val_accuracy improved from 0.75210 to 0.75630, saving model to c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train1_model\\ivdd_lstm_20260108-140344_best.keras\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7795 - loss: 0.4508 - val_accuracy: 0.7563 - val_loss: 0.5451 - learning_rate: 7.0000e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m88/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7826 - loss: 0.4500\n",
      "Epoch 50: val_accuracy did not improve from 0.75630\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7833 - loss: 0.4484 - val_accuracy: 0.7563 - val_loss: 0.5457 - learning_rate: 7.0000e-05\n",
      "[INFO] 学習曲線を保存: c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\fig\\curve_20260108-140344.png\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "[INFO] 検証ウィンドウ全件を保存: c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\val_misclassified\\val_windows_20260108-140344.csv\n",
      "[INFO] 学習ログを保存: c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\logs\\train_log_20260108-140344.txt\n",
      "[INFO] Best model : c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train1_model\\ivdd_lstm_20260108-140344_best.keras\n",
      "[INFO] Final model: c:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train1_model\\ivdd_lstm_20260108-140344_final.keras\n",
      "[DONE] Training complete.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "IVDD binary classification (ivdd vs normal) from DeepLabCut CSV (3-level header)\n",
    "\n",
    "本スクリプトは以下を満たします：\n",
    "- one / two / ivdd → 「ivdd」扱い、normal → 「normal」扱い（ファイル名のトークンから推定）\n",
    "- キーポイント: tail_set, right_tarsal, right_paw, left_tarsal, left_paw（各(x,y)で10次元）\n",
    "- 前処理: z-score（ファイル単位）\n",
    "- ウィンドウ: SEQ_LEN=60, STRIDE=30\n",
    "- モデル: TimeDistributed(Dense→ReLU) → LSTM → LSTM → Dense(2 logits, from_logits=True)\n",
    "- 学習率固定 + ReduceLROnPlateau（EarlyStoppingなし）\n",
    "- 出力先: data/train/{train_csv, fig, train1_model, val_misclassified, logs}\n",
    "  - 学習曲線（loss & accuracy 1枚）\n",
    "  - best/final モデルを日時付きで保存\n",
    "  - バリデーションで使った「全ウィンドウ」をCSV保存（正解/誤分類フラグ付き）\n",
    "  - 学習設定・使用フレーム数・各epochのloss/accuracy・最終のclassification_reportをtxt保存\n",
    "\"\"\"\n",
    "\n",
    "# ====== 安定運用（必要なら） ======\n",
    "import os\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"-1\")\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "\n",
    "import re, glob, json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ========= プロジェクトルート検出 =========\n",
    "def detect_project_root() -> Path:\n",
    "    # 1) 環境変数優先\n",
    "    env = os.environ.get(\"IVDD_PROJECT_ROOT\")\n",
    "    if env:\n",
    "        p = Path(env).expanduser().resolve()\n",
    "        if (p / \"data\").is_dir() or (p / \"scripts\").is_dir():\n",
    "            return p\n",
    "\n",
    "    # 2) __file__ から（.pyで実行時）\n",
    "    try:\n",
    "        here = Path(__file__).resolve()\n",
    "        if here.parent.name == \"scripts\":\n",
    "            cand = here.parent.parent\n",
    "            if (cand / \"data\").is_dir() or (cand / \"scripts\").is_dir():\n",
    "                return cand\n",
    "        if (here.parent / \"data\").is_dir() or (here.parent / \"scripts\").is_dir():\n",
    "            return here.parent\n",
    "    except NameError:\n",
    "        pass  # Notebook では __file__ がない\n",
    "\n",
    "    # 3) CWD から上に辿る\n",
    "    cwd = Path.cwd()\n",
    "    for cand in [cwd] + list(cwd.parents):\n",
    "        if (cand / \"data\").is_dir() and (cand / \"scripts\").is_dir():\n",
    "            return cand\n",
    "\n",
    "    # 4) 最後の手段\n",
    "    return cwd.parent if cwd.name == \"scripts\" else cwd\n",
    "\n",
    "PROJ_ROOT     = detect_project_root()\n",
    "TRAIN_DIR     = PROJ_ROOT / \"data\" / \"train\"\n",
    "TRAIN_CSV_DIR = TRAIN_DIR / \"train_csv\"\n",
    "FIG_DIR       = TRAIN_DIR / \"fig\"\n",
    "MODEL_DIR     = TRAIN_DIR / \"train1_model\"\n",
    "VAL_DIR       = TRAIN_DIR / \"val_misclassified\"   # ここに「全ウィンドウ」の検証結果を保存\n",
    "LOG_DIR       = TRAIN_DIR / \"logs\"\n",
    "for d in [TRAIN_CSV_DIR, FIG_DIR, MODEL_DIR, VAL_DIR, LOG_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[INFO] Project root: {PROJ_ROOT}\")\n",
    "print(f\"[INFO] Train CSV dir: {TRAIN_CSV_DIR}\")\n",
    "\n",
    "# ========= 設定 =========\n",
    "KEYPOINTS = [\n",
    "    #\"tail_set\",\n",
    "    # \"right_tarsal\",\n",
    "    # \"right_paw\",\n",
    "    \"left_tarsal\",\n",
    "    \"left_paw\",\n",
    "]\n",
    "USE_LIKELIHOOD      = False\n",
    "MIN_KEEP_LIKELIHOOD = 0.6\n",
    "\n",
    "SEQ_LEN   = 30\n",
    "STRIDE    = 15\n",
    "DIMS      = len(KEYPOINTS) * 2\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS     = 50\n",
    "LR         = 7e-5\n",
    "VAL_SPLIT_BY_FILE = True\n",
    "\n",
    "# 2クラス（2 logits, softmax）\n",
    "CLASS_NAMES  = [\"ivdd\", \"normal\"]   # index 0: ivdd, 1: normal\n",
    "CLASS_TO_IDX = {c:i for i,c in enumerate(CLASS_NAMES)}\n",
    "N_CLASSES    = 2\n",
    "\n",
    "# 保存ファイル名に使う日時\n",
    "DATE_STR = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# ========= ユーティリティ =========\n",
    "def _norm_name(s: str) -> str:\n",
    "    return \"\".join(ch for ch in s.lower() if ch not in \" _-\")\n",
    "\n",
    "def _resolve_keypoints(all_bodyparts, requested):\n",
    "    \"\"\"\n",
    "    DLC 3段ヘッダの bodyparts（実名）と、要求名（大文字小文字/空白/ハイフン無視）を突合。\n",
    "    戻り値: 実際に使われる実名リスト\n",
    "    \"\"\"\n",
    "    norm2orig = {}\n",
    "    for bp in all_bodyparts:\n",
    "        k = _norm_name(bp)\n",
    "        if k not in norm2orig:\n",
    "            norm2orig[k] = bp\n",
    "    resolved, missing = [], []\n",
    "    for req in requested:\n",
    "        k = _norm_name(req)\n",
    "        if k in norm2orig:\n",
    "            resolved.append(norm2orig[k])\n",
    "        else:\n",
    "            missing.append(req)\n",
    "    if missing:\n",
    "        raise ValueError(f\"指定キーポイントがCSVで見つかりません: {missing}\\n利用可能: {all_bodyparts}\")\n",
    "    return resolved\n",
    "\n",
    "# ===== ラベル推定（one/two/ivdd → ivdd, normal → normal）=====\n",
    "IVDD_ALIASES   = {\"ivdd\", \"one\", \"two\"}\n",
    "NORMAL_ALIASES = {\"normal\"}\n",
    "\n",
    "def infer_label_from_filename(path: str) -> int:\n",
    "    \"\"\"\n",
    "    ルール（上から順に適用）:\n",
    "      1) ファイル名（拡張子除く）を非英数字で分割し、トークン集合に\n",
    "         - IVDD_ALIASES いずれかが含まれ、かつ NORMAL_ALIASES が含まれない → ivdd\n",
    "         - NORMAL_ALIASES が含まれ、かつ IVDD_ALIASES が含まれない → normal\n",
    "      2) 1)で未決なら、先頭トークンが上記のどれかなら採用\n",
    "      3) なお未決なら、親ディレクトリのトークンで同様に判定（片側のみヒット）\n",
    "      4) それでも未決なら ValueError\n",
    "    \"\"\"\n",
    "    name = os.path.basename(path).lower()\n",
    "    stem = os.path.splitext(name)[0]\n",
    "    tokens = [t for t in re.split(r'[^a-z0-9]+', stem) if t]\n",
    "    token_set = set(tokens)\n",
    "\n",
    "    has_ivdd   = any(t in IVDD_ALIASES   for t in token_set)\n",
    "    has_normal = any(t in NORMAL_ALIASES for t in token_set)\n",
    "\n",
    "    if has_ivdd and not has_normal:\n",
    "        return CLASS_TO_IDX[\"ivdd\"]\n",
    "    if has_normal and not has_ivdd:\n",
    "        return CLASS_TO_IDX[\"normal\"]\n",
    "\n",
    "    if tokens:\n",
    "        if tokens[0] in IVDD_ALIASES:   return CLASS_TO_IDX[\"ivdd\"]\n",
    "        if tokens[0] in NORMAL_ALIASES: return CLASS_TO_IDX[\"normal\"]\n",
    "\n",
    "    parent_tokens = [t for t in re.split(r'[^a-z0-9]+', os.path.dirname(path).lower()) if t]\n",
    "    pset = set(parent_tokens)\n",
    "    p_has_ivdd   = any(t in IVDD_ALIASES   for t in pset)\n",
    "    p_has_normal = any(t in NORMAL_ALIASES for t in pset)\n",
    "    if p_has_ivdd and not p_has_normal: return CLASS_TO_IDX[\"ivdd\"]\n",
    "    if p_has_normal and not p_has_ivdd: return CLASS_TO_IDX[\"normal\"]\n",
    "\n",
    "    raise ValueError(f\"ラベル不明: {name}（推奨: 先頭を one_/two_/ivdd_/normal_）\")\n",
    "\n",
    "def read_dlc_xy(csv_path: str,\n",
    "                keypoints,\n",
    "                use_likelihood=False,\n",
    "                min_keep_likelihood=0.6):\n",
    "    \"\"\"\n",
    "    DLC 3段ヘッダCSVを読み込み、指定KPの (x,y) のみ抽出して (T, 2*len(KP)) を返す。\n",
    "    低likelihoodはNaN→線形補間→前後補完→0埋め。\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, header=[0,1,2], index_col=0)\n",
    "    bodyparts = list({bp for (_, bp, _) in df.columns})\n",
    "    use_kps = _resolve_keypoints(bodyparts, keypoints)\n",
    "\n",
    "    cols = {}\n",
    "    for bp in use_kps:\n",
    "        cols[f\"{bp}_x\"] = df.xs((bp, \"x\"), level=[1,2], axis=1)\n",
    "        cols[f\"{bp}_y\"] = df.xs((bp, \"y\"), level=[1,2], axis=1)\n",
    "    X_df = pd.concat(cols.values(), axis=1)\n",
    "    X_df.columns = list(cols.keys())\n",
    "\n",
    "    if use_likelihood:\n",
    "        for bp in use_kps:\n",
    "            try:\n",
    "                lcol = df.xs((bp, \"likelihood\"), level=[1,2], axis=1).values.flatten()\n",
    "                low = lcol < min_keep_likelihood\n",
    "                for c in [f\"{bp}_x\", f\"{bp}_y\"]:\n",
    "                    vals = X_df[c].values\n",
    "                    vals[low] = np.nan\n",
    "                    X_df[c] = vals\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "    X_df = X_df.interpolate(method=\"linear\", limit_direction=\"both\", axis=0)\n",
    "    X_df = X_df.bfill().ffill().fillna(0.0)\n",
    "    return X_df.values.astype(np.float32), use_kps  # (T, D), 実名KPリスト\n",
    "\n",
    "def zscore_per_file(X: np.ndarray, eps: float=1e-6) -> np.ndarray:\n",
    "    mu = X.mean(axis=0, keepdims=True)\n",
    "    sd = X.std(axis=0, keepdims=True)\n",
    "    return (X - mu) / (sd + eps)\n",
    "\n",
    "def make_windows(X: np.ndarray, seq_len: int, stride: int):\n",
    "    n = X.shape[0]\n",
    "    if n < seq_len:\n",
    "        return np.empty((0, seq_len, X.shape[1]), dtype=X.dtype), []\n",
    "    starts = list(range(0, n - seq_len + 1, stride))\n",
    "    Xw = np.stack([X[s:s+seq_len] for s in starts], axis=0)\n",
    "    return Xw, starts\n",
    "\n",
    "def build_dataset(csv_paths, seq_len=SEQ_LEN, stride=STRIDE):\n",
    "    X_list, y_list, file_ids, starts_list = [], [], [], []\n",
    "    used_kps_any = None\n",
    "\n",
    "    for p in csv_paths:\n",
    "        y_lab = infer_label_from_filename(p)\n",
    "        X_raw, used_kps = read_dlc_xy(\n",
    "            p, keypoints=KEYPOINTS,\n",
    "            use_likelihood=USE_LIKELIHOOD,\n",
    "            min_keep_likelihood=MIN_KEEP_LIKELIHOOD\n",
    "        )\n",
    "        if X_raw.shape[1] != DIMS:\n",
    "            raise ValueError(f\"{os.path.basename(p)}: 次元 {X_raw.shape[1]} != 期待 {DIMS}\")\n",
    "\n",
    "        X_norm = zscore_per_file(X_raw)\n",
    "        X_win, starts = make_windows(X_norm, seq_len, stride)  # (M,T,D), [M]\n",
    "        if X_win.shape[0] == 0:\n",
    "            print(f\"[WARN] {os.path.basename(p)}: フレーム不足（{seq_len}未満）でスキップ\")\n",
    "            continue\n",
    "\n",
    "        X_list.append(X_win)\n",
    "        y_list.append(np.full((X_win.shape[0],), y_lab, dtype=np.int64))\n",
    "        file_ids += [os.path.basename(p)] * X_win.shape[0]\n",
    "        starts_list += starts\n",
    "        used_kps_any = used_kps\n",
    "\n",
    "    if not X_list:\n",
    "        raise RuntimeError(\"データが作れませんでした。CSVと命名規則（one/two/ivdd/normal）を確認してください。\")\n",
    "\n",
    "    X = np.concatenate(X_list, axis=0)\n",
    "    y = np.concatenate(y_list, axis=0)\n",
    "    file_ids = np.array(file_ids)\n",
    "    starts_arr = np.array(starts_list)\n",
    "    print(f\"[INFO] 使用キーポイント（CSV実名）: {used_kps_any}\")\n",
    "    return X, y, file_ids, starts_arr\n",
    "\n",
    "# ========= モデル =========\n",
    "class LSTM_RNN(keras.Model):\n",
    "    def __init__(self, n_input, n_hidden, n_classes):\n",
    "        super().__init__()\n",
    "        self.input_dense = keras.layers.Dense(n_hidden, activation='relu')\n",
    "        self.time_dist   = keras.layers.TimeDistributed(self.input_dense)\n",
    "        self.lstm1 = keras.layers.LSTM(n_hidden, return_sequences=True)\n",
    "        self.lstm2 = keras.layers.LSTM(n_hidden)\n",
    "        self.out   = keras.layers.Dense(n_classes)  # logits\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.time_dist(x)\n",
    "        x = self.lstm1(x, training=training)\n",
    "        x = self.lstm2(x, training=training)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class LSTMWithL2(LSTM_RNN):\n",
<<<<<<< HEAD
    "    def __init__(self, n_input, n_hidden, n_classes, l2_lambda=1e-6):\n",
=======
    "    def __init__(self, n_input, n_hidden, n_classes, l2_lambda=1e-4):\n",
>>>>>>> parent of e433e32 (cnn追加、正規化軌跡出力、csv前処理追加)
    "        super().__init__(n_input, n_hidden, n_classes)\n",
    "        self.l2_lambda   = l2_lambda\n",
    "        self.loss_fn     = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        self.metric_acc  = keras.metrics.CategoricalAccuracy(name=\"accuracy\")\n",
    "        self.metric_loss = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.metric_loss, self.metric_acc]\n",
    "\n",
    "    def compile(self, optimizer, **kwargs):\n",
    "        super().compile(optimizer=optimizer, **kwargs)\n",
    "\n",
    "    def _l2(self):\n",
    "        if not self.trainable_variables:\n",
    "            return 0.0\n",
    "        return self.l2_lambda * tf.add_n([tf.nn.l2_loss(v) for v in self.trainable_variables])\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, (list, tuple)) and len(data) == 3:\n",
    "            x, y, sample_weight = data\n",
    "        else:\n",
    "            x, y = data\n",
    "            sample_weight = None\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self(x, training=True)\n",
    "            loss = self.loss_fn(y, logits, sample_weight=sample_weight) + self._l2()\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        self.metric_loss.update_state(loss)\n",
    "        self.metric_acc.update_state(y, logits, sample_weight=sample_weight)\n",
    "        return {\"loss\": self.metric_loss.result(), \"accuracy\": self.metric_acc.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        if isinstance(data, (list, tuple)) and len(data) == 3:\n",
    "            x, y, sample_weight = data\n",
    "        else:\n",
    "            x, y = data\n",
    "            sample_weight = None\n",
    "        logits = self(x, training=False)\n",
    "        loss = self.loss_fn(y, logits, sample_weight=sample_weight) + self._l2()\n",
    "        self.metric_loss.update_state(loss)\n",
    "        self.metric_acc.update_state(y, logits, sample_weight=sample_weight)\n",
    "        return {\"loss\": self.metric_loss.result(), \"accuracy\": self.metric_acc.result()}\n",
    "\n",
    "# ========= データ読み込み =========\n",
    "CSV_GLOB = str(TRAIN_CSV_DIR / \"*.csv\")\n",
    "csv_files = sorted(glob.glob(CSV_GLOB))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"学習CSVが見つかりません: {CSV_GLOB}\")\n",
    "\n",
    "X, y, file_ids, starts = build_dataset(csv_files, SEQ_LEN, STRIDE)\n",
    "print(\"X:\", X.shape, \"y:\", y.shape, \"unique files:\", len(np.unique(file_ids)))\n",
    "\n",
    "# one-hot\n",
    "y_oh = keras.utils.to_categorical(y, num_classes=N_CLASSES)\n",
    "\n",
    "# ========= 分割 =========\n",
    "if VAL_SPLIT_BY_FILE:\n",
    "    uniq = np.unique(file_ids)\n",
    "    tr_files, va_files = train_test_split(uniq, test_size=0.2, random_state=42, shuffle=True)\n",
    "    tr_mask = np.isin(file_ids, tr_files)\n",
    "    va_mask = np.isin(file_ids, va_files)\n",
    "    X_train, y_train = X[tr_mask], y_oh[tr_mask]\n",
    "    X_val,   y_val   = X[va_mask], y_oh[va_mask]\n",
    "    val_file_names = file_ids[va_mask]\n",
    "    val_starts     = starts[va_mask]\n",
    "else:\n",
    "    X_train, X_val, y_train, y_val, idx_train, idx_val = train_test_split(\n",
    "        X, y_oh, np.arange(len(X)), test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    val_file_names = file_ids[idx_val]\n",
    "    val_starts     = starts[idx_val]\n",
    "\n",
    "# ========= クラス重み =========\n",
    "cls_w = compute_class_weight(\"balanced\", classes=np.arange(N_CLASSES), y=np.argmax(y_train, axis=1))\n",
    "class_weight = {int(c): float(w) for c, w in enumerate(cls_w)}\n",
    "print(\"class_weight:\", class_weight)\n",
    "\n",
    "# ========= モデル準備・学習 =========\n",
    "n_hidden = 30\n",
<<<<<<< HEAD
    "model = LSTMWithL2(n_input=DIMS, n_hidden=n_hidden, n_classes=N_CLASSES, l2_lambda=1e-6)\n",
=======
    "model = LSTMWithL2(n_input=DIMS, n_hidden=n_hidden, n_classes=N_CLASSES, l2_lambda=1e-4)\n",
>>>>>>> parent of e433e32 (cnn追加、正規化軌跡出力、csv前処理追加)
    "opt = keras.optimizers.Adam(learning_rate=LR)\n",
    "model.compile(optimizer=opt)\n",
    "\n",
    "best_path  = str(MODEL_DIR / f\"ivdd_lstm_{DATE_STR}_best.keras\")\n",
    "final_path = str(MODEL_DIR / f\"ivdd_lstm_{DATE_STR}_final.keras\")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        best_path, monitor=\"val_accuracy\", mode=\"max\",\n",
    "        save_best_only=True, verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", mode=\"min\",\n",
    "        factor=0.5, patience=5, min_lr=1e-6, verbose=1\n",
    "    ),\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_weight=class_weight,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# ========= 学習曲線（1枚） =========\n",
    "fig_path = str(FIG_DIR / f\"curve_{DATE_STR}.png\")\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history[\"loss\"], label=\"train\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val\")\n",
    "plt.title(\"Loss\"); plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history[\"accuracy\"], label=\"train\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val\")\n",
    "plt.title(\"Accuracy\"); plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_path, dpi=150)\n",
    "plt.close()\n",
    "print(f\"[INFO] 学習曲線を保存: {fig_path}\")\n",
    "\n",
    "# ========= 検証セットでの推論（全ウィンドウを記録） =========\n",
    "logits_val = model.predict(X_val, batch_size=64)\n",
    "probs_val  = tf.nn.softmax(logits_val, axis=1).numpy()\n",
    "y_val_true = np.argmax(y_val, axis=1)\n",
    "y_val_pred = probs_val.argmax(axis=1)\n",
    "\n",
    "# ウィンドウごとのCSV（正解/誤分類を含む）を保存\n",
    "# 列: file, start, true, pred, correct, p_ivdd, p_normal\n",
    "df_val = pd.DataFrame({\n",
    "    \"file\":    val_file_names,\n",
    "    \"start\":   val_starts,\n",
    "    \"true\":    [CLASS_NAMES[i] for i in y_val_true],\n",
    "    \"pred\":    [CLASS_NAMES[i] for i in y_val_pred],\n",
    "    \"correct\": (y_val_true == y_val_pred),\n",
    "    \"p_ivdd\":  probs_val[:, CLASS_TO_IDX[\"ivdd\"]],\n",
    "    \"p_normal\":probs_val[:, CLASS_TO_IDX[\"normal\"]],\n",
    "}).sort_values([\"file\",\"start\"], kind=\"stable\")\n",
    "\n",
    "val_csv_path = str(VAL_DIR / f\"val_windows_{DATE_STR}.csv\")\n",
    "df_val.to_csv(val_csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[INFO] 検証ウィンドウ全件を保存: {val_csv_path}\")\n",
    "\n",
    "# ========= テキストログ（設定／使用フレーム数／各epoch／最終CR） =========\n",
    "date_str = DATE_STR  # 統一\n",
    "train_windows = int((y_train.shape[0]))\n",
    "val_windows   = int((y_val.shape[0]))\n",
    "train_frames  = int(train_windows * SEQ_LEN)\n",
    "val_frames    = int(val_windows   * SEQ_LEN)\n",
    "\n",
    "params_text = []\n",
    "params_text.append(\"[Training Parameters]\")\n",
    "params_text.append(f\"RUN_ID              : {date_str}\")\n",
    "params_text.append(f\"PROJECT_ROOT        : {PROJ_ROOT}\")\n",
    "params_text.append(f\"TRAIN_CSV_DIR       : {TRAIN_CSV_DIR}\")\n",
    "params_text.append(f\"MODEL_DIR           : {MODEL_DIR}\")\n",
    "params_text.append(f\"FIG_DIR             : {FIG_DIR}\")\n",
    "params_text.append(f\"VAL_DIR             : {VAL_DIR}\")\n",
    "params_text.append(f\"KEYPOINTS           : {KEYPOINTS}\")\n",
    "params_text.append(f\"USE_LIKELIHOOD      : {USE_LIKELIHOOD}\")\n",
    "params_text.append(f\"MIN_KEEP_LIKELIHOOD : {MIN_KEEP_LIKELIHOOD}\")\n",
    "params_text.append(f\"SEQ_LEN             : {SEQ_LEN}\")\n",
    "params_text.append(f\"STRIDE              : {STRIDE}\")\n",
    "params_text.append(f\"DIMS                : {DIMS}\")\n",
    "params_text.append(f\"N_HIDDEN            : {n_hidden}\")\n",
    "params_text.append(f\"BATCH_SIZE          : {BATCH_SIZE}\")\n",
    "params_text.append(f\"EPOCHS              : {EPOCHS}\")\n",
    "params_text.append(f\"LR                  : {LR}\")\n",
    "params_text.append(f\"VAL_SPLIT_BY_FILE   : {VAL_SPLIT_BY_FILE}\")\n",
    "params_text.append(f\"CLASS_NAMES         : {CLASS_NAMES}\")\n",
    "params_text.append(f\"class_weight        : {class_weight}\")\n",
    "\n",
    "summary_text = []\n",
    "summary_text.append(\"\\n[Data Usage Summary]\")\n",
    "summary_text.append(f\"train_windows: {train_windows}\")\n",
    "summary_text.append(f\"val_windows  : {val_windows}\")\n",
    "summary_text.append(f\"train_frames : {train_frames}\")\n",
    "summary_text.append(f\"val_frames   : {val_frames}\")\n",
    "\n",
    "hist = history.history\n",
    "num_epochs = len(hist.get(\"loss\", []))\n",
    "epochs_text = []\n",
    "epochs_text.append(\"\\n[Per-epoch Metrics]\")\n",
    "epochs_text.append(\"epoch,loss,accuracy,val_loss,val_accuracy\")\n",
    "for i in range(num_epochs):\n",
    "    tr_loss = hist.get(\"loss\",         [np.nan]*num_epochs)[i]\n",
    "    tr_acc  = hist.get(\"accuracy\",     [np.nan]*num_epochs)[i]\n",
    "    va_loss = hist.get(\"val_loss\",     [np.nan]*num_epochs)[i]\n",
    "    va_acc  = hist.get(\"val_accuracy\", [np.nan]*num_epochs)[i]\n",
    "    epochs_text.append(f\"{i+1},{tr_loss:.6f},{tr_acc:.6f},{va_loss:.6f},{va_acc:.6f}\")\n",
    "\n",
    "report_text = []\n",
    "report_text.append(\"\\n[Validation Classification Report (window-level)]\")\n",
    "report_text.append(classification_report(y_val_true, y_val_pred, target_names=CLASS_NAMES, digits=4))\n",
    "report_text.append(\"[Validation Confusion Matrix (window-level)]\")\n",
    "report_text.append(str(confusion_matrix(y_val_true, y_val_pred, labels=[0,1])))\n",
    "\n",
    "log_path = str(LOG_DIR / f\"train_log_{date_str}.txt\")\n",
    "with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(params_text))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"\\n\".join(summary_text))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"\\n\".join(epochs_text))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"\\n\".join(report_text))\n",
    "print(f\"[INFO] 学習ログを保存: {log_path}\")\n",
    "\n",
    "# ========= 最終モデル保存 =========\n",
    "model.save(final_path)\n",
    "print(f\"[INFO] Best model : {best_path}\")\n",
    "print(f\"[INFO] Final model: {final_path}\")\n",
    "print(\"[DONE] Training complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLya",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
