{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f9794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "IVDD binary classification (2-class: ivdd / normal) from DeepLabCut CSV (3-level header)\n",
    "- 5 keypoints × (x,y) = 10 dims を使用（likelihood 低品質点は補間）\n",
    "- Windowing: SEQ_LEN=30, STRIDE=5\n",
    "- Model: TimeDistributed(Dense->ReLU) -> LSTM -> LSTM -> Dense(2 logits)\n",
    "- 学習率は固定（Adam(LR)）。ReduceLROnPlateau で停滞時のみ減衰。\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ========= 設定 =========\n",
    "DATA_DIR   = r\"C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\ivdd\\train\"                 # CSV を置くフォルダ\n",
    "CSV_GLOB   = os.path.join(DATA_DIR, \"*.csv\")\n",
    "\n",
    "# ★ 指定の5点（ユーザー指定）\n",
    "KEYPOINTS = [\n",
    "    \"left back paw\",\n",
    "    \"right back paw\",\n",
    "    \"left front paw\",\n",
    "    \"right front paw\",\n",
    "    \"tail set\",\n",
    "]\n",
    "\n",
    "# 前処理系\n",
    "USE_LIKELIHOOD       = True\n",
    "MIN_KEEP_LIKELIHOOD  = 0.6\n",
    "\n",
    "# 窓切り\n",
    "SEQ_LEN   = 60\n",
    "STRIDE    = 30\n",
    "DIMS      = 10                 # 5点×(x,y)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS     = 50\n",
    "\n",
    "# 最適化\n",
    "LR = 1e-3                      # ★ 固定学習率（ReduceLROnPlateau と競合しない）\n",
    "VAL_SPLIT_BY_FILE = True       # ファイル単位でtrain/val分割（リーク防止）\n",
    "\n",
    "# 2クラス\n",
    "CLASS_NAMES  = [\"ivdd\", \"normal\"]\n",
    "CLASS_TO_IDX = {c:i for i,c in enumerate(CLASS_NAMES)}\n",
    "N_CLASSES    = 2\n",
    "\n",
    "# 再現性（任意）\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ========= ユーティリティ =========\n",
    "def _norm_name(s: str) -> str:\n",
    "    return \"\".join(ch for ch in s.lower() if ch not in \" _-\")\n",
    "\n",
    "def _resolve_keypoints(all_bodyparts, requested):\n",
    "    norm2orig = {}\n",
    "    for bp in all_bodyparts:\n",
    "        k = _norm_name(bp)\n",
    "        if k not in norm2orig:\n",
    "            norm2orig[k] = bp\n",
    "    resolved, missing = [], []\n",
    "    for req in requested:\n",
    "        k = _norm_name(req)\n",
    "        if k in norm2orig:\n",
    "            resolved.append(norm2orig[k])\n",
    "        else:\n",
    "            missing.append(req)\n",
    "    if missing:\n",
    "        raise ValueError(f\"指定キーポイントがCSVで見つかりません: {missing}\\n利用可能: {all_bodyparts}\")\n",
    "    return resolved\n",
    "\n",
    "def infer_label_from_filename(filename: str) -> int:\n",
    "    \"\"\"\n",
    "    'ivdd' / 'normal' を「単語」として判定。\n",
    "    例: 'IvddOct30' は無視、'ivdd_...' や '..._normal_' は一致。\n",
    "    \"\"\"\n",
    "    base = os.path.basename(filename).lower()\n",
    "    ivdd_match   = re.search(r'(?<![a-z])ivdd(?![a-z])', base, flags=re.I)\n",
    "    normal_match = re.search(r'(?<![a-z])normal(?![a-z])', base, flags=re.I)\n",
    "\n",
    "    if ivdd_match and not normal_match:\n",
    "        return CLASS_TO_IDX[\"ivdd\"]\n",
    "    if normal_match and not ivdd_match:\n",
    "        return CLASS_TO_IDX[\"normal\"]\n",
    "\n",
    "    # フォールバック：先頭トークン\n",
    "    token = re.split(r'[_\\-.]', base)[0]\n",
    "    if token in CLASS_TO_IDX:\n",
    "        return CLASS_TO_IDX[token]\n",
    "    raise ValueError(f\"ラベルを特定できません: {filename}（'ivdd' または 'normal' を単語として含めてください）\")\n",
    "\n",
    "def read_dlc_5kp_xy(csv_path: str,\n",
    "                    keypoints,\n",
    "                    use_likelihood=True,\n",
    "                    min_keep_likelihood=0.6):\n",
    "    \"\"\"\n",
    "    DLCの3段ヘッダCSVを読み込み、指定5点の (x,y) だけを抽出して (N,10) を返す。\n",
    "    likelihood < 閾値 の (x,y) は NaN → 線形補間。\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, header=[0,1,2], index_col=0)\n",
    "    bodyparts = list({bp for (_, bp, _) in df.columns})\n",
    "    use_kps = _resolve_keypoints(bodyparts, keypoints)\n",
    "\n",
    "    # (x,y) を抽出\n",
    "    cols = {}\n",
    "    for bp in use_kps:\n",
    "        cols[f\"{bp}_x\"] = df.xs((bp, \"x\"), level=[1,2], axis=1)\n",
    "        cols[f\"{bp}_y\"] = df.xs((bp, \"y\"), level=[1,2], axis=1)\n",
    "    X_df = pd.concat(cols.values(), axis=1)\n",
    "    X_df.columns = list(cols.keys())\n",
    "\n",
    "    # likelihood で品質管理\n",
    "    if use_likelihood:\n",
    "        for bp in use_kps:\n",
    "            try:\n",
    "                lcol = df.xs((bp, \"likelihood\"), level=[1,2], axis=1).values.flatten()\n",
    "                low = lcol < min_keep_likelihood\n",
    "                for c in [f\"{bp}_x\", f\"{bp}_y\"]:\n",
    "                    vals = X_df[c].values\n",
    "                    vals[low] = np.nan\n",
    "                    X_df[c] = vals\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "    # 線形補間→前後補完→0埋め\n",
    "    X_df = X_df.interpolate(method=\"linear\", limit_direction=\"both\", axis=0)\n",
    "    X_df = X_df.fillna(method=\"bfill\").fillna(method=\"ffill\").fillna(0.0)\n",
    "\n",
    "    X = X_df.values.astype(np.float32)          # (N, 10)\n",
    "    return X, use_kps\n",
    "\n",
    "def zscore_per_file(X: np.ndarray, eps: float=1e-6) -> np.ndarray:\n",
    "    mu = X.mean(axis=0, keepdims=True)\n",
    "    sd = X.std(axis=0, keepdims=True)\n",
    "    return (X - mu) / (sd + eps)\n",
    "\n",
    "def make_windows(X: np.ndarray, seq_len: int, stride: int) -> np.ndarray:\n",
    "    n = X.shape[0]\n",
    "    if n < seq_len:\n",
    "        return np.empty((0, seq_len, X.shape[1]), dtype=X.dtype)\n",
    "    starts = range(0, n - seq_len + 1, stride)\n",
    "    return np.stack([X[s:s+seq_len] for s in starts], axis=0)\n",
    "\n",
    "def build_dataset(csv_paths, seq_len=SEQ_LEN, stride=STRIDE):\n",
    "    X_list, y_list, file_ids = [], [], []\n",
    "    used_kps_any = None\n",
    "\n",
    "    for p in csv_paths:\n",
    "        y_lab = infer_label_from_filename(p)\n",
    "\n",
    "        X_raw, used_kps = read_dlc_5kp_xy(\n",
    "            p,\n",
    "            keypoints=KEYPOINTS,\n",
    "            use_likelihood=USE_LIKELIHOOD,\n",
    "            min_keep_likelihood=MIN_KEEP_LIKELIHOOD\n",
    "        )\n",
    "        if X_raw.shape[1] != DIMS:\n",
    "            raise ValueError(f\"{os.path.basename(p)}: 取り出し次元 {X_raw.shape[1]} が期待の {DIMS} と違います\")\n",
    "\n",
    "        X_raw = zscore_per_file(X_raw)\n",
    "        X_win = make_windows(X_raw, seq_len, stride)   # (M, T, D)\n",
    "        if X_win.shape[0] == 0:\n",
    "            print(f\"[WARN] {os.path.basename(p)}: フレーム不足（{seq_len}未満）でスキップ\")\n",
    "            continue\n",
    "\n",
    "        X_list.append(X_win)\n",
    "        y_list.append(np.full((X_win.shape[0],), y_lab, dtype=np.int64))\n",
    "        file_ids += [os.path.basename(p)]*X_win.shape[0]\n",
    "        used_kps_any = used_kps\n",
    "\n",
    "    if not X_list:\n",
    "        raise RuntimeError(\"データが作れませんでした。CSV と命名規則（ivdd/normal）を確認してください。\")\n",
    "    X = np.concatenate(X_list, axis=0)\n",
    "    y = np.concatenate(y_list, axis=0)\n",
    "    file_ids = np.array(file_ids)\n",
    "    print(f\"[INFO] 使用キーポイント実名: {used_kps_any}\")\n",
    "    return X, y, file_ids\n",
    "\n",
    "# ========= モデル（構造そのまま＋L2） =========\n",
    "class LSTM_RNN(keras.Model):\n",
    "    def __init__(self, n_input, n_hidden, n_classes):\n",
    "        super().__init__()\n",
    "        self.input_dense = keras.layers.Dense(n_hidden, activation='relu')\n",
    "        self.time_dist   = keras.layers.TimeDistributed(self.input_dense)\n",
    "        self.lstm1 = keras.layers.LSTM(n_hidden, return_sequences=True)\n",
    "        self.lstm2 = keras.layers.LSTM(n_hidden)\n",
    "        self.out   = keras.layers.Dense(n_classes)  # logits\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.time_dist(x)\n",
    "        x = self.lstm1(x, training=training)\n",
    "        x = self.lstm2(x, training=training)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class LSTMWithL2(LSTM_RNN):\n",
    "    def __init__(self, n_input, n_hidden, n_classes, l2_lambda=1e-4):\n",
    "        super().__init__(n_input, n_hidden, n_classes)\n",
    "        self.l2_lambda   = l2_lambda\n",
    "        self.loss_fn     = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        self.metric_acc  = keras.metrics.CategoricalAccuracy(name=\"accuracy\")\n",
    "        self.metric_loss = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # Keras に登録（自動で reset_state される）\n",
    "        return [self.metric_loss, self.metric_acc]\n",
    "\n",
    "    def compile(self, optimizer, **kwargs):\n",
    "        super().compile(optimizer=optimizer, **kwargs)\n",
    "\n",
    "    def _l2(self):\n",
    "        if not self.trainable_variables:\n",
    "            return 0.0\n",
    "        return self.l2_lambda * tf.add_n([tf.nn.l2_loss(v) for v in self.trainable_variables])\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # class_weight を使うと (x,y,sample_weight) が来ることがある\n",
    "        if isinstance(data, (list, tuple)) and len(data) == 3:\n",
    "            x, y, sample_weight = data\n",
    "        else:\n",
    "            x, y = data\n",
    "            sample_weight = None\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self(x, training=True)\n",
    "            loss = self.loss_fn(y, logits, sample_weight=sample_weight) + self._l2()\n",
    "\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "        self.metric_loss.update_state(loss)\n",
    "        self.metric_acc.update_state(y, logits, sample_weight=sample_weight)\n",
    "        return {\"loss\": self.metric_loss.result(), \"accuracy\": self.metric_acc.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        if isinstance(data, (list, tuple)) and len(data) == 3:\n",
    "            x, y, sample_weight = data\n",
    "        else:\n",
    "            x, y = data\n",
    "            sample_weight = None\n",
    "\n",
    "        logits = self(x, training=False)\n",
    "        loss = self.loss_fn(y, logits, sample_weight=sample_weight) + self._l2()\n",
    "\n",
    "        self.metric_loss.update_state(loss)\n",
    "        self.metric_acc.update_state(y, logits, sample_weight=sample_weight)\n",
    "        return {\"loss\": self.metric_loss.result(), \"accuracy\": self.metric_acc.result()}\n",
    "\n",
    "# ========= データ読み込み & 学習 =========\n",
    "csv_files  = sorted(glob.glob(CSV_GLOB))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"CSV が見つかりません: {CSV_GLOB}\")\n",
    "\n",
    "X, y, file_ids = build_dataset(csv_files, SEQ_LEN, STRIDE)\n",
    "print(\"X:\", X.shape, \"y:\", y.shape, \"files:\", len(np.unique(file_ids)))\n",
    "\n",
    "# one-hot\n",
    "y_oh = keras.utils.to_categorical(y, num_classes=N_CLASSES)\n",
    "\n",
    "# ファイルリーク防止の分割\n",
    "if VAL_SPLIT_BY_FILE:\n",
    "    uniq = np.unique(file_ids)\n",
    "    tr_files, va_files = train_test_split(uniq, test_size=0.2, random_state=42, shuffle=True)\n",
    "    tr_mask = np.isin(file_ids, tr_files)\n",
    "    va_mask = np.isin(file_ids, va_files)\n",
    "    X_train, y_train = X[tr_mask], y_oh[tr_mask]\n",
    "    X_val,   y_val   = X[va_mask], y_oh[va_mask]\n",
    "else:\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y_oh, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# クラス不均衡対策\n",
    "cls_w = compute_class_weight(\"balanced\", classes=np.arange(N_CLASSES), y=np.argmax(y_train, axis=1))\n",
    "class_weight = {int(c): float(w) for c, w in enumerate(cls_w)}\n",
    "print(\"class_weight:\", class_weight)\n",
    "\n",
    "# モデル\n",
    "n_hidden = 30\n",
    "model = LSTMWithL2(n_input=DIMS, n_hidden=n_hidden, n_classes=N_CLASSES, l2_lambda=1e-4)\n",
    "\n",
    "# ★ 固定LR（ReduceLROnPlateau と相性良い）\n",
    "opt = keras.optimizers.Adam(learning_rate=LR)\n",
    "model.compile(optimizer=opt)\n",
    "\n",
    "# コールバック（val_accuracy は最大化 / val_loss は最小化）\n",
    "ckpt_dir  = os.path.join(DATA_DIR, \"checkpoints\"); os.makedirs(ckpt_dir, exist_ok=True)\n",
    "ckpt_path = os.path.join(ckpt_dir, \"best.keras\")\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        ckpt_path, monitor=\"val_accuracy\", mode=\"max\",\n",
    "        save_best_only=True, verbose=1\n",
    "    ),\n",
    "    # keras.callbacks.EarlyStopping(\n",
    "    #     monitor=\"val_accuracy\", mode=\"max\",\n",
    "    #     patience=10, restore_best_weights=True, verbose=1\n",
    "    # ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", mode=\"min\",\n",
    "        factor=0.5, patience=5, min_lr=1e-6, verbose=1\n",
    "    ),\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_weight=class_weight,   # ← sample_weight に展開され、train_step で受ける\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# ========= 評価 =========\n",
    "y_val_prob = model.predict(X_val, batch_size=BATCH_SIZE)\n",
    "y_val_pred = np.argmax(y_val_prob, axis=1)\n",
    "y_val_true = np.argmax(y_val, axis=1)\n",
    "\n",
    "print(\"\\nValidation report:\")\n",
    "print(classification_report(y_val_true, y_val_pred, target_names=CLASS_NAMES, digits=4))\n",
    "cm = confusion_matrix(y_val_true, y_val_pred)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "# 学習曲線\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1); plt.plot(history.history[\"loss\"]); plt.plot(history.history[\"val_loss\"]); plt.title(\"Loss\"); plt.legend([\"train\",\"val\"])\n",
    "plt.subplot(1,2,2); plt.plot(history.history[\"accuracy\"]); plt.plot(history.history[\"val_accuracy\"]); plt.title(\"Accuracy\"); plt.legend([\"train\",\"val\"])\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# 最終保存\n",
    "final_path = os.path.join(DATA_DIR, \"ivdd_lstm_final.keras\")\n",
    "model.save(final_path)\n",
    "print(\"Saved model to:\", final_path)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
