{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4de5b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 走査: C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train_csv\n",
      "[INFO] レポート保存: C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\likelihood_qc\\dlc_lowlikelihood_report_20251219-154455.csv\n",
      "[INFO] 要修正ファイル一覧: C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\likelihood_qc\\dlc_lowlikelihood_problem_files_20251219-154455.txt\n",
      "\n",
      "===== SUMMARY =====\n",
      "対象ディレクトリ数: 1\n",
      "解析ファイル総数:   117\n",
      "要修正ファイル数:   106\n",
      "例:\n",
      " - C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train_csv\\normal_10DLC_resnet50_sotuken1Dec17shuffle1_100000.csv\n",
      " - C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train_csv\\normal_11DLC_resnet50_sotuken1Dec17shuffle1_100000.csv\n",
      " - C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train_csv\\normal_13DLC_resnet50_sotuken1Dec17shuffle1_100000.csv\n",
      " - C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train_csv\\normal_17DLC_resnet50_sotuken1Dec17shuffle1_100000.csv\n",
      " - C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train_csv\\normal_20DLC_resnet50_sotuken1Dec17shuffle1_100000.csv\n",
      " - C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train_csv\\normal_22DLC_resnet50_sotuken1Dec17shuffle1_100000.csv\n",
      " - C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train_csv\\normal_27DLC_resnet50_sotuken1Dec17shuffle1_100000.csv\n",
      " - C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train_csv\\normal_30DLC_resnet50_sotuken1Dec17shuffle1_100000.csv\n",
      " - C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train_csv\\normal_31DLC_resnet50_sotuken1Dec17shuffle1_100000.csv\n",
      " - C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train_csv\\normal_33DLC_resnet50_sotuken1Dec17shuffle1_100000.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "DLC CSV の likelihood 品質チェック\n",
    "- 各 CSV について、指定キーポイント群のいずれかで \"likelihood <= LIKELIHOOD_THRESH\" となるフレームの割合を計算\n",
    "- その割合が RATIO_THRESHOLD（デフォ 0.5=50%）を超えるファイルを「要修正」として保存\n",
    "- 出力:\n",
    "    1) 詳細レポート CSV（各ファイルの総フレーム数、any低信頼フレーム割合、各KPの割合など）\n",
    "    2) 要修正ファイル名リスト TXT\n",
    "- DLC 3レベルヘッダ (scorer, bodypart, {x,y,likelihood}) を想定\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ======== 設定（必要に応じて変更）========\n",
    "# 監視対象ディレクトリ（複数可）: train と eval を一気に確認したい時は配列に追加してください\n",
    "DIRECTORIES = [\n",
    "    r\"C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\data\\train\\train_csv\",\n",
    "    \n",
    "]\n",
    "\n",
    "# 使うキーポイント（None なら CSV 内の全キーポイントを対象）\n",
    "#KEYPOINTS = [\"tail_set\", \"right_tarsal\", \"right_paw\", \"left_tarsal\", \"left_paw\"]\n",
    "KEYPOINTS = None  # ←全キーポイントで評価したい場合はこちら\n",
    "\n",
    "# likelihood 閾値（この値以下を低信頼と見なす）\n",
    "LIKELIHOOD_THRESH = 0.6\n",
    "\n",
    "# 「低信頼フレームの割合（any低信頼）」がこの比率を超えたら要修正と判定（初期値：0.5 = 5割）\n",
    "RATIO_THRESHOLD = 0.5\n",
    "\n",
    "# 出力サブフォルダ名（対象ディレクトリの“ひとつ上”に作成します）\n",
    "OUT_SUBDIR_NAME = \"likelihood_qc\"\n",
    "\n",
    "# =========================================\n",
    "\n",
    "\n",
    "def _norm_name(s: str) -> str:\n",
    "    \"\"\"比較用に小文字化 + 空白/アンダーバー/ハイフンを除去\"\"\"\n",
    "    return \"\".join(ch for ch in s.lower() if ch not in \" _-\")\n",
    "\n",
    "\n",
    "def _resolve_keypoints(all_bodyparts, requested):\n",
    "    \"\"\"\n",
    "    CSV内に存在する実名ボディパーツを、requested（正規化済み比較）に応じて抽出。\n",
    "    requested が None の場合は all_bodyparts（重複除去）をそのまま返す。\n",
    "    \"\"\"\n",
    "    uniq = []\n",
    "    seen = set()\n",
    "    for bp in all_bodyparts:\n",
    "        if bp not in seen:\n",
    "            uniq.append(bp)\n",
    "            seen.add(bp)\n",
    "\n",
    "    if requested is None:\n",
    "        return uniq\n",
    "\n",
    "    # 実名 <-> 正規化名 の対応\n",
    "    norm2orig = {}\n",
    "    for bp in uniq:\n",
    "        k = _norm_name(bp)\n",
    "        if k not in norm2orig:\n",
    "            norm2orig[k] = bp\n",
    "\n",
    "    resolved, missing = [], []\n",
    "    for req in requested:\n",
    "        k = _norm_name(req)\n",
    "        if k in norm2orig:\n",
    "            resolved.append(norm2orig[k])\n",
    "        else:\n",
    "            missing.append(req)\n",
    "\n",
    "    if missing:\n",
    "        print(f\"[WARN] 指定キーポイントがCSVで見つかりません: {missing}\\n  利用可能: {uniq}\")\n",
    "    return resolved\n",
    "\n",
    "\n",
    "def analyze_csv(csv_path: str, use_keypoints):\n",
    "    \"\"\"\n",
    "    単一 CSV を解析し、以下を返す:\n",
    "      - n_frames: 総フレーム数\n",
    "      - ratio_any_low: いずれかの対象KPで likelihood<=閾値 となるフレーム割合\n",
    "      - per-kp の低信頼割合（辞書）\n",
    "    \"\"\"\n",
    "    # 3段ヘッダで読み込み\n",
    "    df = pd.read_csv(csv_path, header=[0, 1, 2], index_col=0)\n",
    "    # CSVに存在する全ボディパーツ\n",
    "    bodyparts = list({bp for (_, bp, _) in df.columns})\n",
    "\n",
    "    # 対象KPを決める\n",
    "    use_kps = _resolve_keypoints(bodyparts, use_keypoints)\n",
    "    if not use_kps:\n",
    "        raise ValueError(f\"対象キーポイントが空です（CSV: {os.path.basename(csv_path)}）\")\n",
    "\n",
    "    # 各KPの likelihood 列を収集\n",
    "    lik_cols = []\n",
    "    per_kp_ratio = {}\n",
    "    for bp in use_kps:\n",
    "        try:\n",
    "            lik = df.xs((bp, \"likelihood\"), level=[1, 2], axis=1).values.flatten()\n",
    "        except KeyError:\n",
    "            # likelihood 列が無いKPはスキップ\n",
    "            print(f\"[WARN] likelihood列なし: {bp} in {os.path.basename(csv_path)}\")\n",
    "            continue\n",
    "\n",
    "        if lik.size == 0:\n",
    "            continue\n",
    "\n",
    "        lik_cols.append(lik)\n",
    "        low_ratio = np.mean(lik <= LIKELIHOOD_THRESH)\n",
    "        per_kp_ratio[bp] = float(low_ratio)\n",
    "\n",
    "    if not lik_cols:\n",
    "        # 対象KPについて likelihood 列が1つもない\n",
    "        return 0, float(\"nan\"), per_kp_ratio\n",
    "\n",
    "    # いずれかのKPで低信頼（行方向の OR）\n",
    "    L = np.column_stack(lik_cols)  # shape (T, K)\n",
    "    any_low = np.any(L <= LIKELIHOOD_THRESH, axis=1)\n",
    "    ratio_any_low = float(np.mean(any_low))\n",
    "    n_frames = int(L.shape[0])\n",
    "\n",
    "    return n_frames, ratio_any_low, per_kp_ratio\n",
    "\n",
    "\n",
    "def main():\n",
    "    date_str = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    all_rows = []\n",
    "    flagged = []\n",
    "\n",
    "    for target_dir in DIRECTORIES:\n",
    "        target_dir = os.path.abspath(target_dir)\n",
    "        if not os.path.isdir(target_dir):\n",
    "            print(f\"[WARN] ディレクトリなし: {target_dir}\")\n",
    "            continue\n",
    "\n",
    "        # 出力先は「target_dir のひとつ上」に OUT_SUBDIR_NAME をぶら下げる\n",
    "        parent = os.path.dirname(target_dir)\n",
    "        out_dir = os.path.join(parent, OUT_SUBDIR_NAME)\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"[INFO] 走査: {target_dir}\")\n",
    "        csv_list = sorted(glob.glob(os.path.join(target_dir, \"*.csv\")))\n",
    "        if not csv_list:\n",
    "            print(f\"[WARN] CSV が見つかりません: {target_dir}\")\n",
    "            continue\n",
    "\n",
    "        for csv_path in csv_list:\n",
    "            try:\n",
    "                n_frames, ratio_any_low, per_kp_ratio = analyze_csv(csv_path, KEYPOINTS)\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] 解析失敗: {os.path.basename(csv_path)} -> {e}\")\n",
    "                continue\n",
    "\n",
    "            row = {\n",
    "                \"dir\": target_dir,\n",
    "                \"file\": os.path.basename(csv_path),\n",
    "                \"n_frames\": n_frames,\n",
    "                \"ratio_any_low\": ratio_any_low,\n",
    "                \"threshold_likelihood\": LIKELIHOOD_THRESH,\n",
    "                \"threshold_ratio\": RATIO_THRESHOLD,\n",
    "            }\n",
    "            # KPごとの割合も列として付与（列名: ratio_<normed_kpname>）\n",
    "            for bp, r in per_kp_ratio.items():\n",
    "                col = \"ratio_\" + _norm_name(bp)\n",
    "                row[col] = r\n",
    "\n",
    "            all_rows.append(row)\n",
    "\n",
    "            # 閾値超過ファイルをフラグ\n",
    "            if n_frames > 0 and ratio_any_low > RATIO_THRESHOLD:\n",
    "                flagged.append(os.path.join(target_dir, os.path.basename(csv_path)))\n",
    "\n",
    "        # 各 target_dir ごとに保存（タイムスタンプ付き）\n",
    "        if all_rows:\n",
    "            df = pd.DataFrame(all_rows)\n",
    "            # 同じ parent のものだけ抜き出して保存（見やすさのため）\n",
    "            df_parent = df[df[\"dir\"] == target_dir].copy()\n",
    "\n",
    "            csv_out = os.path.join(out_dir, f\"dlc_lowlikelihood_report_{date_str}.csv\")\n",
    "            df_parent.to_csv(csv_out, index=False, encoding=\"utf-8-sig\")\n",
    "            print(f\"[INFO] レポート保存: {csv_out}\")\n",
    "\n",
    "            # フラグ対象のみの TXT\n",
    "            flagged_parent = [p for p in flagged if os.path.dirname(p) == target_dir]\n",
    "            txt_out = os.path.join(out_dir, f\"dlc_lowlikelihood_problem_files_{date_str}.txt\")\n",
    "            with open(txt_out, \"w\", encoding=\"utf-8\") as f:\n",
    "                for p in flagged_parent:\n",
    "                    f.write(p + \"\\n\")\n",
    "            print(f\"[INFO] 要修正ファイル一覧: {txt_out}\")\n",
    "\n",
    "    # 全体サマリ\n",
    "    print(\"\\n===== SUMMARY =====\")\n",
    "    print(f\"対象ディレクトリ数: {len(DIRECTORIES)}\")\n",
    "    print(f\"解析ファイル総数:   {len(set(os.path.basename(p) for p in flagged)) + 0 if not all_rows else sum(1 for _ in all_rows)}\")\n",
    "    print(f\"要修正ファイル数:   {len(flagged)}\")\n",
    "    if flagged:\n",
    "        print(\"例:\")\n",
    "        for p in flagged[:10]:\n",
    "            print(\" -\", p)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLya",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
