{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bb9bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Eval (3 keypoints) with a trained .keras model\n",
    "- NORMALIZE_MODE must match training: \"zscore\" or \"tail_minmax\"\n",
    "- outputs go to: data/test/eval_outputs/<TS>/\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"-1\")\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "\n",
    "import re, glob\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ====== パス設定 ======\n",
    "REPO_ROOT = r\"C:\\kanno\\vscode\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\\RNN-for-Human-Activity-Recognition-using-2D-Pose-Input-master\"\n",
    "TRAIN_ROOT = os.path.join(REPO_ROOT, \"data\", \"train\")\n",
    "TEST_ROOT  = os.path.join(REPO_ROOT, \"data\", \"test\")\n",
    "\n",
    "EVAL_CSV_DIR = os.path.join(TEST_ROOT, \"eval_csv\")\n",
    "EVAL_GLOB    = os.path.join(EVAL_CSV_DIR, \"*.csv\")\n",
    "OUT_BASE     = os.path.join(os.path.dirname(EVAL_CSV_DIR), \"eval_outputs\")  # sibling of eval_csv\n",
    "assert os.path.basename(EVAL_CSV_DIR) == \"eval_csv\", \"eval_outputsがeval_csvの内側に作られないよう防御\"\n",
    "os.makedirs(OUT_BASE, exist_ok=True)\n",
    "\n",
    "RUN_ID   = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "OUT_DIR  = os.path.join(OUT_BASE, RUN_ID)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ====== モデルの場所 ======\n",
    "# 明示的に指定するならここを編集（例: bestモデルのフルパス）\n",
    "CKPT_PATH = \"\"  # \"\" の場合は最新モデルを自動探索\n",
    "\n",
    "# 学習モードに合わせる\n",
    "NORMALIZE_MODE = \"tail_minmax\"  # or \"zscore\"\n",
    "\n",
    "# ====== モデル自動探索（CKPT_PATHが空のとき） ======\n",
    "def _latest_keras_in(dir_path: str) -> str | None:\n",
    "    files = glob.glob(os.path.join(dir_path, \"*.keras\"))\n",
    "    if not files:\n",
    "        return None\n",
    "    files.sort(key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "    return files[0]\n",
    "\n",
    "if CKPT_PATH == \"\":\n",
    "    cand = _latest_keras_in(os.path.join(TRAIN_ROOT, \"train1_model\"))\n",
    "    if cand is None:\n",
    "        cand = _latest_keras_in(os.path.join(TRAIN_ROOT, \"train2_model\"))\n",
    "    if cand is None:\n",
    "        raise FileNotFoundError(\"最新モデル(.keras)が見つかりませんでした。CKPT_PATHを明示してください。\")\n",
    "    CKPT_PATH = cand\n",
    "\n",
    "print(\"[INFO] using model:\", CKPT_PATH)\n",
    "\n",
    "# ====== データ設定 ======\n",
    "KEYPOINTS = [\"left back paw\", \"right back paw\", \"tail set\"]\n",
    "USE_LIKELIHOOD      = False\n",
    "MIN_KEEP_LIKELIHOOD = 0.6\n",
    "SEQ_LEN  = 60\n",
    "STRIDE   = 30\n",
    "DIMS     = 6\n",
    "\n",
    "CLASS_NAMES = [\"normal\", \"ivdd\"]\n",
    "NAME2IDX    = {\"normal\":0, \"ivdd\":1}\n",
    "\n",
    "# ====== 前処理 ======\n",
    "def _norm_name(s: str) -> str:\n",
    "    return \"\".join(ch for ch in s.lower() if ch not in \" _-\")\n",
    "\n",
    "def _resolve_keypoints(all_bodyparts, requested):\n",
    "    norm2orig = {}\n",
    "    for bp in all_bodyparts:\n",
    "        k = _norm_name(bp)\n",
    "        if k not in norm2orig:\n",
    "            norm2orig[k] = bp\n",
    "    resolved, missing = [], []\n",
    "    for req in requested:\n",
    "        k = _norm_name(req)\n",
    "        if k in norm2orig:\n",
    "            resolved.append(norm2orig[k])\n",
    "        else:\n",
    "            missing.append(req)\n",
    "    if missing:\n",
    "        raise ValueError(f\"指定KPが見つかりません: {missing}\\n利用可能: {all_bodyparts}\")\n",
    "    return resolved\n",
    "\n",
    "def infer_label_from_filename(path: str) -> int:\n",
    "    name = os.path.basename(path).lower()\n",
    "    stem = os.path.splitext(name)[0]\n",
    "    tokens = [t for t in re.split(r'[^a-z0-9]+', stem) if t]\n",
    "    token_set = set(tokens)\n",
    "\n",
    "    has_ivdd = any(t == \"ivdd\" or t.startswith(\"ivdd\") for t in tokens)\n",
    "    has_normal = \"normal\" in token_set\n",
    "\n",
    "    if has_ivdd and not has_normal:\n",
    "        return NAME2IDX[\"ivdd\"]\n",
    "    if has_normal and not has_ivdd:\n",
    "        return NAME2IDX[\"normal\"]\n",
    "\n",
    "    if tokens and tokens[0] in NAME2IDX:\n",
    "        return NAME2IDX[tokens[0]]\n",
    "\n",
    "    parent_tokens = [t for t in re.split(r'[^a-z0-9]+', os.path.dirname(path).lower()) if t]\n",
    "    p_has_ivdd   = any(t == \"ivdd\" or t.startswith(\"ivdd\") for t in parent_tokens)\n",
    "    p_has_normal = \"normal\" in set(parent_tokens)\n",
    "    if p_has_ivdd and not p_has_normal:\n",
    "        return NAME2IDX[\"ivdd\"]\n",
    "    if p_has_normal and not p_has_ivdd:\n",
    "        return NAME2IDX[\"normal\"]\n",
    "\n",
    "    raise ValueError(f\"ラベル不明: {name}\")\n",
    "\n",
    "def read_dlc_3kp_xy(csv_path: str, keypoints, use_likelihood=True, min_keep_likelihood=0.6):\n",
    "    df = pd.read_csv(csv_path, header=[0,1,2], index_col=0)\n",
    "    bodyparts = list({bp for (_, bp, _) in df.columns})\n",
    "    use_kps = _resolve_keypoints(bodyparts, keypoints)\n",
    "\n",
    "    cols = {}\n",
    "    for bp in use_kps:\n",
    "        cols[f\"{bp}_x\"] = df.xs((bp, \"x\"), level=[1,2], axis=1)\n",
    "        cols[f\"{bp}_y\"] = df.xs((bp, \"y\"), level=[1,2], axis=1)\n",
    "    X_df = pd.concat(cols.values(), axis=1)\n",
    "    X_df.columns = list(cols.keys())\n",
    "\n",
    "    if use_likelihood:\n",
    "        for bp in use_kps:\n",
    "            try:\n",
    "                lcol = df.xs((bp, \"likelihood\"), level=[1,2], axis=1).values.flatten()\n",
    "                low = lcol < min_keep_likelihood\n",
    "                for c in [f\"{bp}_x\", f\"{bp}_y\"]:\n",
    "                    v = X_df[c].values\n",
    "                    v[low] = np.nan\n",
    "                    X_df[c] = v\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "    X_df = X_df.interpolate(method=\"linear\", limit_direction=\"both\", axis=0)\n",
    "    X_df = X_df.bfill().ffill().fillna(0.0)\n",
    "    return X_df.values.astype(np.float32), use_kps  # (T,6)\n",
    "\n",
    "def zscore_per_file(X: np.ndarray, eps=1e-6) -> np.ndarray:\n",
    "    mu = X.mean(axis=0, keepdims=True)\n",
    "    sd = X.std(axis=0, keepdims=True)\n",
    "    return (X - mu) / (sd + eps)\n",
    "\n",
    "def normalize_tailset_minmax(X: np.ndarray, used_kps: list[str], ref_name=\"tail set\", eps=1e-6) -> np.ndarray:\n",
    "    low = [s.lower() for s in used_kps]\n",
    "    if ref_name.lower() not in low:\n",
    "        raise ValueError(f\"'{ref_name}' が used_kps にありません: {used_kps}\")\n",
    "    r = low.index(ref_name.lower())\n",
    "\n",
    "    Xc = X.copy()\n",
    "    cx, cy = X[:, 2*r], X[:, 2*r+1]\n",
    "    for i in range(len(used_kps)):\n",
    "        Xc[:, 2*i]   -= cx\n",
    "        Xc[:, 2*i+1] -= cy\n",
    "\n",
    "    mn = Xc.min(axis=0, keepdims=True)\n",
    "    mx = Xc.max(axis=0, keepdims=True)\n",
    "    return (Xc - mn) / (mx - mn + eps)\n",
    "\n",
    "def make_windows(X: np.ndarray, seq_len: int, stride: int):\n",
    "    n = X.shape[0]\n",
    "    if n < seq_len:\n",
    "        return np.empty((0, seq_len, X.shape[1]), dtype=X.dtype), []\n",
    "    starts = list(range(0, n - seq_len + 1, stride))\n",
    "    Xw = np.stack([X[s:s+seq_len] for s in starts], axis=0)\n",
    "    return Xw, starts\n",
    "\n",
    "def build_dataset(csv_paths, seq_len=SEQ_LEN, stride=STRIDE):\n",
    "    Xs, ys, fids, starts_all = [], [], [], []\n",
    "    used_kps_any = None\n",
    "    for p in csv_paths:\n",
    "        y = infer_label_from_filename(p)\n",
    "        X_raw, used_kps = read_dlc_3kp_xy(p, KEYPOINTS, USE_LIKELIHOOD, MIN_KEEP_LIKELIHOOD)\n",
    "        if used_kps_any is None:\n",
    "            used_kps_any = used_kps\n",
    "\n",
    "        if X_raw.shape[1] != DIMS:\n",
    "            raise ValueError(f\"{os.path.basename(p)}: 次元{X_raw.shape[1]} != 期待{DIMS}\")\n",
    "\n",
    "        if NORMALIZE_MODE == \"zscore\":\n",
    "            Xn = zscore_per_file(X_raw)\n",
    "        elif NORMALIZE_MODE == \"tail_minmax\":\n",
    "            Xn = normalize_tailset_minmax(X_raw, used_kps)\n",
    "        else:\n",
    "            raise ValueError(\"NORMALIZE_MODE は 'zscore' or 'tail_minmax'\")\n",
    "\n",
    "        Xw, sidx = make_windows(Xn, seq_len, stride)\n",
    "        if Xw.shape[0] == 0:\n",
    "            print(f\"[WARN] {os.path.basename(p)}: フレーム不足でスキップ\")\n",
    "            continue\n",
    "\n",
    "        Xs.append(Xw)\n",
    "        ys.append(np.full((Xw.shape[0],), y, dtype=np.int64))\n",
    "        fids.extend([os.path.basename(p)]*Xw.shape[0])\n",
    "        starts_all.extend(sidx)\n",
    "\n",
    "    if not Xs:\n",
    "        raise RuntimeError(\"評価用データが作れませんでした。eval_csv内のCSV名を確認してください。\")\n",
    "    X = np.concatenate(Xs, axis=0)\n",
    "    y = np.concatenate(ys, axis=0)\n",
    "    fids = np.array(fids)\n",
    "    starts_all = np.array(starts_all)\n",
    "    print(f\"[INFO] 使用キーポイント: {used_kps_any}\")\n",
    "    return X, y, fids, starts_all\n",
    "\n",
    "# ====== CSV読み込み ======\n",
    "eval_csvs = sorted(glob.glob(EVAL_GLOB))\n",
    "if not eval_csvs:\n",
    "    raise FileNotFoundError(f\"評価用CSVが見つかりません: {EVAL_GLOB}\")\n",
    "\n",
    "X, y, file_ids, starts = build_dataset(eval_csvs, SEQ_LEN, STRIDE)\n",
    "print(\"X:\", X.shape, \"y:\", y.shape, \"files:\", len(np.unique(file_ids)))\n",
    "\n",
    "# ====== モデル読み込み ======\n",
    "model = keras.models.load_model(CKPT_PATH, compile=False)\n",
    "logits = model.predict(X, batch_size=64)\n",
    "\n",
    "# 出力の形に応じて確率を計算（Dense(1) or Dense(2) どちらでもOKに）\n",
    "if logits.ndim == 2 and logits.shape[1] == 1:\n",
    "    p_ivdd = tf.math.sigmoid(logits).numpy().ravel()\n",
    "elif logits.ndim == 2 and logits.shape[1] == 2:\n",
    "    probs = tf.nn.softmax(logits, axis=1).numpy()\n",
    "    # 推定上、「ivdd」をindex=1とする（学習時 0=normal,1=ivdd に合わせる）\n",
    "    p_ivdd = probs[:, 1]\n",
    "else:\n",
    "    raise ValueError(f\"未知のlogits形状: {logits.shape}\")\n",
    "\n",
    "p_normal = 1.0 - p_ivdd\n",
    "y_pred = (p_ivdd >= 0.5).astype(int)\n",
    "\n",
    "# ====== 保存パス ======\n",
    "paths = {\n",
    "    \"win_csv\":           os.path.join(OUT_DIR, f\"window_predictions_{RUN_ID}.csv\"),\n",
    "    \"roc_png\":           os.path.join(OUT_DIR, f\"roc_window_{RUN_ID}.png\"),\n",
    "    \"cm_win_png\":        os.path.join(OUT_DIR, f\"cm_window_{RUN_ID}.png\"),\n",
    "    \"file_pred_csv\":     os.path.join(OUT_DIR, f\"file_level_predictions_{RUN_ID}.csv\"),\n",
    "    \"file_errors_csv\":   os.path.join(OUT_DIR, f\"file_level_errors_{RUN_ID}.csv\"),\n",
    "    \"cm_file_major_png\": os.path.join(OUT_DIR, f\"cm_file_majority_{RUN_ID}.png\"),\n",
    "    \"cm_file_mean_png\":  os.path.join(OUT_DIR, f\"cm_file_meanprob_{RUN_ID}.png\"),\n",
    "}\n",
    "\n",
    "# ====== ウィンドウ単位レポート ======\n",
    "print(\"\\n[Window-level] classification_report:\")\n",
    "print(classification_report(y, y_pred, target_names=CLASS_NAMES, digits=4))\n",
    "cm_win = confusion_matrix(y, y_pred, labels=[0,1])\n",
    "print(\"[Window-level] confusion matrix:\\n\", cm_win)\n",
    "\n",
    "df_win = (\n",
    "    pd.DataFrame({\n",
    "        \"file\": file_ids, \"start\": starts, \"true\": y, \"pred\": y_pred,\n",
    "        \"p_ivdd\": p_ivdd, \"p_normal\": p_normal\n",
    "    })\n",
    "    .sort_values([\"file\",\"start\"])\n",
    ")\n",
    "df_win.to_csv(paths[\"win_csv\"], index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[INFO] 保存:\", paths[\"win_csv\"])\n",
    "\n",
    "# ====== ROC（ウィンドウ） ======\n",
    "try:\n",
    "    auc_val = roc_auc_score(y, p_ivdd)\n",
    "    fpr, tpr, _ = roc_curve(y, p_ivdd, pos_label=1)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC={auc_val:.3f}\")\n",
    "    plt.plot([0,1],[0,1],\"--\")\n",
    "    plt.title(\"ROC (window-level)\"); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(paths[\"roc_png\"], dpi=150); plt.close()\n",
    "    print(\"[INFO] 保存:\", paths[\"roc_png\"])\n",
    "except Exception as e:\n",
    "    print(\"[WARN] ROC失敗:\", e)\n",
    "\n",
    "# ====== 混同行列（ウィンドウ） ======\n",
    "def plot_cm(cm, labels, title, out_png):\n",
    "    cmn = cm.astype(np.float32) / np.maximum(cm.sum(axis=1, keepdims=True), 1)\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    im = ax.imshow(cmn, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(len(labels)), yticks=np.arange(len(labels)),\n",
    "           xticklabels=labels, yticklabels=labels, ylabel=\"True\", xlabel=\"Pred\", title=title)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, int(cm[i,j]), ha=\"center\",\n",
    "                    color=\"white\" if cmn[i,j] > cmn.max()/2 else \"black\", fontsize=12)\n",
    "    plt.tight_layout(); plt.savefig(out_png, dpi=150); plt.close()\n",
    "\n",
    "plot_cm(cm_win, CLASS_NAMES, \"Confusion Matrix (window-level)\", paths[\"cm_win_png\"])\n",
    "print(\"[INFO] 保存:\", paths[\"cm_win_png\"])\n",
    "\n",
    "# ====== ファイル単位（多数決 / 平均確率） ======\n",
    "major_pred = df_win.groupby(\"file\")[\"pred\"].agg(lambda a: np.bincount(a, minlength=2).argmax())\n",
    "true_file  = df_win.groupby(\"file\")[\"true\"].first()  # 同一ファイルは同一ラベル想定\n",
    "\n",
    "mean_prob = df_win.groupby(\"file\")[[\"p_normal\",\"p_ivdd\"]].mean()\n",
    "mean_pred = (mean_prob[\"p_ivdd\"].values >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n[File-level Majority] report:\")\n",
    "print(classification_report(true_file.values, major_pred.values, target_names=CLASS_NAMES, digits=4))\n",
    "print(\"[File-level Majority] CM:\\n\", confusion_matrix(true_file.values, major_pred.values, labels=[0,1]))\n",
    "\n",
    "print(\"\\n[File-level MeanProb] report:\")\n",
    "print(classification_report(true_file.values, mean_pred, target_names=CLASS_NAMES, digits=4))\n",
    "print(\"[File-level MeanProb] CM:\\n\", confusion_matrix(true_file.values, mean_pred, labels=[0,1]))\n",
    "\n",
    "df_file = pd.DataFrame({\n",
    "    \"file\": mean_prob.index,\n",
    "    \"true\": [CLASS_NAMES[t] for t in true_file.values],\n",
    "    \"pred_majority\": [CLASS_NAMES[p] for p in major_pred.values],\n",
    "    \"pred_meanprob\": [CLASS_NAMES[p] for p in mean_pred],\n",
    "    \"p_normal_mean\": mean_prob[\"p_normal\"].values,\n",
    "    \"p_ivdd_mean\": mean_prob[\"p_ivdd\"].values,\n",
    "})\n",
    "df_file.to_csv(paths[\"file_pred_csv\"], index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[INFO] 保存:\", paths[\"file_pred_csv\"])\n",
    "\n",
    "# 誤分類のみ\n",
    "df_err = df_file[df_file[\"true\"] != df_file[\"pred_meanprob\"]].copy()\n",
    "df_err.to_csv(paths[\"file_errors_csv\"], index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[INFO] 保存:\", paths[\"file_errors_csv\"])\n",
    "\n",
    "# ファイル単位 CM 画像\n",
    "def plot_cm_filelevel(y_true, y_pred, labels, out_png, title):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    cmn = cm.astype(np.float32) / np.maximum(cm.sum(axis=1, keepdims=True), 1)\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    im = ax.imshow(cmn, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(len(labels)), yticks=np.arange(len(labels)),\n",
    "           xticklabels=labels, yticklabels=labels, ylabel=\"True\", xlabel=\"Pred\", title=title)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, int(cm[i,j]), ha=\"center\",\n",
    "                    color=\"white\" if cmn[i,j] > cmn.max()/2 else \"black\", fontsize=12)\n",
    "    plt.tight_layout(); plt.savefig(out_png, dpi=150); plt.close()\n",
    "\n",
    "y_true_f = true_file.values\n",
    "y_pred_major = major_pred.values\n",
    "y_pred_mean  = mean_pred\n",
    "\n",
    "plot_cm_filelevel(y_true_f, y_pred_major, CLASS_NAMES, paths[\"cm_file_major_png\"],\n",
    "                  \"Confusion Matrix (File-level, Majority Vote)\")\n",
    "plot_cm_filelevel(y_true_f, y_pred_mean, CLASS_NAMES, paths[\"cm_file_mean_png\"],\n",
    "                  \"Confusion Matrix (File-level, Mean Probability)\")\n",
    "print(\"[INFO] すべて保存完了:\", OUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
